<h1 class="firstHeading">15.3.3 Theory of Nonlinear Curve Fitting</h1><p class='urlname' style='display: none'>NLFit-Theory</p>
<div class="toclimit-4"><div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#How_Origin_Fits_the_Curve"><span class="tocnumber">1</span> <span class="toctext">How Origin Fits the Curve</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Explicit_Functions"><span class="tocnumber">1.1</span> <span class="toctext">Explicit Functions</span></a>
<ul>
<li class="toclevel-3 tocsection-3"><a href="#Fitting_Model"><span class="tocnumber">1.1.1</span> <span class="toctext">Fitting Model</span></a></li>
<li class="toclevel-3 tocsection-4"><a href="#Least-Squares_Algorithms"><span class="tocnumber">1.1.2</span> <span class="toctext">Least-Squares Algorithms</span></a>
<ul>
<li class="toclevel-4 tocsection-5"><a href="#Levenberg-Marquardt_.28L-M.29_Algorithm"><span class="tocnumber">1.1.2.1</span> <span class="toctext">Levenberg-Marquardt (L-M) Algorithm</span></a></li>
<li class="toclevel-4 tocsection-6"><a href="#Downhil_Simplex_Algorithm"><span class="tocnumber">1.1.2.2</span> <span class="toctext">Downhil Simplex Algorithm</span></a></li>
</ul>
</li>
<li class="toclevel-3 tocsection-7"><a href="#Orthogonal_Distance_Regression_.28ODR.29_Algorithm"><span class="tocnumber">1.1.3</span> <span class="toctext">Orthogonal Distance Regression (ODR) Algorithm</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="#Comparison_between_ODR_and_L-M"><span class="tocnumber">1.2</span> <span class="toctext">Comparison between ODR and L-M</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Implicit_Functions"><span class="tocnumber">1.3</span> <span class="toctext">Implicit Functions</span></a>
<ul>
<li class="toclevel-3 tocsection-10"><a href="#Fitting_Model_2"><span class="tocnumber">1.3.1</span> <span class="toctext">Fitting Model</span></a></li>
<li class="toclevel-3 tocsection-11"><a href="#Orthogonal_Distance_Regression_.28ODR.29_Algorithm_2"><span class="tocnumber">1.3.2</span> <span class="toctext">Orthogonal Distance Regression (ODR) Algorithm</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-12"><a href="#Weighted_Fitting"><span class="tocnumber">1.4</span> <span class="toctext">Weighted Fitting</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-13"><a href="#Parameters"><span class="tocnumber">2</span> <span class="toctext">Parameters</span></a>
<ul>
<li class="toclevel-2 tocsection-14"><a href="#The_Fitted_Value"><span class="tocnumber">2.1</span> <span class="toctext">The Fitted Value</span></a></li>
<li class="toclevel-2 tocsection-15"><a href="#Parameter_Standard_Errors"><span class="tocnumber">2.2</span> <span class="toctext">Parameter Standard Errors</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#The_Standard_Error_for_Derived_Parameter"><span class="tocnumber">2.3</span> <span class="toctext">The Standard Error for Derived Parameter</span></a></li>
<li class="toclevel-2 tocsection-17"><a href="#Confidence_Intervals"><span class="tocnumber">2.4</span> <span class="toctext">Confidence Intervals</span></a>
<ul>
<li class="toclevel-3 tocsection-18"><a href="#Asymptotic-Symmetry_Method"><span class="tocnumber">2.4.1</span> <span class="toctext">Asymptotic-Symmetry Method</span></a></li>
<li class="toclevel-3 tocsection-19"><a href="#Model-Comparison_Method"><span class="tocnumber">2.4.2</span> <span class="toctext">Model-Comparison Method</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-20"><a href="#t_Value"><span class="tocnumber">2.5</span> <span class="toctext">t Value</span></a></li>
<li class="toclevel-2 tocsection-21"><a href="#Prob.3E.7Ct.7C"><span class="tocnumber">2.6</span> <span class="toctext">Prob&gt;|t|</span></a></li>
<li class="toclevel-2 tocsection-22"><a href="#Dependency"><span class="tocnumber">2.7</span> <span class="toctext">Dependency</span></a></li>
<li class="toclevel-2 tocsection-23"><a href="#CI_Half_Width"><span class="tocnumber">2.8</span> <span class="toctext">CI Half Width</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-24"><a href="#Statistics"><span class="tocnumber">3</span> <span class="toctext">Statistics</span></a>
<ul>
<li class="toclevel-2 tocsection-25"><a href="#Degree_of_Freedom"><span class="tocnumber">3.1</span> <span class="toctext">Degree of Freedom</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Residual_Sum_of_Squares"><span class="tocnumber">3.2</span> <span class="toctext">Residual Sum of Squares</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Reduced_Chi-Sqr"><span class="tocnumber">3.3</span> <span class="toctext">Reduced Chi-Sqr</span></a></li>
<li class="toclevel-2 tocsection-28"><a href="#R-Square_.28COD.29"><span class="tocnumber">3.4</span> <span class="toctext">R-Square (COD)</span></a></li>
<li class="toclevel-2 tocsection-29"><a href="#Adj._R-Square"><span class="tocnumber">3.5</span> <span class="toctext">Adj. R-Square</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="#R_Value"><span class="tocnumber">3.6</span> <span class="toctext">R Value</span></a></li>
<li class="toclevel-2 tocsection-31"><a href="#Root-MSE_.28SD.29"><span class="tocnumber">3.7</span> <span class="toctext">Root-MSE (SD)</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-32"><a href="#ANOVA_Table"><span class="tocnumber">4</span> <span class="toctext">ANOVA Table</span></a></li>
<li class="toclevel-1 tocsection-33"><a href="#Confidence_and_Prediction_Bands"><span class="tocnumber">5</span> <span class="toctext">Confidence and Prediction Bands</span></a>
<ul>
<li class="toclevel-2 tocsection-34"><a href="#Confidence_Band"><span class="tocnumber">5.1</span> <span class="toctext">Confidence Band</span></a></li>
<li class="toclevel-2 tocsection-35"><a href="#Prediction_Band"><span class="tocnumber">5.2</span> <span class="toctext">Prediction Band</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-36"><a href="#Topics_for_Further_Reading"><span class="tocnumber">6</span> <span class="toctext">Topics for Further Reading</span></a></li>
<li class="toclevel-1 tocsection-37"><a href="#Reference"><span class="tocnumber">7</span> <span class="toctext">Reference</span></a></li>
</ul>
</div>
</div>
<h2><a name="How_Origin_Fits_the_Curve"></a><span class="mw-headline">How Origin Fits the Curve</span></h2>
<p>The aim of nonlinear fitting is to estimate the parameter values which best describe the data. Generally we can describe the process of nonlinear curve fitting as below. 
</p>
<ol><li> Generate an initial function curve from the <a href="../../UserGuide/UserGuide/Parameter_Initialization.html" title="UserGuide:Parameter Initialization">initial values</a>.</li>
<li> Iterate to adjust parameter values to make data points closer to the curve.</li>
<li> Stop when minimum distance reaches the stopping criteria to get the best fit</li></ol>
<p>Origin provides options of different algorithm, which have different iterative procedure and statistics to define minimum distance.
</p>
<h3><a name="Explicit_Functions"></a><span class="mw-headline">Explicit Functions</span></h3>
<h4><a name="Fitting_Model"></a><span class="mw-headline">Fitting Model</span></h4>
<p>A general nonlinear model can be expressed as follows:
</p>
<table class="formula">

<tr>
<td> <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-bcabf9847b705f433568f8c400c6e79c.png?v=0" title="Y=f(X, \boldsymbol{\beta})+\varepsilon " alt="Y=f(X, \boldsymbol{\beta})+\varepsilon " class="tex"/>
</td>
<td> (1)
</td></tr></table>
<p>where <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-35f21cd4e2e75c09315e49b2b031710b.png?v=0" title="X = (x_1, x_2, \cdots , x_k)&#39;" alt="X = (x_1, x_2, \cdots , x_k)&#39;" class="tex"/> is the independent variables and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-ca56912a8efd3e2d420b7ceeaf293903.png?v=0" title="\boldsymbol{\beta} = (\beta_1, \beta_2, \cdots , \beta_k)&#39;" alt="\boldsymbol{\beta} = (\beta_1, \beta_2, \cdots , \beta_k)&#39;" class="tex"/> is the parameters.
</p>
<table class="noborder"><tr>
	<td style="vertical-align:top" width="60"><img src="../images/Theory_of_Nonlinear_Curve_Fitting/Tip_icon.png?v=9614" width="57"  border="0" /></td><td><p><b>Examples of the Explicit Function</b>
</p>
<ul><li> <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-974247a2296cfc3a2c93ad0e93168732.png?v=0" title="y=y_0+Ae^{-x/t}" alt="y=y_0+Ae^{-x/t}" class="tex"/></li></ul>
</td></tr></table>
<h4><a name="Least-Squares_Algorithms"></a><span class="mw-headline">Least-Squares Algorithms</span></h4>
<p>The least square algorithm is to choose the parameters that would minimize the deviations of the theoretical curve(s) from the experimental points. This method is also called chi-square minimization, defined as follows:
</p>
<table class="formula">

<tr>
<td> <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-06e0bc4dd4c9e79eae6f116e751e9eb5.png?v=0" title="\chi ^2=\sum_{i=1}^n \left [ \frac{Y_i-f(x_i^{\prime },\hat{\beta }) } {\sigma _i} \right ]^2" alt="\chi ^2=\sum_{i=1}^n \left [ \frac{Y_i-f(x_i^{\prime },\hat{\beta }) } {\sigma _i} \right ]^2" class="tex"/>
</td>
<td> (2)
</td></tr></table>
<p>where <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-a347f189f4162c04370051b770afbfa1.png?v=0" title="x_i^{\prime }" alt="x_i^{\prime }" class="tex"/> is the row vector for the <i>i</i>th (<i>i</i> = 1, 2, ... , n) observation.
</p>
<table class="noborder"><tr>
	<td style="vertical-align:top" width="60"><img src="../images/Theory_of_Nonlinear_Curve_Fitting/Tip_icon.png?v=9614" width="57"  border="0" /></td><td><p>The figure below illustrates the concept to a simple linear model (Note that multiple regression and nonlinear fitting are similar).
</p>
<dl><dd><a  class="image"><img alt="Illustration of the Least Squares Method.png" src="../images/Theory_of_Nonlinear_Curve_Fitting/350px-Illustration_of_the_Least_Squares_Method.png?v=75510" width="350"   /></a></dd></dl>
<p>The <b>Best-Fit Curve</b> represents the assumed theoretical model. For a particular point <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-2d2896b8e02816556a3f43ee67a81ed4.png?v=0" title="(x_i,y_i)\,\!" alt="(x_i,y_i)\,\!" class="tex"/> in the original dataset, the corresponding theoretical value at <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-d2583020b138319a535bc3c88278ab33.png?v=0" title="x_i\,\!" alt="x_i\,\!" class="tex"/> is denoted by<small><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-bc2145775fa07d048a39d1846a4f4148.png?v=0" title="\widehat{y_i}" alt="\widehat{y_i}" class="tex"/></small>.
</p><p>If there are two independent variables in the regression model, the least square estimation will minimize the deviation of experimental data points to the best fitted surface.  When there are more then 3 independent variables, the fitted model will be a hypersurface. In this case, the fitted surface (or curve) will not be plotted when regression is performed.
</p></td></tr></table>
<p>Origin provides two options to adjust the parameter values in the iterative procedure
</p>
<dl><dd><ul><li> <a href="../../UserGuide/UserGuide/Settings_Tab_Upper_Panel.html#Function_Selection" title="UserGuide:Settings Tab Upper Panel">Levenberg-Marquardt (L-M) algorithm</a></li>
<li> <a href="../../UserGuide/UserGuide/NLFit_Dialog_Buttons.html" title="UserGuide:NLFit Dialog Buttons">Downhill Simplex approximation</a></li></ul></dd></dl>
<h5><a name="Levenberg-Marquardt_.28L-M.29_Algorithm"></a><span class="mw-headline">Levenberg-Marquardt (L-M) Algorithm</span></h5>
<p>The <a href="../../UserGuide/UserGuide/Settings_Tab_Upper_Panel.html#Function_Selection" title="UserGuide:Settings Tab Upper Panel">Levenberg-Marquardt (L-M) algorithm</a><sup><a href="#Reference">11</a></sup> is a iterative procedure which combines the Gauss-Newton method and the steepest descent method. 
The algorithm works well for most cases and become the standard of nonlinear least square routines.
</p>
<ol><li> Compute the <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-204b209474ca9158d3be8bf938264518.png?v=0" title="\chi ^2(b)" alt="\chi ^2(b)" class="tex"/> value from the given <a href="../../UserGuide/UserGuide/Parameter_Initialization.html" title="UserGuide:Parameter Initialization">initial values</a>: <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-92eb5ffee6ae2fec3ad71c777531578f.png?v=0" title="b" alt="b" class="tex"/> .</li>
<li> Pick a modest value for <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-c6a6eb61fd9c6c913da73b3642ca147d.png?v=0" title="\lambda" alt="\lambda" class="tex"/>, say <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-c6a6eb61fd9c6c913da73b3642ca147d.png?v=0" title="\lambda" alt="\lambda" class="tex"/> = 0.001</li>
<li> Solve the Levenberg-Marquardt funciton<sup><a href="#Reference">11</a></sup> for <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-02df0bfa3fa822a063d48e627b7ad7fc.png?v=0" title="\delta b" alt="\delta b" class="tex"/> and evaluate <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-2c4f070d67741634bb7dcb3815ba11db.png?v=0" title="\chi ^2(\beta + \delta b)" alt="\chi ^2(\beta + \delta b)" class="tex"/></li>
<li> If <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f81fcda6393e0a6fbdb5bfc2c572c20c.png?v=0" title="\chi ^2(\beta + \delta b) \geq \chi ^2(b)" alt="\chi ^2(\beta + \delta b) \geq \chi ^2(b)" class="tex"/>,increase <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-c6a6eb61fd9c6c913da73b3642ca147d.png?v=0" title="\lambda" alt="\lambda" class="tex"/> by a factor of 10 and go back to step 3</li>
<li> if <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-96084296a143c0c942baae5959dc08ac.png?v=0" title="\chi ^2(\beta + \delta b) \leq \chi ^2(b)" alt="\chi ^2(\beta + \delta b) \leq \chi ^2(b)" class="tex"/>, decrease <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-c6a6eb61fd9c6c913da73b3642ca147d.png?v=0" title="\lambda" alt="\lambda" class="tex"/> by a factor of 10, update the parameter values to be <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-02df0bfa3fa822a063d48e627b7ad7fc.png?v=0" title="\delta b" alt="\delta b" class="tex"/> and go back to step 3</li>
<li> Stop until the <a  class="image"><img alt="Temp-Regression and Curve Fitting-image125.gif" src="../images/Theory_of_Nonlinear_Curve_Fitting/Temp-Regression_and_Curve_Fitting-image125.gif?v=10966" width="21"  /></a> values computed in two successive iterations are small enough (compared with the <a href="../../UserGuide/UserGuide/Settings_Tab_Upper_Panel.html#Advanced" title="UserGuide:Settings Tab Upper Panel">tolerance</a>)</li></ol>
<h5><a name="Downhil_Simplex_Algorithm"></a><span class="mw-headline">Downhil Simplex Algorithm</span></h5>
<p>Besides the L-M method, Origin also provides a <a href="../../UserGuide/UserGuide/NLFit_Dialog_Buttons.html" title="UserGuide:NLFit Dialog Buttons">Downhill Simplex approximation</a><sup><a href="#Reference">9,10</a></sup>. In geometry, a simplex is a polytope of N + 1 vertices in N dimensions. In non-linear optimization, an analog exists for an objective function of N variables. During the iterations, the Simplex algorithm (also known as Nelder-Mead) adjusts the parameter "simplex" until it converges to a local minimum.
</p><p>Different from L-M method, the Simplex method does not require derivatives, and it is effective when the computational burden is small. Normally, if you did not get a good value for parameter initialization, you can try this method to get the approximate parameter value for further fitting calculations with L-M. The Simplex method tends to be more stable in that it is less likely to wander into a meaningless part of the parameter space; on the other hand, it is generally much slower than L-M, especially very close to a local minimum. Actually, there is no "perfect" algorithm for nonlinear fitting, and many things may affect the result (e.g., initial values). In complicated models, you may find one method may do better than the other. Additionally, you may want to try both methods to perform the fitting operation.
</p>
<h4><a name="Orthogonal_Distance_Regression_.28ODR.29_Algorithm"></a><span class="mw-headline">Orthogonal Distance Regression (ODR) Algorithm</span></h4>
<p>The <a href="../../UserGuide/UserGuide/Settings_Tab_Upper_Panel.html#Function_Selection" title="UserGuide:Settings Tab Upper Panel">Orthogonal Distance Regression (ODR) algorithm</a> minimizes the residual sum of squares by adjusting both fitting parameters and values of the independent variable in the iterative process. The residual in ODR is not the difference between the observed value and the predicted value for the dependent variable, but the orthogonal distance from the data to the fitted curve.
</p><p>Origin uses the ODR algorithm in ODRPACK95<sup><a href="#Reference">8</a></sup>. 
</p><p>For a explict function, the ODR algorithm could be expressed as:
</p><p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-63152dbc3e42c69b87cc39bdb5bf39f2.png?v=0" title="\min\left (\sum_{i=1}^{n}\left (w_{yi}\cdot \epsilon_{i} ^{2}+w_{xi}\cdot \delta_{i}^{2}  \right )  \right )" alt="\min\left (\sum_{i=1}^{n}\left (w_{yi}\cdot \epsilon_{i} ^{2}+w_{xi}\cdot \delta_{i}^{2}  \right )  \right )" class="tex"/>
</p><p>subject to the constraints:
</p><p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-bd52ec18c34bbb8cc60c2c5170b022c0.png?v=0" title="y_{i}=f\left ( x_{i} +\delta_{i}; \beta  \right )-\epsilon _{i}\ \ \ \ \ \ i=1,...,n" alt="y_{i}=f\left ( x_{i} +\delta_{i}; \beta  \right )-\epsilon _{i}\ \ \ \ \ \ i=1,...,n" class="tex"/>
</p><p>where <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f0ed6a7e9e5079b9b9f2bcd01768ddcc.png?v=0" title="w_{xi}" alt="w_{xi}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-6f64afa2febc1a132beeeba5e4bd81ef.png?v=0" title="w_{yi}" alt="w_{yi}" class="tex"/> are the user input weights of <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-05e42209d67fe1eb15a055e9d3b3770e.png?v=0" title="x_{i}" alt="x_{i}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-18daef71b5d25ce76b8628a81e4fc76b.png?v=0" title="y_{i}" alt="y_{i}" class="tex"/>, <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f046a1b3002c9fd615276bb4e1a3a761.png?v=0" title="\delta_{i}" alt="\delta_{i}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-67b31d49c0358463d6dbefd1c0c6c18c.png?v=0" title="\epsilon_{i}" alt="\epsilon_{i}" class="tex"/> are the residual of the corresponding <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-05e42209d67fe1eb15a055e9d3b3770e.png?v=0" title="x_{i}" alt="x_{i}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-18daef71b5d25ce76b8628a81e4fc76b.png?v=0" title="y_{i}" alt="y_{i}" class="tex"/>, and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-b0603860fcffe94e5b8eec59ed813421.png?v=0" title="\beta" alt="\beta" class="tex"/> is the fitting parameter.
</p><p>For more details of the ODR algorithm, please refer to ODRPACK95<sup><a href="#Reference">8</a></sup>.
</p>
<h3><a name="Comparison_between_ODR_and_L-M"></a><span class="mw-headline">Comparison between ODR and L-M</span></h3>
<p>To choose the ODR or L-M algorithm for your fitting, you may refer to the following table for information:
</p>
<table class="simple">
<tr>
<th>
</th>
<th> Orthogonal Distance Regression
</th>
<th> Levenberg-Marquardt
</th></tr>
<tr>
<td style="background:#C9C9C9"> <b>Application</b>
</td>
<td> Both implicit and explicit functions
</td>
<td> Only explicit functions
</td></tr>
<tr>
<td style="background:#C9C9C9"> <b>Weight</b>
</td>
<td> Support both x weight and y weight
</td>
<td> Support only y weight
</td></tr>
<tr>
<td style="background:#C9C9C9"> <b>Residual Source</b>
</td>
<td> The orthogonal distance from the data to the fitted curve
</td>
<td> The difference between the observed value and the predicted value
</td></tr>
<tr>
<td style="background:#C9C9C9"> <b>Iteration Process</b>
</td>
<td> Adjusting the values of fitting parameters and independent variables
</td>
<td> Adjusting the values of fitting parameters
</td></tr></table>
<dl><dd><a  class="image"><img alt="LM vs ODR 85pc.png" src="../images/Theory_of_Nonlinear_Curve_Fitting/LM_vs_ODR_85pc.png?v=84925" width="799"  /></a></dd></dl>
<h3><a name="Implicit_Functions"></a><span class="mw-headline">Implicit Functions</span></h3>
<h4><a name="Fitting_Model_2"></a><span class="mw-headline">Fitting Model</span></h4>
<p>A general implicit function could be expressed as:
</p>
<table class="formula">

<tr>
<td> <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-2c70edb6aaa03e8925fa8e82fb079995.png?v=0" title="f\left ( X, Y, \beta \right )-const=0 " alt="f\left ( X, Y, \beta \right )-const=0 " class="tex"/>
</td>
<td> (5)
</td></tr></table>
<p>where <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-35f21cd4e2e75c09315e49b2b031710b.png?v=0" title="X = (x_1, x_2, \cdots , x_k)&#39;" alt="X = (x_1, x_2, \cdots , x_k)&#39;" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-efc29c0295bfc803602e300317eb1204.png?v=0" title="Y = (y_1, y_2, \cdots , y_k)&#39;" alt="Y = (y_1, y_2, \cdots , y_k)&#39;" class="tex"/> are the variables, <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-b0603860fcffe94e5b8eec59ed813421.png?v=0" title="\beta" alt="\beta" class="tex"/> are the fitting parameters and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-6680dba00f3a88f66f8029a93d71d93c.png?v=0" title="const" alt="const" class="tex"/> is a constant.
</p>
<table class="noborder"><tr>
	<td style="vertical-align:top" width="60"><img src="../images/Theory_of_Nonlinear_Curve_Fitting/Tip_icon.png?v=9614" width="57"  border="0" /></td><td><p><b>Examples of the Implicit Function:</b>
</p>
<ul><li> <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-d45d2cae23282528539e4f2dccaa0047.png?v=0" title="f = \left(\frac{x-x_c}{a}\right)^2 + \left(\frac{y-y_c}{b}\right)^2 - 1" alt="f = \left(\frac{x-x_c}{a}\right)^2 + \left(\frac{y-y_c}{b}\right)^2 - 1" class="tex"/></li></ul>
<p><br />
</p></td></tr></table>
<h4><a name="Orthogonal_Distance_Regression_.28ODR.29_Algorithm_2"></a><span class="mw-headline">Orthogonal Distance Regression (ODR) Algorithm</span></h4>
<p>The ODR method can be used for both implicit functions and explicit functions. To learn more details of ODR method, please refer to the description of ODR mehtod in <a href="#Orthogonal_Distance_Regression_.28ODR.29_Algorithm">above section</a>
</p><p>For implicit functions, the ODR algorithm could be expressed as:
</p><p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-a4443476c7d74800f2fd5e4c90a79988.png?v=0" title="\min\left (\sum_{i=1}^{n}\left ( w_{xi}\cdot \delta_{xi}^{2}+w_{yi}\cdot \delta_{yi}^{2} \right )  \right )" alt="\min\left (\sum_{i=1}^{n}\left ( w_{xi}\cdot \delta_{xi}^{2}+w_{yi}\cdot \delta_{yi}^{2} \right )  \right )" class="tex"/>
</p><p>subject to:
</p><p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-75d51c5f694e13f570e6ee14eab0fcf7.png?v=0" title="f\left ( x_{i}+\delta_{xi},y_{i}+\delta_{yi},\beta \right )= 0\ \ \ \ \ \ i=1,...,n" alt="f\left ( x_{i}+\delta_{xi},y_{i}+\delta_{yi},\beta \right )= 0\ \ \ \ \ \ i=1,...,n" class="tex"/> 
</p><p>where <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f0ed6a7e9e5079b9b9f2bcd01768ddcc.png?v=0" title="w_{xi}" alt="w_{xi}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-6f64afa2febc1a132beeeba5e4bd81ef.png?v=0" title="w_{yi}" alt="w_{yi}" class="tex"/> are the user input weights of <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-05e42209d67fe1eb15a055e9d3b3770e.png?v=0" title="x_{i}" alt="x_{i}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-18daef71b5d25ce76b8628a81e4fc76b.png?v=0" title="y_{i}" alt="y_{i}" class="tex"/>, <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-2543a24cee2f57e100a071caebc57326.png?v=0" title="\delta_{xi}" alt="\delta_{xi}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-37644270c5e8eefafd1d93fd743e6bbc.png?v=0" title="\delta_{yi}" alt="\delta_{yi}" class="tex"/> are the residual of the corresponding <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-05e42209d67fe1eb15a055e9d3b3770e.png?v=0" title="x_{i}" alt="x_{i}" class="tex"/> and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-18daef71b5d25ce76b8628a81e4fc76b.png?v=0" title="y_{i}" alt="y_{i}" class="tex"/>, and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-b0603860fcffe94e5b8eec59ed813421.png?v=0" title="\beta" alt="\beta" class="tex"/> is the fitting parameter.
</p>
<h3><a name="Weighted_Fitting"></a><span class="mw-headline">Weighted Fitting</span></h3>
<p>When the measurement errors are unknown, <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-86d759d3081512d7183f5d7b87a1e825.png?v=0" title="\sigma _i\,\!" alt="\sigma _i\,\!" class="tex"/> are set to 1 for all <i>i</i>, and the curve fitting is performed without weighting. However, when the experimental errors are known, we can treat these errors as weights and use weighted fitting.  In this case, the chi-square can be written as:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-d8212e6e4627f94f8d8ca2a1ede630e2.png?v=0" title="\chi ^2=\sum_{i=1}^nw_i[Y_i-f(x_i^{\prime },\hat \beta )]^2" alt="\chi ^2=\sum_{i=1}^nw_i[Y_i-f(x_i^{\prime },\hat \beta )]^2" class="tex"/>
</p>
</td>
<td>
<p>(6)  
</p>
</td></tr></table>
<p>There are a number of weighting methods available in Origin.  Please read <a href="../../UserGuide/UserGuide/Fitting_with_Errors_and_Weighting.html" title="UserGuide:Fitting with Errors and Weighting">Fitting with Errors and Weighting</a> in the Origin Help file for more details.
</p>
<h2><a name="Parameters"></a><span class="mw-headline">Parameters</span></h2>
<p>The fit-related formulas are summarized here:
</p>
<dl><dd><a  class="image"><img alt="The Fit Results.png" src="../images/Theory_of_Nonlinear_Curve_Fitting/The_Fit_Results.png?v=30353" width="654"  /></a></dd></dl>
<h3><a name="The_Fitted_Value"></a><span class="mw-headline">The Fitted Value</span></h3>
<p>Computing the fitted values in nonlinear regression is an iterative procedure.  You can read a brief introduction in the above section (<a href="#How_Origin_Fits_the_Curve">How Origin Fits the Curve</a>), or see the below-referenced material for more detailed information.
</p>
<h3><a name="Parameter_Standard_Errors"></a><span class="mw-headline">Parameter Standard Errors</span></h3>
<p>During L-M iteration, we need to calculate the partial derivatives matrix <b><i>F</i></b>, whose element in <i>i</i>th row and <i>j</i>th column is:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-7df4a0ef76e047f4b002d1b4628f7a6d.png?v=0" title="F_{ij}=\frac{\partial f(x,\theta )}{\sigma _i\partial \theta _j}" alt="F_{ij}=\frac{\partial f(x,\theta )}{\sigma _i\partial \theta _j}" class="tex"/>  
</p>
</td>
<td>
<p>(7) 
</p>
</td></tr></table>
<p>where <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-571f263e771aca60a4a284b14251565c.png?v=0" title="\sigma _i" alt="\sigma _i" class="tex"/> is the error of y for the <i>i</i>th observation if <b>Instrumental</b> weight is used. If there is no weight, <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-ece93aac8aa97d60ece2b228ea95b246.png?v=0" title="\sigma _i = 1" alt="\sigma _i = 1" class="tex"/>. And <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-ba404f144da76afb0ad9ee47351ed6bf.png?v=0" title="F_{ij}" alt="F_{ij}" class="tex"/> is evaluated for each observation <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-1ba8aaab47179b3d3e24b0ccea9f4e30.png?v=0" title="x_i" alt="x_i" class="tex"/> in each iteration. 
</p><p>Then we can get the <b>Variance-Covariance</b> Matrix for parameters <a  class="image"><img alt="Temp-Regression and Curve Fitting-image124.gif" src="../images/Theory_of_Nonlinear_Curve_Fitting/Temp-Regression_and_Curve_Fitting-image124.gif?v=10965" width="15"  /></a> by:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-b468175c71479a7b2b5bc140c0d13325.png?v=0" title="C=(F&#39;F)^{-1}s^2\,\!" alt="C=(F&#39;F)^{-1}s^2\,\!" class="tex"/> 
</p>
</td>
<td>
<p>(8)  
</p>
</td></tr></table>
<p>where <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-5df7e7b0f9a17b149f36fec7c1142e01.png?v=0" title="F&#39;" alt="F&#39;" class="tex"/> is the transpose of the <i>F</i> matrix, s<sup>2</sup> is the mean residual variance, also called <b>Reduced Chi-Sqr</b>, or the <b>Deviation</b> of the Model, and can be calculated as follows:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-135afe5aa195767729555499d3c7aff8.png?v=0" title="s^2=\frac{RSS}{n-p}" alt="s^2=\frac{RSS}{n-p}" class="tex"/>
</p>
</td>
<td>
<p>(9)  
</p>
</td></tr></table>
<p>where <i>n</i> is the number of points, and <i>p</i> is the number of parameters.
</p><p>The square root of the main diagonal value of this matrix <i>C</i> is the <b>Standard Error</b> of the corresponding parameter
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-1bd9c4721fff6c989d8b8284ea46193c.png?v=0" title="s_{\theta _i}=\sqrt{c_{ii}}\,\!" alt="s_{\theta _i}=\sqrt{c_{ii}}\,\!" class="tex"/> 
</p>
</td>
<td>
<p>(10)  
</p>
</td></tr></table>
<p>where <b><i>C</i></b><sub>ii</sub> is the element in <i>i</i>th row and <i>i</i>th column of the matrix <b><i>C</i></b>. <b><i>C</i></b><sub>ij</sub> is the covariance between <i>&#952;</i><sub>i</sub> and <i>&#952;</i><sub>j</sub>.
</p><p>You can choose whether to exclude s<sup>2</sup> when calculating the covariance matrix.  This will affect the Standard Error values. When excluding s<sup>2</sup>, clear the <b>Use reduce Chi-Sqr</b> check box on the <b>Advanced</b> page under <b>Fit Control</b> panel.  The covariance is then calculated by:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-7ba5bc942d74535e881bac8fffef8e0d.png?v=0" title="c=(F&#39;F)^{-1}\,\!" alt="c=(F&#39;F)^{-1}\,\!" class="tex"/> 
</p>
</td>
<td>
<p>(11)  
</p>
</td></tr></table>
<p>So the Standard Error now becomes:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-45acf7fd19d29633c1814cab358125b4.png?v=0" title="s_{\theta _i}^{\prime }=\frac{s_{\theta _i}}s\,\!" alt="s_{\theta _i}^{\prime }=\frac{s_{\theta _i}}s\,\!" class="tex"/> 
</p>
</td>
<td>
<p>(12)  
</p>
</td></tr></table>
<p>The parameter standard errors can give us an idea of the precision of the fitted values.  Typically, the magnitude of the standard error values should be lower than the fitted values.  If the standard error values are much greater than the fitted values, the fitting model may be overparameterized.
</p>
<h3><a name="The_Standard_Error_for_Derived_Parameter"></a><span class="mw-headline">The Standard Error for Derived Parameter</span></h3>
<p>Origin estimates the standard errors for the derived parameters according to the Error Propagation formula, which is an approximate formula.
</p><p>Let <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-9886143d781b18fae944d111614266b3.png?v=0" title="z = f\left (\theta _1, \theta _2, ..., \theta _p \right )" alt="z = f\left (\theta _1, \theta _2, ..., \theta _p \right )" class="tex"/> be the function with a combination (linear or non-linear) of <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-5a34bb082daf037b3c4b14c13af6855b.png?v=0" title="p\," alt="p\," class="tex"/> variables <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-4a096a413b1ea45201f320996396be49.png?v=0" title="\theta _1, \theta _2, ..., \theta _p \," alt="\theta _1, \theta _2, ..., \theta _p \," class="tex"/>.  
</p><p>The general law of error propagation is:
</p>
<dl><dd><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-a1a4895080bf724c87cfcd4c7b64292f.png?v=0" title="\sigma_z^2 = \sum_i^p \sum_j^p \frac {\partial z}{\partial \theta_i} COV_{\theta_i \theta_j} \frac {\partial z}{\partial \theta_j}" alt="\sigma_z^2 = \sum_i^p \sum_j^p \frac {\partial z}{\partial \theta_i} COV_{\theta_i \theta_j} \frac {\partial z}{\partial \theta_j}" class="tex"/></dd></dl>
<p>where  <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f3bfa311268b39ae3ce3ae42e47f6937.png?v=0" title="COV_{\theta_i \theta_j}\," alt="COV_{\theta_i \theta_j}\," class="tex"/> is the covariance value for <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f633d81011ebbdb486d3bfee35e4789e.png?v=0" title="\left (\theta_i, \theta_j \right )" alt="\left (\theta_i, \theta_j \right )" class="tex"/>, and <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-edb838306186ccc2230aa890ba0bad64.png?v=0" title="\left (i = 1, 2, ..., p \right ), \left (j = 1, 2, ..., p \right )" alt="\left (i = 1, 2, ..., p \right ), \left (j = 1, 2, ..., p \right )" class="tex"/>. 
</p><p>You can choose whether to exclude mean residual variance <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-e25a9ed7a5b522d46c98a8813634ab5d.png?v=0" title="s^2" alt="s^2" class="tex"/> when calculating the covariance matrix  <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-94e8fbdc16330d396a135eb8ff7c399e.png?v=0" title="COV_{\theta_i \theta_j}" alt="COV_{\theta_i \theta_j}" class="tex"/>, which affects the Standard Error values  for derived parameters. When excluding <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-e25a9ed7a5b522d46c98a8813634ab5d.png?v=0" title="s^2" alt="s^2" class="tex"/>, clear the <b>Use reduce Chi-Sqr</b> check box on the <b>Advanced</b> page under <b>Fit Control</b> panel. 
</p><p>For example, using three variables
</p><p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-5a9033d9c4d88c1ee1a786cbfbbf1d27.png?v=0" title="z = f\left (\theta_1, \theta_2, \theta_3 \right )" alt="z = f\left (\theta_1, \theta_2, \theta_3 \right )" class="tex"/>
</p><p>we get:
</p>
<dl><dd><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-37a2eeec5b384e170c8c1a0ddda74e60.png?v=0" title="\sigma_z^2 = \left (\frac {\partial z}{\partial \theta_1} \right )^2 \sigma_{\theta_1}^2 + \left (\frac {\partial z}{\partial \theta_2} \right )^2 \sigma_{\theta_2}^2 + \left (\frac {\partial z}{\partial \theta_3} \right )^2 \sigma_{\theta_3}^2 + 2 \left (\frac {\partial z}{\partial \theta_1} \frac {\partial z}{\partial \theta_2} \right ) COV_{\theta_1 \theta_2} + 2 \left (\frac {\partial z}{\partial \theta_1} \frac {\partial z}{\partial \theta_3} \right ) COV_{\theta_1 \theta_3} + 2 \left (\frac {\partial z}{\partial \theta_2} \frac {\partial z}{\partial \theta_3} \right ) COV_{\theta_2 \theta_3}&#10;" alt="\sigma_z^2 = \left (\frac {\partial z}{\partial \theta_1} \right )^2 \sigma_{\theta_1}^2 + \left (\frac {\partial z}{\partial \theta_2} \right )^2 \sigma_{\theta_2}^2 + \left (\frac {\partial z}{\partial \theta_3} \right )^2 \sigma_{\theta_3}^2 + 2 \left (\frac {\partial z}{\partial \theta_1} \frac {\partial z}{\partial \theta_2} \right ) COV_{\theta_1 \theta_2} + 2 \left (\frac {\partial z}{\partial \theta_1} \frac {\partial z}{\partial \theta_3} \right ) COV_{\theta_1 \theta_3} + 2 \left (\frac {\partial z}{\partial \theta_2} \frac {\partial z}{\partial \theta_3} \right ) COV_{\theta_2 \theta_3}&#10;" class="tex"/></dd></dl>
<p><br />
Now, let the derived parameter be <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-77698ae92ac0435f8da1e266eeb528e3.png?v=0" title="z\," alt="z\," class="tex"/>, and let the fitting parameters be <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-2170397abaa4c195e1337c282185dacb.png?v=0" title="\theta_1, \theta_2, ..., \theta_p\," alt="\theta_1, \theta_2, ..., \theta_p\," class="tex"/>.  The standard error for the derived parameter <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-77698ae92ac0435f8da1e266eeb528e3.png?v=0" title="z\," alt="z\," class="tex"/> is <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-62f8edd0067312992b259e2cea2f98fa.png?v=0" title="\sigma_z\," alt="\sigma_z\," class="tex"/>.
</p>
<h3><a name="Confidence_Intervals"></a><span class="mw-headline">Confidence Intervals</span></h3>
<p>Origin provides two methods to calculate the confidence intervals for parameters: <a href="../../UserGuide/UserGuide/Settings_Tab_Upper_Panel.html#Advanced" title="UserGuide:Settings Tab Upper Panel">Asymptotic-Symmetry method and Model-Comparison method</a>.
</p>
<h5><a name="Asymptotic-Symmetry_Method"></a><span class="mw-headline">Asymptotic-Symmetry Method</span></h5>
<p>One assumption in regression analysis is that data is normally distributed, so we can use the standard error values to construct the <b>Parameter Confidence Intervals</b>. For a given significance level, &#945;, the (1-&#945;)x100% confidence interval for the parameter is:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f9deaed4f0f2b676163099e46b6375b1.png?v=0" title="\hat \theta _j-t_{(\frac \alpha 2,n-p)}s_{\theta _j}\leq \hat \theta _j\leq \hat \theta _j+t_{(\frac \alpha 2,n-p)}s_{\theta _j}" alt="\hat \theta _j-t_{(\frac \alpha 2,n-p)}s_{\theta _j}\leq \hat \theta _j\leq \hat \theta _j+t_{(\frac \alpha 2,n-p)}s_{\theta _j}" class="tex"/> 
</p>
</td>
<td>
<p>(13)  
</p>
</td></tr></table>
<p>The parameter confidence interval indicates how likely the interval is to contain the true value.
</p><p>The confidence interval illustrated above is <b>Asymptotic</b>, which is the most frequently used method to calculate the confidence interval. The "Asymptotic" here means it is an approximate value. 
</p>
<h5><a name="Model-Comparison_Method"></a><span class="mw-headline">Model-Comparison Method</span></h5>
<p>If you need more accurate values, you can use the <b>Model Comparison Based</b> method to estimate the confidence interval.
</p><p>If the Model Comparison method is used, the upper and lower confidence limits will be calculated by searching for the values of each parameter <i>p</i> that makes <i>RSS(&#952;<sub>j</sub>)</i> (minimized over the remaining parameters) greater than <i>RSS</i> by a factor of (<i>1+F</i>/(<i>n-p</i>)).
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-e4335f63c75b0341e9888b260ef77e7f.png?v=0" title="RSS(\theta _j)=RSS(1+F\frac 1{n-p})" alt="RSS(\theta _j)=RSS(1+F\frac 1{n-p})" class="tex"/> 
</p>
</td>
<td>
<p>(14)  
</p>
</td></tr></table>
<p>where <i>F = Ftable</i>(<i>&#945;,1,n-p</i>)and <i>RSS</i> is the minimum residual sum of square found during the fitting session.
</p>
<h3><a name="t_Value"></a><span class="mw-headline">t Value</span></h3>
<p>You can choose to perform a <i>t</i>-test on each parameter to see whether its value is equal to 0.  The null hypothesis of the t-test on the <i>j</i>th parameter is:
</p>
<table class="formula">

<tr>
<td> <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-e1506f5dbd3c5df22513df890dba20fb.png?v=0" title="H_0: \theta_j = 0 \," alt="H_0: \theta_j = 0 \," class="tex"/>
</td>
<td>
</td></tr></table>
<p>And the alternative hypothesis is:
</p>
<table class="formula">

<tr>
<td> <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-11a56a7d0ca6131863575566cc7117c9.png?v=0" title="H_\alpha&#160;: \theta_j \ne 0" alt="H_\alpha&#160;: \theta_j \ne 0" class="tex"/>
</td>
<td>
</td></tr></table>
<p>The <i>t</i>-value can be computed as:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-7ddf34c66dcaeadff80cfc8fe6a519b4.png?v=0" title="t=\frac{\hat \beta _j-0}{s_{\hat \beta _j}}" alt="t=\frac{\hat \beta _j-0}{s_{\hat \beta _j}}" class="tex"/> 
</p>
</td>
<td>
<p>(15)  
</p>
</td></tr></table>
<h3><a name="Prob.3E.7Ct.7C"></a><span class="mw-headline">Prob&gt;|t|</span></h3>
<p>The probability that <i>H</i><sub>0</sub> in the <i>t</i> test above is true.
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-3e5335a131705b3ef0bb8b250033167d.png?v=0" title="prob=2(1-tcdf(|t|,df_{Error}))\,\!" alt="prob=2(1-tcdf(|t|,df_{Error}))\,\!" class="tex"/> 
</p>
</td>
<td>
<p>(16)  
</p>
</td></tr></table>
<p>where <i>tcdf(t, df)</i> computes the lower tail probability for Student's <i>t</i> distribution with <i>df</i> degree of freedom.
</p>
<h3><a name="Dependency"></a><span class="mw-headline">Dependency</span></h3>
<p>If the equation is overparameterized, there will be mutual dependency between parameters. The dependency for the <i>i</i>th parameter is defined as:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-20b1f9a2fd5f5c4f53e55b4d9e537e8d.png?v=0" title="1-\frac 1{c_{ii}(c^{-1})_{ii}}" alt="1-\frac 1{c_{ii}(c^{-1})_{ii}}" class="tex"/> 
</p>
</td>
<td>
<p>(17)  
</p>
</td></tr></table>
<p>and (<b><i>C</i></b><sup>-1</sup>)<sub>ii</sub> is the (<i>i</i>, <i>i</i>)th diagonal element of the inverse of matrix <b><i>C</i></b>. If this value is close to 1, there is strong dependency. 
</p><p>To learn more about how the value assess the quality of a fit model, see <a href="../../UserGuide/UserGuide/Model_Diagnosis_Using_Dependency_Values.html" title="UserGuide:Model Diagnosis Using Dependency Values">Model Diagnosis Using Dependency Values</a> page
</p>
<h3><a name="CI_Half_Width"></a><span class="mw-headline">CI Half Width</span></h3>
<p>The Confidence Interval Half Width is:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-77ec7c61ac9d06bfc9ad4c0ae47b85ce.png?v=0" title="CI=\frac{UCL-LCL}2" alt="CI=\frac{UCL-LCL}2" class="tex"/> 
</p>
</td>
<td>
<p>(18)  
</p>
</td></tr></table>
<p>where UCL and LCL is the Upper Confidence Interval and Lower Confidence Interval, respectively.
</p>
<h2><a name="Statistics"></a><span class="mw-headline">Statistics</span></h2>
<p>Several fit statistics formulas are summarized below:
</p><p><a  class="image"><img alt="The Fit Results 02.png" src="../images/Theory_of_Nonlinear_Curve_Fitting/The_Fit_Results_02.png?v=30352" width="252"  /></a>
</p>
<h3><a name="Degree_of_Freedom"></a><span class="mw-headline">Degree of Freedom</span></h3>
<p>The <i>Error</i> degree of freedom. Please refer to the <a href="#ANOVA_Table">ANOVA Table</a> for more details.
</p>
<h3><a name="Residual_Sum_of_Squares"></a><span class="mw-headline">Residual Sum of Squares</span></h3>
<p>The residual sum of squares:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-098376be8e9d0c82d36905f8384a9f90.png?v=0" title="RSS(X,\hat \theta )=\sum_{i=1}^n w_i[Y_i-f(x_i^{\prime },\hat \theta )]^2" alt="RSS(X,\hat \theta )=\sum_{i=1}^n w_i[Y_i-f(x_i^{\prime },\hat \theta )]^2" class="tex"/>
</p>
</td>
<td>
<p>(19)  
</p>
</td></tr></table>
<h3><a name="Reduced_Chi-Sqr"></a><span class="mw-headline">Reduced Chi-Sqr</span></h3>
<p>The Reduced Chi-square value, which equals the residual sum of square divided by the degree of freedom.
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-b8e3b965a160ce8b28ff9f67113f6a4a.png?v=0" title="Reduced\chi ^2=\frac{\chi ^2}{df_{Error}}=\frac{RSS}{df_{Error}}" alt="Reduced\chi ^2=\frac{\chi ^2}{df_{Error}}=\frac{RSS}{df_{Error}}" class="tex"/> 
</p>
</td>
<td>
<p>(20)  
</p>
</td></tr></table>
<h3><a name="R-Square_.28COD.29"></a><span class="mw-headline">R-Square (COD)</span></h3>
<p>The <i>R</i><sup>2</sup> value shows the goodness of a fit, and can be computed by:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-85527db992f009b94c01e40bd6650d3d.png?v=0" title="R^2=\frac{Explained\,variation}{Total\,variation}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}" alt="R^2=\frac{Explained\,variation}{Total\,variation}=\frac{TSS-RSS}{TSS}=1-\frac{RSS}{TSS}" class="tex"/> 
</p>
</td>
<td>
<p>(21)  
</p>
</td></tr></table>
<p>where <i>TSS</i> is the total sum of square, and <i>RSS</i> is the residual sum of square.
</p>
<h3><a name="Adj._R-Square"></a><span class="mw-headline">Adj. R-Square</span></h3>
<p>The adjusted <i>R</i><sup>2</sup> value:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-3dce278823ecfde5fe7c017ab3681a62.png?v=0" title="\bar R^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" alt="\bar R^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" class="tex"/> 
</p>
</td>
<td>
<p>(22)  
</p>
</td></tr></table>
<h3><a name="R_Value"></a><span class="mw-headline">R Value</span></h3>
<p>The <i>R</i> value is the square root of <i>R</i><sup>2</sup>:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-8dea6550a6f24458c884bb59c192c935.png?v=0" title="R=\sqrt{R^2}" alt="R=\sqrt{R^2}" class="tex"/> 
</p>
</td>
<td>
<p>(23)  
</p>
</td></tr></table>
<p>For more information on <i>R</i><sup>2</sup>, adjusted <i>R</i><sup>2</sup> and <i>R</i>, please see <a href="../../UserGuide/Category/Interpreting_Regression_Results.html#Goodness_of_Fit" title="Category:Interpreting Regression Results">Goodness of Fit</a>.
</p>
<h3><a name="Root-MSE_.28SD.29"></a><span class="mw-headline">Root-MSE (SD)</span></h3>
<p>Root mean square of the error, or the <b>Standard Deviation</b> of the residuals, equal to the square root of reduced &#967;<sup>2</sup>:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-54a849a10ec028930b7e06f130fd19fe.png?v=0" title="Root\,MSE=\sqrt{Reduced \,\chi ^2}" alt="Root\,MSE=\sqrt{Reduced \,\chi ^2}" class="tex"/> 
</p>
</td>
<td>
<p>(24)  
</p>
</td></tr></table>
<h2><a name="ANOVA_Table"></a><span class="mw-headline">ANOVA Table</span></h2>
<p>The ANOVA Table:
</p><p><b>Note:</b> The <b>ANOVA table</b> is not available for implicit function fitting.  
</p>
<table class="simple">
<tr>
<th>
</th>
<th> df
</th>
<th> Sum of Squares
</th>
<th> Mean Square
</th>
<th> F Value
</th>
<th> Prob &gt; F
</th></tr>
<tr>
<th> Model
</th>
<td>
<p><i>p</i>
</p>
</td>
<td>
<p><i>SS</i><sub>reg</sub> = <i>TSS</i> - <i>RSS</i>
</p>
</td>
<td>
<p><i>MS</i><sub>reg</sub> = <i>SS</i><sub>reg</sub> / <i>p</i>
</p>
</td>
<td>
<p><i>MS</i><sub>reg</sub> / <i>MSE</i>
</p>
</td>
<td>
<p><i>p</i>-<i>value</i>
</p>
</td></tr>
<tr>
<th> Error
</th>
<td>
<p><i>n</i> - <i>p</i>
</p>
</td>
<td>
<p><i>RSS</i>
</p>
</td>
<td>
<p><i>MSE</i> = <i>RSS</i> / (<i>n</i> - <i>p</i>)
</p>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<th> Uncorrected Total
</th>
<td>
<p><i>n</i>
</p>
</td>
<td>
<p><i>TSS</i>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<th> Corrected Total
</th>
<td>
<p><i>n-1</i>
</p>
</td>
<td>
<p><i>TSS</i><sub>corrected</sub>
</p>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></table>
<p><b>Note:</b> In nonlinear fitting, Origin outputs both corrected and uncorrected total sum of squares:
Corrected model:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-88dbe70110d96ad89fd62c2370948912.png?v=0" title="TSS_{corrected}=\sum_{i=1}w_{i}y_{i}^2-\left(\sum_{i=1}\left(y_{i}w_{i} \right )/\sum_{i=1}w_{i} \right )^2\sum_{i=1}w_{i}" alt="TSS_{corrected}=\sum_{i=1}w_{i}y_{i}^2-\left(\sum_{i=1}\left(y_{i}w_{i} \right )/\sum_{i=1}w_{i} \right )^2\sum_{i=1}w_{i}" class="tex"/> 
</p>
</td>
<td>
<p>(25)  
</p>
</td></tr></table>
<p>Uncorrected model:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-5fccc733e8b665c322ce58f630c39be1.png?v=0" title="TSS=\sum_{i=1}^nw_iy_i^2" alt="TSS=\sum_{i=1}^nw_iy_i^2" class="tex"/> 
</p>
</td>
<td>
<p>(26)  
</p>
</td></tr></table>
<p>The F value here is a test of whether the fitting model differs significantly from the model y=constant. Additionally, the <i>p</i>-value, or significance level, is reported with an <i>F</i>-test. We can reject the null hypothesis if the <i>p</i>-value is less than <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>, which means that the fitting model differs significantly from the model y=constant.
</p>
<h2><a name="Confidence_and_Prediction_Bands"></a><span class="mw-headline">Confidence and Prediction Bands</span></h2>
<h3><a name="Confidence_Band"></a><span class="mw-headline">Confidence Band</span></h3>
<p>The confidence interval for the fitting function says how good your estimate of the value of the fitting function is at particular values of the independent variables. You can claim with 100&#945;% confidence that the correct value for the fitting function lies within the confidence interval, where &#945; is the desired level of confidence. This defined confidence interval for the fitting function is computed as:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-82c86845ee3d0efa23d2a1ce2450a05f.png?v=0" title="f(x_{1i},x_{2i},\ldots&#160;;\theta _{1i},\theta _{2i},\ldots )\pm t_{(\frac \alpha 2,dof)}[s^2fcf^{\prime }]^{\frac 12}" alt="f(x_{1i},x_{2i},\ldots&#160;;\theta _{1i},\theta _{2i},\ldots )\pm t_{(\frac \alpha 2,dof)}[s^2fcf^{\prime }]^{\frac 12}" class="tex"/>
</p>
</td>
<td>
<p>(27)  
</p>
</td></tr></table>
<p>where:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-f04def5c58d56a887955d9132f42b968.png?v=0" title="f=[\frac{\partial f}{\partial \theta _1},\frac{\partial f}{\partial \theta _2},\cdots ,\frac{\partial f}{\partial \theta _p}]" alt="f=[\frac{\partial f}{\partial \theta _1},\frac{\partial f}{\partial \theta _2},\cdots ,\frac{\partial f}{\partial \theta _p}]" class="tex"/> 
</p>
</td>
<td>
<p>(28)  
</p>
</td></tr></table>
<h3><a name="Prediction_Band"></a><span class="mw-headline">Prediction Band</span></h3>
<p>The prediction interval for the desired confidence level &#945; is the interval within which 100&#945;% of all the experimental points in a series of repeated measurements are expected to fall at particular values of the independent variables. This defined prediction interval for the fitting function is computed as:
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-29ebc8680acd0d1962dee476a98e9926.png?v=0" title="f(x_{1i},x_{2i},\ldots&#160;;\theta _{1i},\theta _{2i},\ldots )\pm t_{(\frac \alpha 2,dof)}[s^2(1+fcf^{\prime })]^{\frac 12}" alt="f(x_{1i},x_{2i},\ldots&#160;;\theta _{1i},\theta _{2i},\ldots )\pm t_{(\frac \alpha 2,dof)}[s^2(1+fcf^{\prime })]^{\frac 12}" class="tex"/> 
</p>
</td>
<td>
<p>(29)  
</p>
</td></tr></table>
<p>where
</p><p><img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-4f08a0e1ff8ccb6f9fa6b1bf64d3f75f.png?v=0" title="\chi _*^2" alt="\chi _*^2" class="tex"/>  is Reduced <img src="../images/Theory_of_Nonlinear_Curve_Fitting/math-ea14eb92a09af346481a999c310094f2.png?v=0" title="\chi ^2" alt="\chi ^2" class="tex"/>
</p>
<table class="note">

<tr>
<td><b>Notes:</b> The <b>Confidence Band</b> and <b>Prediction Band</b> in the fitted curve plot are not available for implicit function fitting.
</td></tr></table>
<h2><a name="Topics_for_Further_Reading"></a><span class="mw-headline">Topics for Further Reading</span></h2>
<ul><li> <a href="../../UserGuide/UserGuide/Quick_Start.html" title="UserGuide:Quick Start">Nonlinear Curve Fitting: Quick Start</a></li>
<li> <a href="../../UserGuide/Category/The_NLFit_Dialog_Box.html" title="Category:The NLFit Dialog Box">The Nonlinear Curve Fitting Dialog</a></li>
<li> <a href="../../UserGuide/Category/Interpreting_Regression_Results.html" title="Category:Interpreting Regression Results">Interpreting Regression Results</a></li></ul>
<h2><a name="Reference"></a><span class="mw-headline">Reference</span></h2>
<ol><li>William. H. Press, etc. <i>Numerical Recipes in C++</i>. Cambridge University Press, 2002.</li>
<li>Norman R. Draper, Harry Smith. <i>Applied Regression Analysis</i>, Third Edition. John Wiley &amp; Sons, Inc. 1998.</li>
<li>George Casella, et al. <i>Applied Regression Analysis: A Research Tool</i>, Second Edition. Springer-Verlag New York, Inc. 1998.</li>
<li>G. A. F. Seber, C. J. Wild. <i>Nonlinear Regression</i>. John Wiley &amp; Sons, Inc. 2003.</li>
<li>David A. Ratkowsky. <i>Handbook of Nonlinear Regression Models</i>. Marcel Dekker, Inc. 1990.</li>
<li>Douglas M. Bates, Donald G. Watts. <i>Nonlinear Regression Analysis &amp; Its Applications</i>. John Wiley &amp; Sons, Inc. 1988.</li>
<li>Marko Ledvij. <i>Curve Fitting Made Easy</i>. The Industrial Physicist. Apr./May 2003. 9:24-27.</li>
<li>"J. W. Zwolak, P.T. Boggs, and L.T. Watson, ``Algorithm 869: ODRPACK95: A weighted orthogonal distance regression code with bound constraints<i>, ACM Transactions on Mathematical Software Vol. 33, Issue 4, August 2007."</i></li>
<li> Nelder, J.A., and R. Mead.  1965.  <i>Computer Journal</i>, vol. 7, pp. 308 -313</li>
<li> <i>Numerical Recipes in C</i>, Ch. 10.4, Downhill Simplex Method in Multidimensions.</li>
<li> <i>Numerical Recipes in C</i>, Ch. 15.5, Nonlinear Models.</li></ol>


