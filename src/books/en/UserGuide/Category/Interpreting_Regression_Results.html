<h1 class="firstHeading">15.4 Interpreting Regression Results</h1><p class='urlname' style='display: none'>Interpret-Regression-Result</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#In_Parameters_Table"><span class="tocnumber">1</span> <span class="toctext">In Parameters Table</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Value"><span class="tocnumber">1.1</span> <span class="toctext">Value</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Standard_Error"><span class="tocnumber">1.2</span> <span class="toctext">Standard Error</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#t-Value"><span class="tocnumber">1.3</span> <span class="toctext">t-Value</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Prob.3E.7Ct.7C"><span class="tocnumber">1.4</span> <span class="toctext">Prob&gt;|t|</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#LCL_and_UCL_.28Parameter_Confidence_Interval.29"><span class="tocnumber">1.5</span> <span class="toctext">LCL and UCL (Parameter Confidence Interval)</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Dependency"><span class="tocnumber">1.6</span> <span class="toctext">Dependency</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-8"><a href="#In_Statistics_Table"><span class="tocnumber">2</span> <span class="toctext">In Statistics Table</span></a>
<ul>
<li class="toclevel-2 tocsection-9"><a href="#Residual_Sum_of_Squares"><span class="tocnumber">2.1</span> <span class="toctext">Residual Sum of Squares</span></a></li>
<li class="toclevel-2 tocsection-10"><a href="#Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"><span class="tocnumber">2.2</span> <span class="toctext">Scale Error with sqrt(Reduced Chi-Sqr)</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Pearson.27s_r"><span class="tocnumber">2.3</span> <span class="toctext">Pearson's r</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#R-Square_.28COD.29"><span class="tocnumber">2.4</span> <span class="toctext">R-Square (COD)</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Adj._R-Square"><span class="tocnumber">2.5</span> <span class="toctext">Adj. R-Square</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-14"><a href="#In_ANOVA_Table"><span class="tocnumber">3</span> <span class="toctext">In ANOVA Table</span></a>
<ul>
<li class="toclevel-2 tocsection-15"><a href="#F_Value"><span class="tocnumber">3.1</span> <span class="toctext">F Value</span></a></li>
<li class="toclevel-2 tocsection-16"><a href="#Prob.3EF"><span class="tocnumber">3.2</span> <span class="toctext">Prob&gt;F</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-17"><a href="#In_Covariance_and_Correlation_Table"><span class="tocnumber">4</span> <span class="toctext">In Covariance and Correlation Table</span></a>
<ul>
<li class="toclevel-2 tocsection-18"><a href="#Covariance"><span class="tocnumber">4.1</span> <span class="toctext">Covariance</span></a></li>
<li class="toclevel-2 tocsection-19"><a href="#Correlation"><span class="tocnumber">4.2</span> <span class="toctext">Correlation</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-20"><a href="#In_Fitted_Curves_Plot"><span class="tocnumber">5</span> <span class="toctext">In Fitted Curves Plot</span></a></li>
<li class="toclevel-1 tocsection-21"><a href="#In_Residual_Plot"><span class="tocnumber">6</span> <span class="toctext">In Residual Plot</span></a></li>
<li class="toclevel-1 tocsection-22"><a href="#Topics_for_Further_Reading"><span class="tocnumber">7</span> <span class="toctext">Topics for Further Reading</span></a></li>
</ul>
</div>

<p>Regression is used frequently to calculate the line of best fit. If you perform a regression analysis, you will generate an analysis report sheet listing the regression results of the model. In this article, we explain how to interpret the imporant regressin reslts quickly and easily
</p>
<table class="note">

<tr>
<td><b>Notes:</b> To learn more about the algorithm and equations of these statistics, see <a href="../../UserGuide/Category/Theory_of_Nonlinear_Curve_Fitting.html" title="Category:Theory of Nonlinear Curve Fitting">Theory of Nonlinear Curve Fitting</a>
</td></tr></table>
<h2><a name="In_Parameters_Table"></a><span class="mw-headline">In Parameters Table</span></h2>
<p>The fitted values are reported in the <b>Parameters</b> table, like what is shown below:
</p>
<dl><dd><a  class="image"><img alt="Interpret Linear Regression Results 1.png" src="../images/Interpreting_Regression_Results/Interpret_Linear_Regression_Results_1.png?v=71802" width="641"  /></a></dd></dl>
<h3><a name="Value"></a><span class="mw-headline">Value</span></h3>
<p>Estimated values for each parameter of the best fit which would make the curve closest to the data points.
</p>
<h3><a name="Standard_Error"></a><span class="mw-headline">Standard Error</span></h3>
<p>The parameter standard errors can give us an idea of the precision of the fitted values. Typically, the magnitude of the standard error values should be lower than the fitted values. If the standard error values are much greater than the fitted values, the fitting model may be <a href="../../UserGuide/UserGuide/The_Fitting_Process_Convergence_Tolerance_and_Dependencies.html#Over-parameterized_functions" title="UserGuide:The Fitting Process Convergence Tolerance and Dependencies">overparameterized</a>.
</p>
<h3><a name="t-Value"></a><span class="mw-headline">t-Value</span></h3>
<h3><a name="Prob.3E.7Ct.7C"></a><span class="mw-headline">Prob&gt;|t|</span></h3>
<p>Is every term in the regression model significant? Or does every predictor contribute to the response? The <b>t-tests for coefficients</b> answer these kinds of questions. 
</p><p>The null hypothesis for a parameter's t-test is that this parameter is equal to zero. So if the null hypothesis is not rejected, the corresponding predictor will be viewed as insignificant, which means that it has little to do with the response.
</p><p>The t-test can also be used as a detection tool. For example, in polynomial regression, we can use it to determine the proper order of the polynomial model. We add higher order terms until a t-test for the newly-added term suggests that it is insignificant.
</p>
<ul><li> <b>t-Value</b>:  the test statistic for t-test 
<dl><dd> t-Value = Fitted value/Standard Error, for example the t-Value for y0 is 5.34198/0.58341 = 9.15655.<br /></dd>
<dd> For this statistical t-value, it usually compares with a critical t-value of a given confident level <img src="../images/Interpreting_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/> (usually be 5%). If the t-value is larger than the critical t-value (<img src="../images/Interpreting_Regression_Results/math-9d38e8e7363d66af41530c3fbdf494f5.png?v=0" title="|t|&gt;t_{\frac \alpha 2}" alt="|t|&gt;t_{\frac \alpha 2}" class="tex"/> ), it can be said that there is a significant difference. </dd>
<dd> However, the <b>Prob&gt;|t|</b> is easier to interpret, and we recommend that you ignore t-Value and judge by Prob&gt;|t|.</dd></dl></li></ul>
<ul><li> <b>Prob&gt;|t|</b>: The p-value for t-test
<dl><dd>If Prob&gt;|t| &lt; <img src="../images/Interpreting_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/> (usually be 5%), that means we find enough evidence to reject H0 of t-test. The smaller Prob&gt;|t|, the more unlikely the parameter is equal to zero.</dd></dl></li></ul>
<h3><a name="LCL_and_UCL_.28Parameter_Confidence_Interval.29"></a><span class="mw-headline">LCL and UCL (Parameter Confidence Interval)</span></h3>
<p>UCL and LCL, upper and lower confidence intervals of parameter, indicate how likely the interval is to contain the true value. For example, in the above image of Parameters table, we are 95% sure that the true value of offset(y0) is between  4.16764 and 6.51631, the  true values of center(x0) is between 24.73246 and 25.08134, and the true values of width(w) is between 9.75801 and 10.58138.
</p>
<h3><a name="Dependency"></a><span class="mw-headline">Dependency</span></h3>
<p>The dependency value, which is computed from the variance-covariance matrix, typically indicates the significance of the parameter in your model. For example, if some dependency values are close to 1, this could mean that there is mutual dependency between those parameters. In other words, the function is over-parameterized and the parameter may be redundant. Note that you should include restrictions to make sure that the fitting model is meaningful, which you can refer to <a href="../../UserGuide/UserGuide/Model_Diagnosis_Using_Dependency_Values.html" title="UserGuide:Model Diagnosis Using Dependency Values">this section</a>.
</p>
<h2><a name="In_Statistics_Table"></a><span class="mw-headline">In Statistics Table</span></h2>
<p>The key linear fit statistics are summarized in the <b>Statistics</b> table, like what is shown below:
</p>
<dl><dd><a  class="image"><img alt="Interpret Linear Regression Results 2.png" src="../images/Interpreting_Regression_Results/Interpret_Linear_Regression_Results_2.png?v=71741" width="225"  /></a></dd></dl>
<h3><a name="Residual_Sum_of_Squares"></a><span class="mw-headline">Residual Sum of Squares</span></h3>
<p>Residual Sum of Squares is usually abbreviated to RSS. It is actually the sum of the square of the vertical deviations from each data point to the fitting regression line. It can be inferred that your data is perfect fit if the value of RSS is equal to zero. This statistic can help to decide if the fitted regression line is a good fit for your data. Generally speaking, the smaller the residual sum of squares, the better your model fits your data.
</p>
<h3><a name="Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"></a><span class="mw-headline">Scale Error with sqrt(Reduced Chi-Sqr)</span></h3>
<p>The Reduced Chi-square value, which is also called Scale Error with square, is equal to the residual sum of square (RSS) divided by the degree of freedom. Typically a Reduced Chi-square value close to 1 indicates a good fit result, and it implies that the difference between observed data and fitted data is consistent with the error variance. If the error variance is over-estimated, the Reduced Chi-square value will be much less than 1. For under-estimated error variance, it will be much greater than 1. Note that it needs to select the correct variance in fitting procedure for Reduced Chi-square. For example, if the y data is multiplied by a scaling factor simply, the Reduced Chi-square will be scaled as well. Only if you also scale the error variance by a correct factor, the value of Reduced Chi-square will turn back into a normal value.
</p>
<h3><a name="Pearson.27s_r"></a><span class="mw-headline">Pearson's r</span></h3>
<p>Pearson’s correlation coefficient, denoted by Pearson’s r, can help to measure the strength of linear relationship between paired data.  The value of Pearson’s r is constrained between -1 to 1. In linear regression, a positive value of Pearson’s r indicates that there is positive linear correlation between predictor variable(x) and response variable(y), while a negative value of Pearson’s r indicates that there is negative linear correlation between predictor variable(x) and response variable(y). The value of zero indicates that there is no linear correlation between data. What’s more, the closer the value is to -1 or 1, the stronger linear correlation is.
</p>
<h3><a name="R-Square_.28COD.29"></a><span class="mw-headline">R-Square (COD)</span></h3>
<p>R-square, which is also known as the coefficient of determination (COD), is a statistical measure to qualify the linear regression. It is a percentage of the response variable variation that explained by the fitted regression line, for example the R-square suggests that the model explains approximately more than 89% of the variability in the response variable.  Hence, R-square is always between 0 and 1. If R-square is 0, it indicates that fitted line explains none of the variability of the response data around its mean; while if R-square is 1, it indicates that the fitted line explains all the variability of the response data around its mean. In general, the larger the R-square, the better the fitted line fits your data.
</p>
<h3><a name="Adj._R-Square"></a><span class="mw-headline">Adj. R-Square</span></h3>
<p>R-square can be used to quantify how well a model fits the data, and R-square will always increase when a new predictor is added. It is a misunderstanding that a model with more predictors has a better fit. The Adj. R-square is a modified version of R-square, which is adjusted for the number of predictor in the fitted line. Thus, it can be used to compare with the fitted lines with different numbers of predictors. If the number of predictors is greater than 1, Adj.-square is always smaller than R-square.
</p>
<table class="noborder"><tr>
	<td style="vertical-align:top" width="60"><img src="../images/Interpreting_Regression_Results/Tip_icon.png?v=9614" width="57"  border="0" /></td><td><p>To learn more about how to judge how good is the fit, see <a href="../../UserGuide/UserGuide/Details_of_R_square.html" title="UserGuide:Details of R square">Additional Information of R-square</a>
</p></td></tr></table>
<h2><a name="In_ANOVA_Table"></a><span class="mw-headline">In ANOVA Table</span></h2>
<p>How the regression equation account for the variability in the ys is answered in ANOVA table, like what is shown below:
</p>
<dl><dd><a  class="image"><img alt="Interpret Linear Regression Results 3.png" src="../images/Interpreting_Regression_Results/Interpret_Linear_Regression_Results_3.png?v=71806" width="486"  /></a></dd></dl>
<h3><a name="F_Value"></a><span class="mw-headline">F Value</span></h3>
<p>F Value is a ratio of two mean squares,which can be computed by dividing the mean square of fitted model by the mean square of error. For example, the F value of the model above is 65764.4768/16441.1192=2103.59577. It is a test statistic for a test of whether the fitted model differs significantly from the model y=constant, which is a flat line with slope being equal to zero. It can be inferred that the more this ratio deviates from 1, the stronger the evidence for the fitted model differing significantly from the model y=constant.
</p>
<h3><a name="Prob.3EF"></a><span class="mw-headline">Prob&gt;F</span></h3>
<p>Prob&gt;F is a p-value for F-test, which is a probability with a value ranging from 0 to 1. If the p-value for F-test is less than the significant level <img src="../images/Interpreting_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/> (usually be 5%), it can be conclude that the fitted model is significantly different from the model y=constant, which inferred that the fitted model is a nonlinear curve or a linear curve with slope that significantly different from zero.
</p>
<table class="note">

<tr>
<td><b>Notes:</b> In linear regression, if fixing the intercept at a certain value, the p value for F-test is not meaningful, and it is different from that in linear regression without the intercept constraint.
</td></tr></table>
<h2><a name="In_Covariance_and_Correlation_Table"></a><span class="mw-headline">In Covariance and Correlation Table</span></h2>
<p>The relationship between the variables can be obsevered in Covariance and Correlation table, like what is shown below:<br />
<a  class="image"><img alt="Interpret Linear Regression Results 6.png" src="../images/Interpreting_Regression_Results/Interpret_Linear_Regression_Results_6.png?v=72048" width="397"  /></a>
</p>
<h3><a name="Covariance"></a><span class="mw-headline">Covariance</span></h3>
<p>The covariance value indicates the correlation between two variables, and the matrices of covariance in regression show the inter-correlations among all parameters. The diagonal values for covariance matrix is equal to the square of parameter error.
</p>
<h3><a name="Correlation"></a><span class="mw-headline">Correlation</span></h3>
<p>The correlation matrix rescales the covariance values so that their range is from -1 to +1. A value close to +1 means the two parameters are positively correlated, while a value close to -1 indicates negative correlation. And a zero value indicates that the two parameters are totally independent.
</p>
<h2><a name="In_Fitted_Curves_Plot"></a><span class="mw-headline"><a href="../../UserGuide/UserGuide/Fitted_Curves_Plot_Analysis.html" title="UserGuide:Fitted Curves Plot Analysis">In Fitted Curves Plot</a></span></h2>
<p>The fitted curve as well as its confidence band, prediction band and ellipse are plotted on the Fitted Curves Plot as below, which can help to interpret the regression model more intuitively.
</p>
<dl><dd><a  class="image"><img alt="Interpret Linear Regression Results 5.png" src="../images/Interpreting_Regression_Results/Interpret_Linear_Regression_Results_5.png?v=72026" width="400"  /></a></dd></dl>
<ul><li> <a href="../../UserGuide/UserGuide/Fitted_Curves_Plot_Analysis.html#Confidence_and_Prediction_Bands" title="UserGuide:Fitted Curves Plot Analysis">Confidence and Prediction Bands</a></li>
<li> <a href="../../UserGuide/UserGuide/Fitted_Curves_Plot_Analysis.html#Ellipse_Plots" title="UserGuide:Fitted Curves Plot Analysis">Ellipse Plots</a></li></ul>
<h2><a name="In_Residual_Plot"></a><span class="mw-headline"><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html" title="UserGuide:Graphic Residual Analysis">In Residual Plot</a></span></h2>
<p>The residual is defined as:
</p><p><img src="../images/Interpreting_Regression_Results/math-04f8f09e63a269a044ceeee72a32143d.png?v=0" title="r_i=y_i-y=observed\,value\,of\,y-predicted\,value\,of\,y" alt="r_i=y_i-y=observed\,value\,of\,y-predicted\,value\,of\,y" class="tex"/>
</p><p>Residual plots can be used to assess the quality of a regression, which is usually at the end of the report as below:<br />
</p>
<dl><dd><a  class="image"><img alt="Interpret Linear Regression Results 4.png" src="../images/Interpreting_Regression_Results/Interpret_Linear_Regression_Results_4.png?v=72025" width="400"  /></a></dd></dl>
<ul><li><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Checking_the_error_variance" title="UserGuide:Graphic Residual Analysis">Checking the error variance</a></li>
<li><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Checking_the_process_drift" title="UserGuide:Graphic Residual Analysis">Checking the process drift</a></li>
<li><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Checking_independence_of_the_error_term" title="UserGuide:Graphic Residual Analysis">Checking independence of the error term</a></li>
<li><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Checking_normality_of_variance" title="UserGuide:Graphic Residual Analysis">Checking normality of variance</a></li>
<li><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Improving_the_regression_model_using_residuals_plots" title="UserGuide:Graphic Residual Analysis">Improving the regression model using residuals plots</a></li>
<li><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Detecting_outliers_by_transforming_residuals" title="UserGuide:Graphic Residual Analysis">Detecting outliers by transforming residuals</a></li>
<li><a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Residual_contour_plots_for_surface_fitting" title="UserGuide:Graphic Residual Analysis">Residual contour plots for surface fitting</a></li></ul>
<h2><a name="Topics_for_Further_Reading"></a><span class="mw-headline">Topics for Further Reading</span></h2>
<ul><li> <a href="../../UserGuide/UserGuide/The_Fitting_Process_Convergence_Tolerance_and_Dependencies.html" title="UserGuide:The Fitting Process Convergence Tolerance and Dependencies">The Reason Why Fail to Converge</a></li>
<li> <a href="../../UserGuide/Category/Theory_of_Nonlinear_Curve_Fitting.html" title="Category:Theory of Nonlinear Curve Fitting">Theory of Nonlinear Curve Fitting</a></li>
<li> <a href="../../UserGuide/Category/Fit_Comparison.html" title="Category:Fit Comparison">Fit Comparison</a></li></ul>


