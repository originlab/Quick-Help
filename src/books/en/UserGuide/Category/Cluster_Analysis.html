<h1 class="firstHeading">17.7.3 Cluster Analysis</h1><p class='urlname' style='display: none'>Cluster-Analysis</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Goals"><span class="tocnumber">1</span> <span class="toctext">Goals</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Hierarchical_Cluster_Analysis"><span class="tocnumber">2</span> <span class="toctext">Hierarchical Cluster Analysis</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Classifying_Observations"><span class="tocnumber">2.1</span> <span class="toctext">Classifying Observations</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Classifying_Variables"><span class="tocnumber">2.2</span> <span class="toctext">Classifying Variables</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Selecting_Cluster_Methods"><span class="tocnumber">2.3</span> <span class="toctext">Selecting Cluster Methods</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="#Number_of_Clusters"><span class="tocnumber">2.3.1</span> <span class="toctext">Number of Clusters</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Standardizing_the_Variables"><span class="tocnumber">2.3.2</span> <span class="toctext">Standardizing the Variables</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="#Distance_Measures"><span class="tocnumber">2.3.3</span> <span class="toctext">Distance Measures</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="#Cluster_Methods"><span class="tocnumber">2.3.4</span> <span class="toctext">Cluster Methods</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#K-Means_Cluster_Analysis"><span class="tocnumber">3</span> <span class="toctext">K-Means Cluster Analysis</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#Selecting_Cluster_Methods_2"><span class="tocnumber">3.1</span> <span class="toctext">Selecting Cluster Methods</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#Handling_Missing_Values"><span class="tocnumber">4</span> <span class="toctext">Handling Missing Values</span></a></li>
</ul>
</div>

<p>Cluster analysis is a common method for constructing smaller groups (clusters) from a large set of data. Similar to <a href="../../UserGuide/Category/Discriminant_Analysis.html" title="Category:Discriminant Analysis">Discriminant Analysis</a>, Cluster analysis is also concerned with classifying observations into groups. However, discriminant analysis requires you to know group membership for the cases used to derived the classification rule. Cluster Analysis is a more primitive technique in that no assumptions are made concerning the number of groups or the group membership
</p>
<dl><dd><a  class="image"><img alt="Dendrogram summary.png" src="../images/Cluster_Analysis/300px-Dendrogram_summary.png?v=71254" width="300"   /></a></dd></dl>
<h2><a name="Goals"></a><span class="mw-headline">Goals</span></h2>
<ul><li><b>Classification</b>
<dl><dd>Cluster Analysis provides a way for users to discover potential relationships and construct systematic structures in large numbers of variables and observations.</dd></dl></li></ul>
<h2><a name="Hierarchical_Cluster_Analysis"></a><span class="mw-headline">Hierarchical Cluster Analysis</span></h2>
<p>Hierarchical Cluster Analysis is the primary statistical method for finding relatively homogeneous clusters of cases based on measured characteristics.  It starts with each case as a separate cluster, and then combines the clusters sequentially, reducing the number of clusters at each step until only one cluster remains.  The clustering method uses the dissimilarities or distances between objects when forming the clusters.
</p><p><br />
</p>
<h3><a name="Classifying_Observations"></a><span class="mw-headline">Classifying Observations</span></h3>
<p>Hierarchical Cluster Analysis is most appropriate for small samples.  When the sample (n) is large, the algorithm may be very slow to reach a solution.  In general, users should consider K-Means Cluster when the sample size is larger than 200.
</p><p><br />
</p>
<h3><a name="Classifying_Variables"></a><span class="mw-headline">Classifying Variables</span></h3>
<p>Hierarchical Cluster Analysis is the only way to observe how homogeneous groups of variables are formed. Note that K-Means Cluster Analysis only supports classifying observations. 
</p><p><br />
</p>
<h3><a name="Selecting_Cluster_Methods"></a><span class="mw-headline">Selecting Cluster Methods</span></h3>
<h4><a name="Number_of_Clusters"></a><span class="mw-headline">Number of Clusters</span></h4>
<p>There is no definitive way to set the number of clusters for your analysis. You may need to examine the  <a href="../../UserGuide/UserGuide/Interpreting_Results_of_Hierarchical_Cluster_Analysis.html#Dendrogram" title="UserGuide:Interpreting Results of Hierarchical Cluster Analysis">dendrogram</a> and the characteristics of the clusters, and then incrementally adjust the number to obtain a good cluster solution.
</p>
<h4><a name="Standardizing_the_Variables"></a><span class="mw-headline">Standardizing the Variables</span></h4>
<p>If the variables are measured in different scales, you can standardize variables. This results in all variables contributing more equally to the distance measurement, though you may lose variability information in the variables.
</p>
<h4><a name="Distance_Measures"></a><span class="mw-headline">Distance Measures</span></h4>
<ul><li>Euclidean distance: 
<dl><dd>Euclidean distance, the most common distance measure, is the geometric distance in multidimensional space. It is suitable only for continuous variables. </dd></dl></li></ul>
<ul><li>Squared Euclidean distance: 
<dl><dd>The squared Euclidean distance places greater emphasis on objects that are further apart. </dd></dl></li></ul>
<ul><li>City block distance:
<dl><dd>Both city block distance and Euclidean distance are special cases of the Minkowski metric. Where the Euclidean distance corresponds to the length of the shortest path between two points, the city-block distance is the sum of distances along each dimension:</dd></dl></li></ul>
<dl><dd><table class="note">

<tr>
<td><b>Notes</b>: Both Euclidean and squared Euclidean distance are sensitive when data are standardized. If we want to standardize data during analysis, city block distance should be used.
</td></tr></table></dd></dl>
<ul><li>Cosine distance
<dl><dd>The cosine of the angle between two vectors of values</dd></dl></li></ul>
<ul><li>Pearson correlation distance
<dl><dd>The difference between 1 and the cosine coefficient of two observations. Cosine coefficient is the cosine of the angle between two vectors.</dd></dl></li></ul>
<ul><li>Jaccard distance
<dl><dd>The difference between 1 and the Jaccard coefficient of two observations. For binary data, Jaccard coefficient equals to the ratio of sizes of intersection and union of two observations</dd></dl></li></ul>
<h4><a name="Cluster_Methods"></a><span class="mw-headline">Cluster Methods</span></h4>
<ul><li>Nearest neighbor. 
<dl><dd>In this method, the distance between two clusters is taken to be the distance between their closest neighboring objects. This method is recommended if plotted clusters are elongated.</dd></dl></li></ul>
<ul><li>Furthest neighbor. 
<dl><dd>In this method, the distance between two clusters is the maximum distance between two objects in different clusters. This method is recommended if the plotted clusters form distinct clumps (not elongated chains).</dd></dl></li></ul>
<ul><li>Group average: 
<dl><dd>In this method, the distance between two clusters is calculated as the average distance between all pairs of objects in the different clusters.  This method is usually recommended as it makes use of more information. </dd></dl></li></ul>
<ul><li>Centroid. 
<dl><dd>The cluster to be merged is the one with the smallest sum of distances between the centroid for all variables. The centroid of a cluster is the average point in the multidimensional space. </dd></dl></li></ul>
<ul><li> Median
<dl><dd> This method is identical to the Centroid method but is unweighted. It should not be used when cluster sizes vary markedly.</dd></dl></li></ul>
<dl><dd><table class="note">

<tr>
<td><b>Note</b> When the Centroid method and Median method is selected, squared Euclidean distance is recommended.
</td></tr></table></dd></dl>
<ul><li>Ward
<dl><dd>For each cluster, the means for all variables are calculated. Then, for each case, the squared Euclidean distance to the cluster means is calculated. These distances are summed for all of the cases. The cluster to be merged is the one which will increase the sum the least. That is, this method minimizes the increase in the overall sum of the squared within-cluster distances.  This method tends to create clusters of small size.</dd></dl></li></ul>
<h2><a name="K-Means_Cluster_Analysis"></a><span class="mw-headline">K-Means Cluster Analysis</span></h2>
<p>K-Means Cluster Analysis is used to classify observations through K number of clusters. The idea is to minimize the distance between the data and the corresponding cluster centroid. K-means analysis is based on one of the simplest algorithms for solving the cluster problem, and is therefore much faster than hierarchical cluster analysis.
</p><p>Users should typically consider K-means analysis when the sample size is larger than 100. Note, however, that K-means cluster analysis assumes the user already knows the centroid of the observations, or, at least, the number of groups to be clustered.
</p>
<h3><a name="Selecting_Cluster_Methods_2"></a><span class="mw-headline">Selecting Cluster Methods</span></h3>
<p>The first step in k-means clustering is to find the cluster centers.  Run <a href="../../UserGuide/Category/Cluster_Analysis.html#Hierarchical_Cluster_Analysis" title="Category:Cluster Analysis">hierarchical cluster analysis</a> with a small sample size to obtain a reasonable initial cluster center. Alternatively, you can specify a number of clusters and then let Origin automatically select a well-separated value as the initial cluster center.  Note that automatic detection is sensitive to outliers, so be sure to screen data for outliers before analyzing.
</p>
<h2><a name="Handling_Missing_Values"></a><span class="mw-headline">Handling Missing Values</span></h2>
<p>If there are missing values in the training data/group range, the whole case (entire row) will be excluded in the analysis
</p><p><br />
</p>
<table class="topiclist">

<tr>
<td>
<p><i><b>Topics covered in this section:</b></i> 
</p>
<ul><li><b>Hierarchical Cluster Analysis</b>
<ul><li><a href="../../UserGuide/UserGuide/The_Hierarchical_Cluster_Analysis_Dialog_Box.html" title="UserGuide:The Hierarchical Cluster Analysis Dialog Box">The Hierarchical Cluster Analysis Dialog Box</a></li>
<li><a href="../../UserGuide/UserGuide/Interpreting_Results_of_Hierarchical_Cluster_Analysis.html" title="UserGuide:Interpreting Results of Hierarchical Cluster Analysis">Interpreting Results</a></li>
<li><a href="../../UserGuide/UserGuide/Algorithm_(Hierarchical_Cluster_Analysis).html" title="UserGuide:Algorithm (Hierarchical Cluster Analysis)">Algorithms</a></li></ul></li>
<li><b>K-Means Cluster Analysis</b>
<ul><li><a href="../../UserGuide/UserGuide/The_K-Means_Cluster_Analysis_Dialog_Box.html" title="UserGuide:The K-Means Cluster Analysis Dialog Box">The K-Means Cluster Analysis Dialog Box</a></li>
<li><a href="../../UserGuide/UserGuide/Interpreting_Results_of_K-Means_Cluster_Analysis.html" title="UserGuide:Interpreting Results of K-Means Cluster Analysis">Interpreting Results</a></li>
<li><a href="../../UserGuide/UserGuide/Algorithm_(K-Means_Cluster_Analysis).html" title="UserGuide:Algorithm (K-Means Cluster Analysis)">Algorithms</a></li></ul></li>
<li><a class="external text" href="../../Tutorial/Tutorial/Cluster_Analysis.html">Tutorial</a></li>
<li><a href="../../UserGuide/UserGuide/Reference_(Cluster_Analysis).html" title="UserGuide:Reference (Cluster Analysis)"><b>References</b></a></li></ul>
</td></tr></table>






