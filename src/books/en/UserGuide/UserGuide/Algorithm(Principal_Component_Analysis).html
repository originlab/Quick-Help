<h1 class="firstHeading">17.7.1.3 Algorithms(Principal Component Analysis)</h1><p class='urlname' style='display: none'>PCA-Algorithm</p>
<p><br />
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Listwise_Exclusion_of_Missing_Values"><span class="tocnumber">1</span> <span class="toctext">Listwise Exclusion of Missing Values</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Matrix_Type_for_Analysis"><span class="tocnumber">1.1</span> <span class="toctext">Matrix Type for Analysis</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Quantities_to_Compute"><span class="tocnumber">1.2</span> <span class="toctext">Quantities to Compute</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Pairwise_Exclusion_of_Missing_Values"><span class="tocnumber">2</span> <span class="toctext">Pairwise Exclusion of Missing Values</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Bartlett.27s_Test"><span class="tocnumber">3</span> <span class="toctext">Bartlett's Test</span></a></li>
</ul>
</div>

<p><b>Principal Component Analysis</b> examines relationships of variables. It can be used to reduce the number of variables in regression and clustering, for example.
</p><p>Each principal component in <b>Principal Component Analysis</b> is the linear combination of the variables and gives a maximized variance. Let <i>X</i> be a matrix for <i>n</i> observations by <i>p</i> variables, and the covariance matrix is <i>S</i>. Then for a linear combination of the variables 
</p>
<dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-ab9343ff9fcb3706ee7abdb2cb379b0e.png?v=0" title="z_1=\sum_{i=1}^p a_{1i}x_i" alt="z_1=\sum_{i=1}^p a_{1i}x_i" class="tex"/></dd></dl>
<p>where <img src="../images/Algorithm(Principal_Component_Analysis)/math-22cf6d90476d6b87655fb83d74ee08f5.png?v=0" title="x_i\ " alt="x_i\ " class="tex"/> is the <i>i</i>th variable, <img src="../images/Algorithm(Principal_Component_Analysis)/math-54042c94adbcd82da042b6c3acb9f2d7.png?v=0" title="a_{1i} \ i=1,2,...,p" alt="a_{1i} \ i=1,2,...,p" class="tex"/> are linear combination coefficients for <img src="../images/Algorithm(Principal_Component_Analysis)/math-c444279b915ee36642fa338e34c13122.png?v=0" title="z_1\ " alt="z_1\ " class="tex"/>, they can be denoted by a column vector <img src="../images/Algorithm(Principal_Component_Analysis)/math-d244e0f74039ac13f15b218c30a1197d.png?v=0" title="a_1\ " alt="a_1\ " class="tex"/>, and normalized by <img src="../images/Algorithm(Principal_Component_Analysis)/math-a4489a65c4ea00a3749c13371e32b1c8.png?v=0" title="a_1^Ta_1=1" alt="a_1^Ta_1=1" class="tex"/>. The variance of <img src="../images/Algorithm(Principal_Component_Analysis)/math-c444279b915ee36642fa338e34c13122.png?v=0" title="z_1\ " alt="z_1\ " class="tex"/> will be <img src="../images/Algorithm(Principal_Component_Analysis)/math-731b4410c9a103b26e052e0fdddb6ab5.png?v=0" title="a_1^TSa_1" alt="a_1^TSa_1" class="tex"/>.
</p><p>The vector <img src="../images/Algorithm(Principal_Component_Analysis)/math-d244e0f74039ac13f15b218c30a1197d.png?v=0" title="a_1\ " alt="a_1\ " class="tex"/> is found by maximizing the variance. And <img src="../images/Algorithm(Principal_Component_Analysis)/math-c444279b915ee36642fa338e34c13122.png?v=0" title="z_1\ " alt="z_1\ " class="tex"/> is called the first principal component. The second principal component can be found in the same way by maximizing:
</p>
<dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-493b8e25d52c2ace8bdb017172b0d8c8.png?v=0" title="a_2^TSa_2" alt="a_2^TSa_2" class="tex"/> subject to the constraints <img src="../images/Algorithm(Principal_Component_Analysis)/math-873ed3c6fc8df3717f6650fc969da6c3.png?v=0" title="a_2^Ta_2=1" alt="a_2^Ta_2=1" class="tex"/> and <img src="../images/Algorithm(Principal_Component_Analysis)/math-ae413d86760f05319c501f0496c6c868.png?v=0" title="a_2^Ta_1=0" alt="a_2^Ta_1=0" class="tex"/></dd></dl>
<p>It gives the second principal component that is orthogonal to the first one. Remaining principal components can be derived in a similar way. In fact coefficients <img src="../images/Algorithm(Principal_Component_Analysis)/math-27a6d6501e3c3aa9f3ea0490bd2883a3.png?v=0" title="a_1, a_2, ..., a_p\ " alt="a_1, a_2, ..., a_p\ " class="tex"/> can be calculated from eigenvectors of the matrix <i>S</i>. Origin uses different methods according to the way of excluding missing values.
</p>
<h2><a name="Listwise_Exclusion_of_Missing_Values"></a><span class="mw-headline">Listwise Exclusion of Missing Values</span></h2>
<p>An observation containing one or more missing values will be excluded in the analysis. And a matrix <img src="../images/Algorithm(Principal_Component_Analysis)/math-d5bc9869a7e5e4c51c3ec29c497c9913.png?v=0" title="X_s\ " alt="X_s\ " class="tex"/> for <b>SVD</b> can be derived from <i>X</i> depending on the matrix type for analysis.
</p>
<h3><a name="Matrix_Type_for_Analysis"></a><span class="mw-headline">Matrix Type for Analysis</span></h3>
<ul><li> Covariance Matrix</li></ul>
<dl><dd><dl><dd> Let <img src="../images/Algorithm(Principal_Component_Analysis)/math-d5bc9869a7e5e4c51c3ec29c497c9913.png?v=0" title="X_s\ " alt="X_s\ " class="tex"/> be the matrix <i>X</i> with each column's mean subtracted from each variable and each column scaled by <img src="../images/Algorithm(Principal_Component_Analysis)/math-c6b39e3953f822c60f04aa96795f8ca5.png?v=0" title="\frac{1}{\sqrt{n-1}}" alt="\frac{1}{\sqrt{n-1}}" class="tex"/>.</dd></dl></dd></dl>
<ul><li> Correlation Matrix</li></ul>
<dl><dd><dl><dd> Let <img src="../images/Algorithm(Principal_Component_Analysis)/math-d5bc9869a7e5e4c51c3ec29c497c9913.png?v=0" title="X_s\ " alt="X_s\ " class="tex"/> be the matrix <i>X</i> with each column's mean subtracted from each variable and each column scaled by <img src="../images/Algorithm(Principal_Component_Analysis)/math-49b8574ecb53ea9393f842389f9eb9c3.png?v=0" title="\frac{1}{\sqrt{n-1}\sigma_i}" alt="\frac{1}{\sqrt{n-1}\sigma_i}" class="tex"/> where <img src="../images/Algorithm(Principal_Component_Analysis)/math-7972095bacc7478fcb70fd6733c0f5bb.png?v=0" title="\sigma_i\ " alt="\sigma_i\ " class="tex"/> is the standard deviation of the <i>i</i>th variable.</dd></dl></dd></dl>
<h3><a name="Quantities_to_Compute"></a><span class="mw-headline">Quantities to Compute</span></h3>
<p>Perform <b>SVD</b> on <img src="../images/Algorithm(Principal_Component_Analysis)/math-d5bc9869a7e5e4c51c3ec29c497c9913.png?v=0" title="X_s\ " alt="X_s\ " class="tex"/>.
</p>
<dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-d498c41ab09cd28f0ed57082a46e997c.png?v=0" title="X_s=V\Lambda P^T\ " alt="X_s=V\Lambda P^T\ " class="tex"/></dd></dl>
<p>where <i>V</i> is an <i>n</i> by <i>p</i> matrix with <img src="../images/Algorithm(Principal_Component_Analysis)/math-019541f74704787977ee873013839295.png?v=0" title="V^TV=I\ " alt="V^TV=I\ " class="tex"/>, <i>P</i> is a <i>p</i> by <i>p</i> matrix, and <img src="../images/Algorithm(Principal_Component_Analysis)/math-781ff4289c6cc5fc2973b7a57791e0e2.png?v=0" title="\Lambda" alt="\Lambda" class="tex"/> is a diagonal matrix with diagonal elements <img src="../images/Algorithm(Principal_Component_Analysis)/math-285c6a5e3a199d684c4b38c337b68b7c.png?v=0" title="s_i \ i=1, 2, ..., p" alt="s_i \ i=1, 2, ..., p" class="tex"/>.
</p>
<ul><li> Eigenvalues</li></ul>
<dl><dd><dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-10d0f1ef59c0ff4f54dd64b9e67956eb.png?v=0" title="\lambda_i=s_i^2" alt="\lambda_i=s_i^2" class="tex"/></dd></dl></dd></dl>
<dl><dd>Eigenvalues are sorted in descending order. The proportion of variance explained by the <i>i</i>th principal component is <img src="../images/Algorithm(Principal_Component_Analysis)/math-0f760222253fcac467e4e4382ea4e49d.png?v=0" title="\lambda_i/\sum_{k=1}^p \lambda_k" alt="\lambda_i/\sum_{k=1}^p \lambda_k" class="tex"/>.</dd></dl>
<ul><li> Eigenvectors</li></ul>
<dl><dd>Eigenvectors are also known as loadings or coefficients for principal components. Each column in <i>P</i> is the eigenvector corresponding to the eigenvalue or principal component.</dd></dl>
<dl><dd><b>Note</b> that the eigenvector's sign is not unique for <b>SVD</b>, Origin normalizes its sign by forcing the sum of each column to be positive.</dd></dl>
<ul><li> Scores</li></ul>
<dl><dd>Each column in <img src="../images/Algorithm(Principal_Component_Analysis)/math-07e38ffc20f9ed4a40e60ed2a7eadd06.png?v=0" title="\sqrt{n-1}V\Lambda" alt="\sqrt{n-1}V\Lambda" class="tex"/> is the scores corresponding to the principal component. And scores will be missing values corresponding to an observation containing missing values.</dd></dl>
<dl><dd><b>Note</b> that variance of scores for each principal component equals its corresponding eigenvalue for this method.
<ul><li> Standardized Scores</li></ul>
<dl><dd> Scores for each principal component are standardized so that they have unit variance.</dd></dl></dd></dl>
<h2><a name="Pairwise_Exclusion_of_Missing_Values"></a><span class="mw-headline">Pairwise Exclusion of Missing Values</span></h2>
<p>An observation is excluded only in the calculation of covariance or correlation between two variables if missing values exist in either of the two variables for the observation.
</p><p>Eigenvalues and eigenvectors are calculated from the covariance or correlation matrix <i>S</i>.
</p>
<dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-1eb525b55da5f5a7b146d626ef443681.png?v=0" title="SP=PD\ " alt="SP=PD\ " class="tex"/></dd></dl>
<p>where <i>P</i> is a <i>p</i> by <i>p</i> matrix and <i>D</i> is a diagonal matrix with diagonal elements <img src="../images/Algorithm(Principal_Component_Analysis)/math-ae1d06f83f13974ab2df299694d371b3.png?v=0" title="\lambda_i \ i=1, 2, ..., p" alt="\lambda_i \ i=1, 2, ..., p" class="tex"/>.
</p>
<ul><li> Eigenvalues</li></ul>
<dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-b42a49b60191bc5e33a25f63bfc4244e.png?v=0" title="\lambda_i\ " alt="\lambda_i\ " class="tex"/> is the <i>i</i>th eigenvalue for the <i>i</i>th principal component. And eigenvalues are sorted in descending order.</dd>
<dd><b>Note</b> that eigenvalues can be negative for missing values excluded in a pairwise way, which will make no sense for principal components. Origin sets the loading and scores to zeros for a negative eigenvalue.</dd></dl>
<ul><li> Eigenvectors</li></ul>
<dl><dd>Each column in <i>P</i> is the eigenvector corresponding to the eigenvalue or principal component.</dd>
<dd><b>Note</b> that the eigenvector's sign is not unique; Origin normalizes its sign by forcing the sum of each column to be positive.  </dd></dl>
<ul><li> Scores</li></ul>
<dl><dd><dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-bb91d01fd09e5459e2c215fa93bf2c63.png?v=0" title="V=X_0P\ " alt="V=X_0P\ " class="tex"/></dd></dl></dd>
<dd>where <img src="../images/Algorithm(Principal_Component_Analysis)/math-529f8b193005f6dea07a06457001ca34.png?v=0" title="X_0\ " alt="X_0\ " class="tex"/> is the matrix X with each column's mean subtracted from each variable.</dd>
<dd>Scores will be missing values corresponding to an observation containing missing values. </dd>
<dd>Note that variance of scores for each principal component may not equal its corresponding eigenvalue for this method. </dd></dl>
<dl><dd><ul><li> Standardized Scores</li></ul>
<dl><dd>Scores for each principal component are scaled by the square root of its eigenvalue.</dd></dl></dd></dl>
<h2><a name="Bartlett.27s_Test"></a><span class="mw-headline">Bartlett's Test</span></h2>
<p><b>Bartlett's Test</b> tests the equality of the remaining <i>p-k</i> eigenvalues. It is available only when analysis matrix is covariance matrix.
</p>
<dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-f7c8bd18f6e041157a76f1fa02889635.png?v=0" title="H_0:\lambda_{k+1}=\lambda_{k+2}=...=\lambda_{p} k=0, 1, ..., p-2\ " alt="H_0:\lambda_{k+1}=\lambda_{k+2}=...=\lambda_{p} k=0, 1, ..., p-2\ " class="tex"/></dd></dl>
<p>It approximates a <img src="../images/Algorithm(Principal_Component_Analysis)/math-e004463241106484d0f7761b9110eeaf.png?v=0" title="\chi_2\ " alt="\chi_2\ " class="tex"/> distribution with <img src="../images/Algorithm(Principal_Component_Analysis)/math-256b80388bbbcc269ef94aa5532dab62.png?v=0" title="\frac{1}{2}(p-k-1)(p-k+2)" alt="\frac{1}{2}(p-k-1)(p-k+2)" class="tex"/> degrees of freedom.
</p>
<dl><dd><img src="../images/Algorithm(Principal_Component_Analysis)/math-465397770aeec65eb2b9adadcdd2c942.png?v=0" title="(n-1-(2p+5)/6)\Big\{-\sum_{i=k+1}^p \mathrm{log}(\lambda_i)+(p-k)\mathrm{log}(\sum_{i=k+1}^p \lambda_i/(p-k))\Big\}" alt="(n-1-(2p+5)/6)\Big\{-\sum_{i=k+1}^p \mathrm{log}(\lambda_i)+(p-k)\mathrm{log}(\sum_{i=k+1}^p \lambda_i/(p-k))\Big\}" class="tex"/></dd></dl>






