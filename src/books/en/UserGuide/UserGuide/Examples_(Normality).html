<h1 class="firstHeading">17.1.8.3 Choosing Normality Tests and Interpreting Results</h1><p class='urlname' style='display: none'>NormalityTest-EX</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Summary"><span class="tocnumber">1</span> <span class="toctext">Summary</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Example"><span class="tocnumber">2</span> <span class="toctext">Example</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Interpreting_Results"><span class="tocnumber">3</span> <span class="toctext">Interpreting Results</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Graphical_Methods"><span class="tocnumber">3.1</span> <span class="toctext">Graphical Methods</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-5"><a href="#Choosing_Normality_Test_Methods"><span class="tocnumber">4</span> <span class="toctext">Choosing Normality Test Methods</span></a></li>
</ul>
</div>

<h2><a name="Summary"></a><span class="mw-headline">Summary</span></h2>
<p>Suppose you want to investigate the health status of some students. You collect 40 students and record their names, genders, ages, heights and weights. After collecting your data, you use a Normality Test procedure to examine whether the weights of the students follow a normal distribution.
</p>
<h2><a name="Example"></a><span class="mw-headline">Example</span></h2>
<ol><li>Import body.dat in the \Samples\Statistics folder.</li>
<li>Highlight Column E</li>
<li>Select <b>Statistics: Descriptive Statistics: Normality Test</b>.</li>
<li>Check all tests under the <b>Quantities to Compute</b> branch.</li>
<li>Select <b>Histograms</b> and <b>Box Chart</b> under the <b>Plots</b> branch.</li>
<li>Click <b>OK</b>.</li></ol>
<h2><a name="Interpreting_Results"></a><span class="mw-headline">Interpreting Results</span></h2>
<p>Statistical models usually depend on some underlying assumptions. One common assumption is that of a normally-distributed population. Unfortunately, many analysts assume normality without any empirical evidence or test. If the assumption of normality is violated, then what we infer might not be reliable. 
</p><p>It is difficult to define a standard for interpreting the results of normality tests because needs vary by discipline and by analyst. Some test methods may be satisfactory for one field but unsatisfactory in another field.
</p><p>There are two primary approaches to normality testing:  graphical methods and numerical methods. Graphical methods tend to be intuitive and easy to interpret. Numerical methods are more precise and hence, more objective.
</p>
<h3><a name="Graphical_Methods"></a><span class="mw-headline">Graphical Methods</span></h3>
<p>Stem-and-leaf plots, (skeletal) box plots, dot plots, histograms, and P-P or Q-Q plots, are useful for visualizing the difference between an empirical distribution and a theoretical normal distribution. Origin's <b>Normality Test</b> tool offers histograms and box charts, but it should be mentioned that Origin also offers P-P and Q-Q plots from the <b>Plot</b> menu.
</p><p>One very straightforward way to "test" for normality is to create a histogram. It is well known that the shape of a normal distribution is symmetrical and classically "bell-shaped." Looking at a histogram, we can we can obtain a rough idea as to the nature of the population distribution. The box chart is something of a complement to the histogram. A box chart effectively summarizes major percentiles, such as minimum, 25th percentile (1st quartile), 50th percentile (median), 75th percentile (3rd quartile) and maximum, using a box and lines. If the 25th and 75th percentiles are symmetrical with respect to the median, and median and mean values are seen to be located at roughly the same position near the center of the box, then we have reason to believe that the variable of interest may be normally distributed. In the body.dat example above, the shape of histogram is not exactly symmetrical, but it is near to a "bell" shape. The box chart of the same data also indicates rough symmetry.
</p>
<h2><a name="Choosing_Normality_Test_Methods"></a><span class="mw-headline">Choosing Normality Test Methods</span></h2>
<p>It is well known that measures of skewness and kurtosis can be applied to normality testing. Skewness is generally defined to be a third standardized moment, a measure of the degree of symmetry. If skewness is greater than zero, the distribution is right-skewed and we count more observations on the left side of the distribution curve; conversely, when skewness is less than 0, observations will be distributed to the right side of the curve. Kurtosis, a fourth standardized moment, measures peak expression or thinness of tails. Note that the standard normal distribution has kurtosis = 0 (in "excess kutosis" definition convention). So, if a calculated kurtosis &gt; 0, the distribution has thinner tails and a higher peak as compared with the standard normal distribution. Origin calculates both skewness and kurtosis. See <a href="../../UserGuide/UserGuide/The_DescStats_Dialog_Box.html" title="UserGuide:The DescStats Dialog Box">Statistics on Columns</a> for details.
</p><p>The Kolmogorov-Smirnov, Kolmogorov-Smirnov-Lilliefors, Anderson-Darling and Cramer-von Mises tests are empirical distribution function (EDF) based methods, while Jarque–Bera and Skewness-Kurtosis (aka D'Agostino K-Squared) tests are Chi-squared distribution based. The Chen-Shapiro test is a normalized  spacing-based method found to be both powerful and simple. Shapiro-Wilk, Ryan-Joiner and Shapiro-Francia tests, like Chen-Shapiro, are regression- and correlation-based methods.
</p>
<ol><li> <b>Kolmogorov-Smirnov</b>: The K-S test, though known to be less powerful, is widely used. Generally, it requires large sample sizes. </li>
<li> <b>Kolmogorov-Smirnov-Lilliefors</b>: An adaptation of the K-S test. More complicated than K-S, since it must be established whether the maximum discrepancy between empirical distribution function and the cumulative distribution function is large enough to be statistically significant. K-S-L is generally recommended over K-S. Some analysts recommend that the sample size of K-S-L be larger than 2000.</li>
<li> <b>Anderson-Darling</b>: One of the best EDF-based statistics for normality testing. Sample size of less than 26 is recommended, but industrial data with 200 and more might pass A-D. The p-value of the A-D test depends on simulation algorithms. The A-D test can be used to test for other distributions with other specified simulation plans. See D’Agostino and Stephens (1986) for details.</li>
<li> <b>D'Agostino K-Squared</b>: Based on skewness and kurtosis measures. See D’Agostino, Belanger, and D’Agostino, Jr. (1990) and Royston (1991) for details. It is worthwhile mentioning that skewness and kurtosis are also affected by sample size.</li>
<li> <b>Shapiro-Wilk</b>: The recommended sample size for this test ranges from 7 to 2000. Origin allows sample sizes from 3 to 5000. However, when sample size is relatively large, D'Agostino K-squared or Lilliefors are generally preferred over Shapiro-Wilk.</li>
<li> <b>Chen-Shapiro</b>: The C-S test extends the S-W test without loss of power. The motivation for C-S is based on the fact that the ratios of the sample spacing to their expected spacing would converge to one due to the consistency of sample quantiles. From the standpoint of power, C-S performs more like the S-W test rather than the S-F test.</li></ol>
<p>Origin provides users with commonly used methods like S-W, K-S, Lilliefors, A-D, D'Agostino-K Squared and C-S tests. There are six additional normality tests in Origin. You can use the following table to guide your choice of tests. Note that in Origin, we report <b>critical values</b> rather than <b>p-values</b> for Chen-Shapiro test. Critical values can also be used for testing. If the specified statistical value is less than or equal to the 5% critical value, then the p-value should be greater than or equal to 0.05. Hence, we would not reject the null hypothesis for alpha = 0.05.
</p>
<table class="simple">
<caption> <b>Summary of Normality tests in Origin</b>
</caption>
<tr>
<th> Test Method </th>
<th> Statistic </th>
<th> N Range </th>
<th> Distribution Based
</th></tr>
<tr>
<td> Kolmogorov-Smirnov </td>
<td> D </td>
<td> 3 &lt;= N </td>
<td> EDF
</td></tr>
<tr>
<td> Lilliefors </td>
<td> L </td>
<td> 4 &lt;= N </td>
<td> EDF
</td></tr>
<tr>
<td> Anderson-Darling </td>
<td> A-square </td>
<td> 8 &lt;= N </td>
<td> EDF
</td></tr>
<tr>
<td> D'Agostino K-Squared </td>
<td> Chi-square </td>
<td> 4 &lt;= N </td>
<td> <img src="../images/Examples_(Normality)/math-0e30d6f5b2ddf5ab5a7bf42ce0306d5a.png?v=0" title="\chi^2(2)" alt="\chi^2(2)" class="tex"/>
</td></tr>
<tr>
<td> Shapiro-Wilk </td>
<td> W </td>
<td> 3 &lt;= N &lt;= 5000 </td>
<td> -
</td></tr>
<tr>
<td> Chen-Shapiro </td>
<td> QH </td>
<td> 10 &lt;= N &lt;= 2000 </td>
<td> -
</td></tr></table>
<p><b>Note</b>: Just because you meet sample size requirements (N in the above table), this does not guarantee that the test result is efficient and powerful. Almost all normality test methods perform poorly for small sample sizes (less than or equal to 30).
</p>





