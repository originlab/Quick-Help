<h1 class="firstHeading">17.4.1.2 Algorithms (One-Way ANOVA)</h1><p class='urlname' style='display: none'>OneWayANOVA-Algorithm</p>
<p><br />
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Theory_of_One-Way_ANOVA"><span class="tocnumber">1</span> <span class="toctext">Theory of One-Way ANOVA</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Homogeneity_of_Variance"><span class="tocnumber">2</span> <span class="toctext">Homogeneity of Variance</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Multiple_Means_Comparisons"><span class="tocnumber">3</span> <span class="toctext">Multiple Means Comparisons</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Power_Analysis"><span class="tocnumber">4</span> <span class="toctext">Power Analysis</span></a></li>
</ul>
</div>

<h2><a name="Theory_of_One-Way_ANOVA"></a><span class="mw-headline">Theory of One-Way ANOVA</span></h2>
<p>Assume we have response data measured in k levels of the factor, where <img src="../images/One_Way_ANOVA_Algorithms/math-53969de94b3708187e040cc38293f661.png?v=0" title="y_{ij}\,\!" alt="y_{ij}\,\!" class="tex"/> represents the value of <i>i</i>th observation (<i>i</i> = 1, 2, ...<img src="../images/One_Way_ANOVA_Algorithms/math-4aa861124eff57dd7988faa6753e8b7e.png?v=0" title="n_j" alt="n_j" class="tex"/>) on the <i>j</i>th factor level (<i>j</i> = 1, 2, ..., <i>k</i>). Then we could write the model of one-way ANOVA as:
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-18ac8e0a81c8177f2a341169e66558ff.png?v=0" title="y_{ij}=u+t_j+\varepsilon _{ij}" alt="y_{ij}=u+t_j+\varepsilon _{ij}" class="tex"/>,<i>j</i> = 1,2, ..., <i>k</i>; <i>i</i> = 1, 2, ...<img src="../images/One_Way_ANOVA_Algorithms/math-4aa861124eff57dd7988faa6753e8b7e.png?v=0" title="n_j" alt="n_j" class="tex"/>
</p><p>Since ANOVA testing whether the mean of two or more populations (levels) are equal. Thus, the null hypothesis is that the means of the different populations are the same and the alternate hypothesis is at least one psample's mean is different from the others. Mathematically, this is expressed as:
</p><p>H0:<img src="../images/One_Way_ANOVA_Algorithms/math-c73bd204f60c2fb90a3fdc612f03dfba.png?v=0" title="\mu =\mu _1=\mu _2=\cdots =\mu _k" alt="\mu =\mu _1=\mu _2=\cdots =\mu _k" class="tex"/>
</p><p>H1:<img src="../images/One_Way_ANOVA_Algorithms/math-83ac8a11cc465d7690f302a008cc1578.png?v=0" title="\mu _p\neq \mu _q" alt="\mu _p\neq \mu _q" class="tex"/>  for some <i>p</i> and <i>q</i>, <img src="../images/One_Way_ANOVA_Algorithms/math-7588000cd59644c7b9cac3c6dd559059.png?v=0" title="1 \leq  p" alt="1 \leq  p" class="tex"/>, <img src="../images/One_Way_ANOVA_Algorithms/math-b923442948b5054ef5552308b82aaa88.png?v=0" title="q \geq  k" alt="q \geq  k" class="tex"/>.
</p><p>where <img src="../images/One_Way_ANOVA_Algorithms/math-732981cec59c615df618a8ca96265370.png?v=0" title="\mu _i\,\!" alt="\mu _i\,\!" class="tex"/> is the jth sample mean. To test the hypothesis, it should be divide the total sample variation into variation between groups and variation within groups, and then using the F-test to test whether these two variations are different.
</p><p>Algebraically, we can use the respective mean square of each part to estimate the variation:
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-318b2225240c5d36c702a65d9791e911.png?v=0" title="\sum_{j=1}^k\sum_{i=1}^{n_1}(y_{ij}-\bar y)^2=\sum_{j=1}^kn_j(\bar y_j-\bar y)^2+\sum_{j=1}^k\sum_{i=1}^{n_1}(y_{ij}-\bar y_j)^2" alt="\sum_{j=1}^k\sum_{i=1}^{n_1}(y_{ij}-\bar y)^2=\sum_{j=1}^kn_j(\bar y_j-\bar y)^2+\sum_{j=1}^k\sum_{i=1}^{n_1}(y_{ij}-\bar y_j)^2" class="tex"/>
</p><p>where the left term is called the "total sum of squares", the second term is called the "sum of squares of treatments", which represents the variation between groups, and the third term is called "sum of squares of error", which represent the variation within groups. The equation is then commonly abbreviated to
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-2e316f3d38bb44586a05ad68555264bc.png?v=0" title="SS_{Total}=SS_{Treatment}+SS_{Error}\,\!" alt="SS_{Total}=SS_{Treatment}+SS_{Error}\,\!" class="tex"/>
</p><p>When <img src="../images/One_Way_ANOVA_Algorithms/math-806277203dedea2ed8321f6cbd465a54.png?v=0" title="H_0\,\!" alt="H_0\,\!" class="tex"/> is true, the <i>k</i> levels sample data will be normally and independently distributed, with mean <img src="../images/One_Way_ANOVA_Algorithms/math-d4af45ea35b4839bf0a40ebb0c7a4407.png?v=0" title="\mu\,\!" alt="\mu\,\!" class="tex"/> and variance <img src="../images/One_Way_ANOVA_Algorithms/math-03a7b0b49bea966adf365bb0d747ad89.png?v=0" title="\sigma ^2\,\!" alt="\sigma ^2\,\!" class="tex"/>. Thus the statistic
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-3ae802b4468722915b610b8a10caa5ca.png?v=0" title="F=\frac{MS_{Treatment}}{MS_{Error}}=\frac{ss_{Treatment}/(k-1)}{ss_{Error}/(n-k)}" alt="F=\frac{MS_{Treatment}}{MS_{Error}}=\frac{ss_{Treatment}/(k-1)}{ss_{Error}/(n-k)}" class="tex"/>
</p><p>will follow an <i>F</i> distribution <img src="../images/One_Way_ANOVA_Algorithms/math-bc03dd17df0bf9580dc44db49f8c532c.png?v=0" title="F_{(k-1, n-k)}\,\!" alt="F_{(k-1, n-k)}\,\!" class="tex"/> where <img src="../images/One_Way_ANOVA_Algorithms/math-fe639299f2dbf4e9160e4f8086e83caf.png?v=0" title="MS_{Treatment}" alt="MS_{Treatment}" class="tex"/> is the mean squares for treatments and <img src="../images/One_Way_ANOVA_Algorithms/math-ff8dc369a4b279c7e064cb1ef6fc4ba1.png?v=0" title="MS_{Error}" alt="MS_{Error}" class="tex"/> is the mean squares for error, which are both formed by dividing the sum of squares by the associated degrees of freedom respectively. Given a certain significance level<img src="../images/One_Way_ANOVA_Algorithms/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/> , if the <i>F</i> statistic exceeds the critical value <img src="../images/One_Way_ANOVA_Algorithms/math-441f7078a6d67a201e54ee601f437ba2.png?v=0" title="F_{(k-1,n-k,\alpha)}\,\!" alt="F_{(k-1,n-k,\alpha)}\,\!" class="tex"/> which is the tabular value of the <i>F</i> distribution with <i>k-1</i> and <i>n-k</i> degrees of freedom at level <img src="../images/One_Way_ANOVA_Algorithms/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/> , or equivalently, the followed <i>P</i> value less than the significance level, the null hypothesis should be rejected. 
</p><p>Typically, it is common to present the results of the analysis of variance in an ANOVA table:
</p>
<table class="simple">
<tr>
<th>Source of Variation
</th>
<th>Degrees of Freedom (DF)
</th>
<th>Sum of Squares (SS)
</th>
<th>Mean Square (MS)
</th>
<th><i>F</i> Value
</th>
<th><i>Prob</i> &gt; <i>F</i>
</th></tr>
<tr>
<th>Model (Factor)
</th>
<td><i>k</i>-1
</td>
<td><img src="../images/One_Way_ANOVA_Algorithms/math-37ee04e2b0deee9bf057596e95fa9d4a.png?v=0" title="SS_{Treatme}" alt="SS_{Treatme}" class="tex"/>
</td>
<td><img src="../images/One_Way_ANOVA_Algorithms/math-fe639299f2dbf4e9160e4f8086e83caf.png?v=0" title="MS_{Treatment}" alt="MS_{Treatment}" class="tex"/>
</td>
<td><img src="../images/One_Way_ANOVA_Algorithms/math-fe639299f2dbf4e9160e4f8086e83caf.png?v=0" title="MS_{Treatment}" alt="MS_{Treatment}" class="tex"/> / <img src="../images/One_Way_ANOVA_Algorithms/math-ff8dc369a4b279c7e064cb1ef6fc4ba1.png?v=0" title="MS_{Error}" alt="MS_{Error}" class="tex"/>
</td>
<td><img src="../images/One_Way_ANOVA_Algorithms/math-4cfa5369bc0906fc9a5c5d9297de4180.png?v=0" title="P\{F\geq F_{(k-1,n-k,\alpha )}\}" alt="P\{F\geq F_{(k-1,n-k,\alpha )}\}" class="tex"/>
</td></tr>
<tr>
<th>Error
</th>
<td><i>n-k</i>
</td>
<td><img src="../images/One_Way_ANOVA_Algorithms/math-349fb0475e9b16a6353bbc1aae235e6a.png?v=0" title="SS_{Error}" alt="SS_{Error}" class="tex"/>
</td>
<td><img src="../images/One_Way_ANOVA_Algorithms/math-ff8dc369a4b279c7e064cb1ef6fc4ba1.png?v=0" title="MS_{Error}" alt="MS_{Error}" class="tex"/>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<th>Total
</th>
<td><i>n</i>-1
</td>
<td>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></table>
<h2><a name="Homogeneity_of_Variance"></a><span class="mw-headline">Homogeneity of Variance</span></h2>
<p>In the analysis of variance, it is assumed that different samples have equal variances, which is commonly called homogeneity of variance. The Levene test and Brown-Forsythe test can be used to verify the assumption. Suppose we have <i>k</i> samples of response data, where <img src="../images/One_Way_ANOVA_Algorithms/math-53969de94b3708187e040cc38293f661.png?v=0" title="y_{ij}\,\!" alt="y_{ij}\,\!" class="tex"/> represents the value of ith observation (<i>i</i> = 1, 2, ...<img src="../images/One_Way_ANOVA_Algorithms/math-4aa861124eff57dd7988faa6753e8b7e.png?v=0" title="n_j" alt="n_j" class="tex"/>) on the <i>j</i>th factor level (<i>j</i> = 1, 2, ..., <i>k</i>). The hypotheses of both Levene test and Brown-Forsythe test can be expressed as:
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-e65765bedcabe42c66ec93228769e82a.png?v=0" title="H_0" alt="H_0" class="tex"/>: <img src="../images/One_Way_ANOVA_Algorithms/math-6ae18486367863bbc938e896756f8df8.png?v=0" title="\sigma^2 _1=\sigma^2 _2=\cdots =\sigma^2 _k" alt="\sigma^2 _1=\sigma^2 _2=\cdots =\sigma^2 _k" class="tex"/>
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-6207a80403dcccc1aa3b5b7303315c4b.png?v=0" title="H_1" alt="H_1" class="tex"/>: <img src="../images/One_Way_ANOVA_Algorithms/math-2b5c2cc59d7a827f54617e61fb32f2d9.png?v=0" title="\sigma^2 _p\neq \sigma^2 _q" alt="\sigma^2 _p\neq \sigma^2 _q" class="tex"/> , for at least one pair (<i>p</i>,<i> q</i>), <img src="../images/One_Way_ANOVA_Algorithms/math-f3befe30a4a0d18509558e6012408028.png?v=0" title="1\leq p,q\leq k" alt="1\leq p,q\leq k" class="tex"/>
</p><p>Define <img src="../images/One_Way_ANOVA_Algorithms/math-2af48c8053eb375472b4bbf78b90cec3.png?v=0" title="Z_{ij}\,\!" alt="Z_{ij}\,\!" class="tex"/> as the following three definitions according to different tests,
</p>
<ol><li>Absolute Levene test:<img src="../images/One_Way_ANOVA_Algorithms/math-b4e5bd6341b0f8ff84bf4a86cd819c58.png?v=0" title="Z_{ij}=|y_{ij}-\bar y_j|" alt="Z_{ij}=|y_{ij}-\bar y_j|" class="tex"/></li>
<li>Squared Levene test:<img src="../images/One_Way_ANOVA_Algorithms/math-ca90d368003ebe629f44501f0e71c07a.png?v=0" title="Z_{ij}^2=(y_{ij}-\bar y_j)^2" alt="Z_{ij}^2=(y_{ij}-\bar y_j)^2" class="tex"/>   </li>
<li>Brown-Forsythe test:<img src="../images/One_Way_ANOVA_Algorithms/math-face015854160d6388d3424185d3c24a.png?v=0" title="Z_{ij}=|y_{ij}-m_j|\,\!" alt="Z_{ij}=|y_{ij}-m_j|\,\!" class="tex"/>     </li></ol>
<p>When <img src="../images/One_Way_ANOVA_Algorithms/math-e65765bedcabe42c66ec93228769e82a.png?v=0" title="H_0" alt="H_0" class="tex"/> holds, the test statistic
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-9550c1c345a48df5bdeda1954598b3a4.png?v=0" title="F=\frac{\sum_{j=1}^kn_j(\bar Z_j-\bar Z)^2/(k-1)}{\sum_{j=1}^k\sum_{i=1}^{n_1}(Z_{ij}-\bar Z_j)^2/(n-k)}" alt="F=\frac{\sum_{j=1}^kn_j(\bar Z_j-\bar Z)^2/(k-1)}{\sum_{j=1}^k\sum_{i=1}^{n_1}(Z_{ij}-\bar Z_j)^2/(n-k)}" class="tex"/>
</p><p>will (approximately) follow an <i>F</i> distribution <img src="../images/One_Way_ANOVA_Algorithms/math-9c7168c68d0f7e7fcc65ef56a0cf4e87.png?v=0" title="F_{(k-1,n-k)}\,\!" alt="F_{(k-1,n-k)}\,\!" class="tex"/>where <img src="../images/One_Way_ANOVA_Algorithms/math-68dd52e4750d73c982c39f0a3a5bc7b2.png?v=0" title="\overline{Z_j}" alt="\overline{Z_j}" class="tex"/>and <img src="../images/One_Way_ANOVA_Algorithms/math-a79056b6644e25416e91e8b496e5d378.png?v=0" title="\overline{Z}" alt="\overline{Z}" class="tex"/>are the group mean of and the overall mean of the <img src="../images/One_Way_ANOVA_Algorithms/math-2af48c8053eb375472b4bbf78b90cec3.png?v=0" title="Z_{ij}\,\!" alt="Z_{ij}\,\!" class="tex"/> respectively.
</p>
<h2><a name="Multiple_Means_Comparisons"></a><span class="mw-headline">Multiple Means Comparisons</span></h2>
<p>Given that an ANOVA experiment has determined that at least one of the population means is significantly different, multiple means comparison subsequently compares all possible pairs of factor level means to determine which mean (or means) is (or are) significantly different. There are various methods for mean comparison in Origin, and we use the <a class="external text" href="http://www.originlab.com/pdfs/nagcl09/manual/pdf/g04/g04dbc.pdf" target="_blank"><b>NAG function nag_anova_confid_interval (g04dbc)</b></a> to perform means comparisons. 
</p><p>Two types of multiple means comparison methods are included in Origin: 
</p>
<ol><li>Single-step method. It creates simultaneous confidence intervals to show how the means differ, including Tukey-Kramer, Bonferroni, Dunn-Sidak, Fisher's LSD, and Scheffe.</li>
<li>Stepwise method. Sequentially perform the hypothesis tests, including Holm-Bonferroni and Holm-Sidak tests.</li></ol>
<h2><a name="Power_Analysis"></a><span class="mw-headline">Power Analysis</span></h2>
<p>The power analysis procedure calculates the actual power for the sample data, as well as the hypothetical power if additional sample sizes are specified.
</p><p>The power of a one-way analysis of variance is a measurement of its sensitivity. Power is the probability that the one-way ANOVA will detect differences in the sample means when real differences exist. In terms of the null and alternative hypotheses, power is the probability that the test statistic <i>F</i> will be extreme enough to reject the null hypothesis when it should be rejected actually (i.e. given the null hypothesis is not true).
</p><p>Power is defined by the equation:  
</p><p><img src="../images/One_Way_ANOVA_Algorithms/math-994e2581ef30349da587e03faa17ce2b.png?v=0" title="power=1-probf(f,dfa,dfe,nc)\,\!" alt="power=1-probf(f,dfa,dfe,nc)\,\!" class="tex"/>
</p><p>where <i>f</i> is the deviate from the non-central <i>F</i>-distribution with <i>dfa</i> and <i>dfe</i>, model and error degrees of freedom, respectively. And <i>nc = SST/MSE</i>, where <i>SST</i> is the sum of squares of the Model, and <i>MSE</i> is the mean square of the Errors. The value of <i>probf( )</i> is obtained using the NAG function <a class="external text" href="http://www.originlab.com/pdfs/nagcl09/manual/pdf/g01/g01gdc.pdf" target="_blank"><b>nag_prob_non_central_f_dist (g01gdc)</b></a>. Please see the NAG documentation for more detailed information.
</p><p>All the above is a brief algorithm outline of one-way analysis of variation, for more information about the detail mathematical deduction, please reference to the corresponding part of the user's manual and NAG document.
</p>





