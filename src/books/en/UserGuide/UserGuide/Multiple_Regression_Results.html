<h1 class="firstHeading">15.2.7 Algorithm (Multiple Linear Regression)</h1><p class='urlname' style='display: none'>Multi-Regression-Algorithm</p>
<div class="toclimit-3"><div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#The_Multiple_Linear_Regression_Model"><span class="tocnumber">1</span> <span class="toctext">The Multiple Linear Regression Model</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Multiple_Linear_Regression_Model"><span class="tocnumber">1.1</span> <span class="toctext">Multiple Linear Regression Model</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-3"><a href="#Fit_Control"><span class="tocnumber">2</span> <span class="toctext">Fit Control</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Errors_as_Weight"><span class="tocnumber">2.1</span> <span class="toctext">Errors as Weight</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#No_Weighting"><span class="tocnumber">2.1.1</span> <span class="toctext">No Weighting</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#Direct_Weighting"><span class="tocnumber">2.1.2</span> <span class="toctext">Direct Weighting</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Instrumental"><span class="tocnumber">2.1.3</span> <span class="toctext">Instrumental</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="#Fix_Intercept_.28at.29"><span class="tocnumber">2.2</span> <span class="toctext">Fix Intercept (at)</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"><span class="tocnumber">2.3</span> <span class="toctext">Scale Error with sqrt(Reduced Chi-Sqr)</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Fitting_Results"><span class="tocnumber">3</span> <span class="toctext">Fitting Results</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#Fit_Parameters"><span class="tocnumber">3.1</span> <span class="toctext">Fit Parameters</span></a>
<ul>
<li class="toclevel-3 tocsection-12"><a href="#The_Fitted_Values"><span class="tocnumber">3.1.1</span> <span class="toctext">The Fitted Values</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#The_Parameter_Standard_Errors"><span class="tocnumber">3.1.2</span> <span class="toctext">The Parameter Standard Errors</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#t-Value_and_Confidence_Level"><span class="tocnumber">3.1.3</span> <span class="toctext">t-Value and Confidence Level</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Prob.3E.7Ct.7C"><span class="tocnumber">3.1.4</span> <span class="toctext">Prob&gt;|t|</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="#LCL_and_UCL"><span class="tocnumber">3.1.5</span> <span class="toctext">LCL and UCL</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#CI_Half_Width"><span class="tocnumber">3.1.6</span> <span class="toctext">CI Half Width</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-18"><a href="#Fit_Statistics"><span class="tocnumber">3.2</span> <span class="toctext">Fit Statistics</span></a>
<ul>
<li class="toclevel-3 tocsection-19"><a href="#Degree_of_Freedom"><span class="tocnumber">3.2.1</span> <span class="toctext">Degree of Freedom</span></a></li>
<li class="toclevel-3 tocsection-20"><a href="#Reduced_Chi-Sqr"><span class="tocnumber">3.2.2</span> <span class="toctext">Reduced Chi-Sqr</span></a></li>
<li class="toclevel-3 tocsection-21"><a href="#Residual_Sum_of_Squares"><span class="tocnumber">3.2.3</span> <span class="toctext">Residual Sum of Squares</span></a></li>
<li class="toclevel-3 tocsection-22"><a href="#R-Square_.28COD.29"><span class="tocnumber">3.2.4</span> <span class="toctext">R-Square (COD)</span></a></li>
<li class="toclevel-3 tocsection-23"><a href="#Adj._R-Square"><span class="tocnumber">3.2.5</span> <span class="toctext">Adj. R-Square</span></a></li>
<li class="toclevel-3 tocsection-24"><a href="#R_Value"><span class="tocnumber">3.2.6</span> <span class="toctext">R Value</span></a></li>
<li class="toclevel-3 tocsection-25"><a href="#Root-MSE_.28SD.29"><span class="tocnumber">3.2.7</span> <span class="toctext">Root-MSE (SD)</span></a></li>
<li class="toclevel-3 tocsection-26"><a href="#Norm_of_Residuals"><span class="tocnumber">3.2.8</span> <span class="toctext">Norm of Residuals</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-27"><a href="#ANOVA_Table"><span class="tocnumber">4</span> <span class="toctext">ANOVA Table</span></a></li>
<li class="toclevel-1 tocsection-28"><a href="#Lack_of_fit_table"><span class="tocnumber">5</span> <span class="toctext">Lack of fit table</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="#Covariance_and_Correlation_Matrix"><span class="tocnumber">6</span> <span class="toctext">Covariance and Correlation Matrix</span></a></li>
<li class="toclevel-1 tocsection-30"><a href="#Residual_Analysis"><span class="tocnumber">7</span> <span class="toctext">Residual Analysis</span></a>
<ul>
<li class="toclevel-2 tocsection-31"><a href="#Standardized"><span class="tocnumber">7.1</span> <span class="toctext">Standardized</span></a></li>
<li class="toclevel-2 tocsection-32"><a href="#Studentized"><span class="tocnumber">7.2</span> <span class="toctext">Studentized</span></a></li>
<li class="toclevel-2 tocsection-33"><a href="#Studentized_deleted"><span class="tocnumber">7.3</span> <span class="toctext">Studentized deleted</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-34"><a href="#Plots"><span class="tocnumber">8</span> <span class="toctext">Plots</span></a>
<ul>
<li class="toclevel-2 tocsection-35"><a href="#Partial_Leverage_Plots"><span class="tocnumber">8.1</span> <span class="toctext">Partial Leverage Plots</span></a></li>
<li class="toclevel-2 tocsection-36"><a href="#Resudial_Type"><span class="tocnumber">8.2</span> <span class="toctext">Resudial Type</span></a></li>
<li class="toclevel-2 tocsection-37"><a href="#Residual_vs._Independent"><span class="tocnumber">8.3</span> <span class="toctext">Residual vs. Independent</span></a></li>
<li class="toclevel-2 tocsection-38"><a href="#Residual_vs._Predicted_Value"><span class="tocnumber">8.4</span> <span class="toctext">Residual vs. Predicted Value</span></a></li>
<li class="toclevel-2 tocsection-39"><a href="#Residual_vs._Order_of_the_Data"><span class="tocnumber">8.5</span> <span class="toctext">Residual vs. Order of the Data</span></a></li>
<li class="toclevel-2 tocsection-40"><a href="#Histogram_of_the_Residual"><span class="tocnumber">8.6</span> <span class="toctext">Histogram of the Residual</span></a></li>
<li class="toclevel-2 tocsection-41"><a href="#Residual_Lag_Plot"><span class="tocnumber">8.7</span> <span class="toctext">Residual Lag Plot</span></a></li>
<li class="toclevel-2 tocsection-42"><a href="#Normal_Probability_Plot_of_Residuals"><span class="tocnumber">8.8</span> <span class="toctext">Normal Probability Plot of Residuals</span></a></li>
</ul>
</li>
</ul>
</div>
</div> 
<h2><a name="The_Multiple_Linear_Regression_Model"></a><span class="mw-headline">The Multiple Linear Regression Model</span></h2>
<h3><a name="Multiple_Linear_Regression_Model"></a><span class="mw-headline">Multiple Linear Regression Model</span></h3>
<p>Multiple linear regression is an extension of the simple linear regression where multiple independent variables exist. It is used to analyze the effect of more than one independent variable <img src="../images/Multiple_Regression_Results/math-ec2ba336386674f7fa253f0d0dcac1a8.png?v=0" title="x_1, x_2, \dots, x_k" alt="x_1, x_2, \dots, x_k" class="tex"/> on the dependent variable y. For a given dataset <img src="../images/Multiple_Regression_Results/math-2c25c9c7a0462237f2a4644881404e23.png?v=0" title="(y, x_1, x_2, \dots, x_k)" alt="(y, x_1, x_2, \dots, x_k)" class="tex"/>, the multiple linear regression fits the dataset to the model: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-262f3316e59dbf9b8b69b139884ec4fa.png?v=0" title="y_i=\beta _0+\beta _1x_{1_i}+\beta _2x_{2_i}+\ldots +\beta _kx_{k_i}+\varepsilon_i" alt="y_i=\beta _0+\beta _1x_{1_i}+\beta _2x_{2_i}+\ldots +\beta _kx_{k_i}+\varepsilon_i" class="tex"/>
</th>
<td>
<p>(1)  
</p>
</td></tr></table>
<p>where <img src="../images/Multiple_Regression_Results/math-44b4e53e7225cafd1b37e050ce75e823.png?v=0" title="\beta _0\,\!" alt="\beta _0\,\!" class="tex"/> is the y-intercept and the parameters <img src="../images/Multiple_Regression_Results/math-3a1e29703e364979554e6bd76761bed6.png?v=0" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"/>, <img src="../images/Multiple_Regression_Results/math-a043a2f0bb7ed3e0c06481290fdf47a2.png?v=0" title="\beta _2\,\!" alt="\beta _2\,\!" class="tex"/>, ... , <img src="../images/Multiple_Regression_Results/math-5b39c0e0cc0595b0300d44912f7c22e7.png?v=0" title="\beta _k\,\!" alt="\beta _k\,\!" class="tex"/> are called the <b>partial coefficients</b>. 
It can be written in matrix form:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-7bed15493bc406eac2438b89f4a6c3ee.png?v=0" title="Y=XB+E\,\!" alt="Y=XB+E\,\!" class="tex"/>
</th>
<td>
<p>(2)  
</p>
</td></tr></table>
<p>where 
</p>
<table class="formula">

<tr>
<td>
<p><img src="../images/Multiple_Regression_Results/math-fa4969dbe77f364fcfaf328ec6e5b52e.png?v=0" title="Y=\begin{bmatrix}&#10;y_1\\&#10;y_2\\&#10;\vdots \\&#10;y_n&#10;\end{bmatrix}&#10;" alt="Y=\begin{bmatrix}&#10;y_1\\&#10;y_2\\&#10;\vdots \\&#10;y_n&#10;\end{bmatrix}&#10;" class="tex"/> , 
<img src="../images/Multiple_Regression_Results/math-74ae0ca33efa266b763df62b49c75ab7.png?v=0" title="X=\begin{bmatrix}&#10;1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k}\\&#10;1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k}\\&#10;\vdots &amp; \vdots  &amp; \vdots &amp; \ddots &amp; \vdots\\&#10;1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nk}&#10;\end{bmatrix}  &#10;" alt="X=\begin{bmatrix}&#10;1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k}\\&#10;1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k}\\&#10;\vdots &amp; \vdots  &amp; \vdots &amp; \ddots &amp; \vdots\\&#10;1 &amp; x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{nk}&#10;\end{bmatrix}  &#10;" class="tex"/>
<img src="../images/Multiple_Regression_Results/math-df7566ba68876e83357d8090cb114a23.png?v=0" title="B=\begin{bmatrix}&#10;\beta_0 \\&#10;\beta_1 \\&#10;\vdots \\&#10;\beta_k&#10;\end{bmatrix} " alt="B=\begin{bmatrix}&#10;\beta_0 \\&#10;\beta_1 \\&#10;\vdots \\&#10;\beta_k&#10;\end{bmatrix} " class="tex"/>, 
<img src="../images/Multiple_Regression_Results/math-237378136fd3d40c04522d7634de4349.png?v=0" title="E=\begin{bmatrix}&#10;\varepsilon _1\\&#10;\varepsilon _2\\&#10;\vdots \\&#10;\varepsilon _n&#10;\end{bmatrix}&#10;" alt="E=\begin{bmatrix}&#10;\varepsilon _1\\&#10;\varepsilon _2\\&#10;\vdots \\&#10;\varepsilon _n&#10;\end{bmatrix}&#10;" class="tex"/>
</p>
</td></tr>
<tr>
<td>
</td></tr></table>
<p>Assuming that <img src="../images/Multiple_Regression_Results/math-3ac22ebe353c690d089056a1a61e884d.png?v=0" title="\varepsilon_i" alt="\varepsilon_i" class="tex"/> are independent and identically distributed as normal random variables with <img src="../images/Multiple_Regression_Results/math-a75d99618896761e0ffbcb97be04b6b7.png?v=0" title="\bar{E}=0" alt="\bar{E}=0" class="tex"/> and <img src="../images/Multiple_Regression_Results/math-5d3cf3883ed3c4d0b917f9fc2e8f776b.png?v=0" title="Var[E]=\sigma^2" alt="Var[E]=\sigma^2" class="tex"/>.
In Order to minimize the <img src="../images/Multiple_Regression_Results/math-1dedbea457f0c6e281fb4fe09fb9cb32.png?v=0" title="\left \|E\right \|" alt="\left \|E\right \|" class="tex"/> with respect to <img src="../images/Multiple_Regression_Results/math-9d5ed678fe57bcca610140957afab571.png?v=0" title="B" alt="B" class="tex"/>, we solve the function:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-22b8dcb05755d4d3e5af8aac28e53a32.png?v=0" title="\frac{\partial E&#39;E}{\partial B}=0" alt="\frac{\partial E&#39;E}{\partial B}=0" class="tex"/>
</th>
<td>
<p>(3)  
</p>
</td></tr></table>
<p>The results <img src="../images/Multiple_Regression_Results/math-b9f1276034ffa4171c37839db1ba85ea.png?v=0" title="\hat B" alt="\hat B" class="tex"/> is the <b>least square estimate</b> of the vector <i>B</i>, and it is the solution to the linear equations, which can be expressed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-95387383fde238dc85ac4085ce5e04a0.png?v=0" title="\hat B=\begin{bmatrix}&#10;\hat \beta_0 \\&#10;\hat \beta_1 \\&#10;\vdots \\&#10;\hat \beta_k&#10;\end{bmatrix}=(X&#39;X)^{-1}X^{\prime }Y&#10;" alt="\hat B=\begin{bmatrix}&#10;\hat \beta_0 \\&#10;\hat \beta_1 \\&#10;\vdots \\&#10;\hat \beta_k&#10;\end{bmatrix}=(X&#39;X)^{-1}X^{\prime }Y&#10;" class="tex"/>
</th>
<td>
<p>(4)  
</p>
</td></tr></table>
<p>where X' is the transpose of X.
The predicted value of Y for a given X is:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-7611ca9662544815a1101d33f3194fd6.png?v=0" title="\hat{Y}=X\hat{B}" alt="\hat{Y}=X\hat{B}" class="tex"/>
</th>
<td>
<p>(5)  
</p>
</td></tr></table>
<p>By substituting <img src="../images/Multiple_Regression_Results/math-40ecbaa26d22c1a0a4ab8103dea95ce9.png?v=0" title="\hat{B}" alt="\hat{B}" class="tex"/> into (4), we can and defined matrix <img src="../images/Multiple_Regression_Results/math-44c29edb103a2872f519ad0c9a0fdaaa.png?v=0" title="P" alt="P" class="tex"/>.
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-9a8d7df1113cf6e1902177ad13919636.png?v=0" title="\hat{Y}=[X(X&#39;X)^{-1}X&#39;]Y=PY" alt="\hat{Y}=[X(X&#39;X)^{-1}X&#39;]Y=PY" class="tex"/>
</th>
<td>
<p>(6)  
</p>
</td></tr></table>
<p>The residuals is defined as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-b1249c78b6d18951184ee1130fcfa8dc.png?v=0" title="res_i=Y-\hat{Y}" alt="res_i=Y-\hat{Y}" class="tex"/>
</th>
<td>
<p>(7)  
</p>
</td></tr></table>
<p>and the residual sum of squares can be written by:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-9b4f79a4a52a65a42176fa7250a5a328.png?v=0" title="RSS=\left \|E \right \|^2={Y}&#39;Y-\hat{B}&#39;X&#39;X\hat{B}" alt="RSS=\left \|E \right \|^2={Y}&#39;Y-\hat{B}&#39;X&#39;X\hat{B}" class="tex"/>
</th>
<td>
<p>(8)  
</p>
</td></tr></table>
<h2><a name="Fit_Control"></a><span class="mw-headline">Fit Control</span></h2>
<h3><a name="Errors_as_Weight"></a><span class="mw-headline">Errors as Weight</span></h3>
<p>We can give weight to each <img src="../images/Multiple_Regression_Results/math-8d62e469fb30ed435a668eb5c035b1f6.png?v=0" title="y_i" alt="y_i" class="tex"/> in fitting process, the <b>yEr±</b> error column <img src="../images/Multiple_Regression_Results/math-65445646e7a531a2185d03b58b4d60e1.png?v=0" title="\sigma_i" alt="\sigma_i" class="tex"/> is treated as weight <img src="../images/Multiple_Regression_Results/math-aa38f107289d4d73d516190581397349.png?v=0" title="w_i" alt="w_i" class="tex"/> for each <img src="../images/Multiple_Regression_Results/math-8d62e469fb30ed435a668eb5c035b1f6.png?v=0" title="y_i" alt="y_i" class="tex"/>, when <b>yEr±</b> is abscent, <img src="../images/Multiple_Regression_Results/math-aa38f107289d4d73d516190581397349.png?v=0" title="w_i" alt="w_i" class="tex"/> should be 1 for all <img src="../images/Multiple_Regression_Results/math-865c0c0b4ab0e063e5caa3387c1a8741.png?v=0" title="i" alt="i" class="tex"/>.
</p><p>The solution <img src="../images/Multiple_Regression_Results/math-40ecbaa26d22c1a0a4ab8103dea95ce9.png?v=0" title="\hat{B}" alt="\hat{B}" class="tex"/> for fitting with weight can be written as:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-83c3de13586e641d2aa5015e071505e9.png?v=0" title="\hat{B}=(X&#39;WX)^{-1}X&#39;WY" alt="\hat{B}=(X&#39;WX)^{-1}X&#39;WY" class="tex"/>
</th>
<td>
<p>(9)  
</p>
</td></tr></table>
<p>where
</p><p><img src="../images/Multiple_Regression_Results/math-8845a41888129439e7f1b3fbdaeb2078.png?v=0" title="W=\begin{bmatrix}&#10; w_1&amp; 0 &amp; \dots &amp;0 \\ &#10;0 &amp; w_2 &amp; \dots &amp;0 \\ &#10; \vdots&amp; \vdots &amp;\ \ddots &amp;\vdots \\ &#10; 0&amp; 0 &amp;\dots  &amp; w_n&#10;\end{bmatrix}" alt="W=\begin{bmatrix}&#10; w_1&amp; 0 &amp; \dots &amp;0 \\ &#10;0 &amp; w_2 &amp; \dots &amp;0 \\ &#10; \vdots&amp; \vdots &amp;\ \ddots &amp;\vdots \\ &#10; 0&amp; 0 &amp;\dots  &amp; w_n&#10;\end{bmatrix}" class="tex"/>
</p>
<h4><a name="No_Weighting"></a><span class="mw-headline">No Weighting</span></h4>
<p>The error bar will not be treated as weight in calculation.
</p>
<h4><a name="Direct_Weighting"></a><span class="mw-headline">Direct Weighting</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-05c143b7a8ecc59d8edb905c4b6e3397.png?v=0" title="w_i=\sigma_i " alt="w_i=\sigma_i " class="tex"/>
</th>
<td>
<p>(10)
</p>
</td></tr></table>
<h4><a name="Instrumental"></a><span class="mw-headline">Instrumental</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-63f1838aba2791cf68521fe43a435763.png?v=0" title="w_i=\frac 1{\sigma_i^2}" alt="w_i=\frac 1{\sigma_i^2}" class="tex"/>
</th>
<td>
<p>(11)
</p>
</td></tr></table>
<h3><a name="Fix_Intercept_.28at.29"></a><span class="mw-headline">Fix Intercept (at)</span></h3>
<p>Fix intercept will set the y-intercept <img src="../images/Multiple_Regression_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png?v=0" title="\beta_0" alt="\beta_0" class="tex"/> to a fixed value, meanwhile, the total degree of freedom will be n*=n-1 due to the intercept fixed.
</p>
<h3><a name="Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"></a><span class="mw-headline">Scale Error with sqrt(Reduced Chi-Sqr)</span></h3>
<p><b>Scale Error with sqrt(Reduced Chi-Sqr)</b> is available when fitting with weight. This option only affects the error on the parameters reported from the fitting process, and does not affect the fitting process or the data in any way. 
By default, it is checked, and <img src="../images/Multiple_Regression_Results/math-10e16c6a764d367ca5077a54bf156f7e.png?v=0" title="\sigma^2" alt="\sigma^2" class="tex"/>, which is the variance of <img src="../images/Multiple_Regression_Results/math-3a3ea00cfc35332cedf6e5e9a32e94da.png?v=0" title="E" alt="E" class="tex"/> is taken into account for calculating error on the parameters, otherwise, variance of <img src="../images/Multiple_Regression_Results/math-3a3ea00cfc35332cedf6e5e9a32e94da.png?v=0" title="E" alt="E" class="tex"/> will not be taken into account for error calculation.
Take Covariance Matrix as example:
</p><p>Scale Error with sqrt(Reduced Chi-Sqr)
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-a219511b6fc22ebe963c31f36a1b4dd7.png?v=0" title="Cov(\beta _i,\beta _j)=\sigma^2 (X^{\prime }X)^{-1}" alt="Cov(\beta _i,\beta _j)=\sigma^2 (X^{\prime }X)^{-1}" class="tex"/>
</th>
<td>
<p>(12)
</p>
</td></tr>
<tr>
<th><img src="../images/Multiple_Regression_Results/math-311ddcb93ec3330a08a572efc3a55aa2.png?v=0" title="\sigma^2=\frac{RSS}{n^{*}-k}" alt="\sigma^2=\frac{RSS}{n^{*}-k}" class="tex"/>
</th></tr></table>
<p>Do not Scale Error with sqrt(Reduced Chi-Sqr)
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-d654da511627b08f8bb2817849d7a889.png?v=0" title="Cov(\beta _i,\beta _j)=(X&#39;X)^{-1}\,\!" alt="Cov(\beta _i,\beta _j)=(X&#39;X)^{-1}\,\!" class="tex"/>
</th>
<td>
<p>(13)
</p>
</td></tr></table>
<p>For weighted fitting, <img src="../images/Multiple_Regression_Results/math-6f369f83036bd2eab887744dd10a6eb7.png?v=0" title="(X&#39;WX)^{-1}\,\!" alt="(X&#39;WX)^{-1}\,\!" class="tex"/> is used instead of <img src="../images/Multiple_Regression_Results/math-8f5640d983fa3d67b37e5356b6a62a78.png?v=0" title="(X&#39;X)^{-1}\,\!" alt="(X&#39;X)^{-1}\,\!" class="tex"/>.
</p>
<h2><a name="Fitting_Results"></a><span class="mw-headline">Fitting Results</span></h2>
<h3><a name="Fit_Parameters"></a><span class="mw-headline">Fit Parameters</span></h3>
<p><a  class="image"><img alt="Mfitted-paramater.png" src="../images/Multiple_Regression_Results/Mfitted-paramater.png?v=63546" width="659"  /></a>
</p>
<h4><a name="The_Fitted_Values"></a><span class="mw-headline">The Fitted Values</span></h4>
<p>Formula (4)
</p>
<h4><a name="The_Parameter_Standard_Errors"></a><span class="mw-headline">The Parameter Standard Errors</span></h4>
<p>For each parameter, the standard error can be obtained by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-8e4bb144039902b6a8fa68ca7ddb3f71.png?v=0" title="s_{\hat \beta _j}=s_\varepsilon \sqrt{C_{jj}}" alt="s_{\hat \beta _j}=s_\varepsilon \sqrt{C_{jj}}" class="tex"/>
</th>
<td>
<p>(14)  
</p>
</td></tr></table>
<p>where <img src="../images/Multiple_Regression_Results/math-befc5e643c332fdb5e94fcc9c9aa08bd.png?v=0" title="C_{jj}" alt="C_{jj}" class="tex"/> is the jth diagonal element of <img src="../images/Multiple_Regression_Results/math-8435c9c5ecdee1bb446adc21fb4019b9.png?v=0" title="(X&#39;X)^{-1}" alt="(X&#39;X)^{-1}" class="tex"/> (note that <img src="../images/Multiple_Regression_Results/math-9ca99bb44e4097061a3d28bbb921d402.png?v=0" title="(X&#39;WX)^{-1}" alt="(X&#39;WX)^{-1}" class="tex"/> is used for weight fitting). The residual standard deviation <img src="../images/Multiple_Regression_Results/math-b9de97a4e5e6f1d978d3b0c42110ae0d.png?v=0" title="s_\varepsilon" alt="s_\varepsilon" class="tex"/>  (also called "std dev", "standard error of estimate", or "root MSE") is computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-ebd838f3aa96453aaa7999a4d91e6327.png?v=0" title="s_\varepsilon =\sqrt{\frac{RSS}{df_{Error}}}=\sqrt{\frac{RSS}{n^{*}-k}}" alt="s_\varepsilon =\sqrt{\frac{RSS}{df_{Error}}}=\sqrt{\frac{RSS}{n^{*}-k}}" class="tex"/>
</th>
<td>
<p>(15)  
</p>
</td></tr></table>
<p><img src="../images/Multiple_Regression_Results/math-7db27a440000c9858319d79169f3d2e1.png?v=0" title="s_\varepsilon^2" alt="s_\varepsilon^2" class="tex"/> is an estimate of <img src="../images/Multiple_Regression_Results/math-0a628ba9b1845b6acce4b4ff2c9ec575.png?v=0" title="\sigma ^2" alt="\sigma ^2" class="tex"/>, which is variance of <img src="../images/Multiple_Regression_Results/math-3a3ea00cfc35332cedf6e5e9a32e94da.png?v=0" title="E" alt="E" class="tex"/>
</p>
<table class="note">

<tr>
<td><b>Note:</b> Please read the <a href="../../UserGuide/UserGuide/Multiple_Regression_Results.html#ANOVA_Table" title="UserGuide:Multiple Regression Results">ANOVA Table</a> for more details about the <b>degree of freedom (df)</b>, dfError.
</td></tr></table>
<h4><a name="t-Value_and_Confidence_Level"></a><span class="mw-headline">t-Value and Confidence Level</span></h4>
<p>If the regression assumptions hold, we can perform the t-tests for the regression coefficients with the null hypotheses and the alternative hypotheses: 
</p><p><img src="../images/Multiple_Regression_Results/math-369bc614f725b892be5394c08b6ad6eb.png?v=0" title="H_0:\beta _j=0\,\!" alt="H_0:\beta _j=0\,\!" class="tex"/>
</p><p><img src="../images/Multiple_Regression_Results/math-166888ab5ad2710775199a0c06d617c3.png?v=0" title="H_\alpha&#160;:\beta _j\neq 0" alt="H_\alpha&#160;:\beta _j\neq 0" class="tex"/>  
</p><p>The <i>t</i>-values can be computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-7ddf34c66dcaeadff80cfc8fe6a519b4.png?v=0" title="t=\frac{\hat \beta _j-0}{s_{\hat \beta _j}}" alt="t=\frac{\hat \beta _j-0}{s_{\hat \beta _j}}" class="tex"/>
</th>
<td>
<p>(16)  
</p>
</td></tr></table>
<p>With the <i>t</i>-value, we can decide whether or not to reject the corresponding null hypothesis. Usually, for a given <b>Confidence Level for Parameters</b>: <img src="../images/Multiple_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/> , we can reject <img src="../images/Multiple_Regression_Results/math-806277203dedea2ed8321f6cbd465a54.png?v=0" title="H_0\,\!" alt="H_0\,\!" class="tex"/> when <img src="../images/Multiple_Regression_Results/math-9d38e8e7363d66af41530c3fbdf494f5.png?v=0" title="|t|&gt;t_{\frac \alpha 2}" alt="|t|&gt;t_{\frac \alpha 2}" class="tex"/>. Additionally, the <i>p</i>-value is less than <img src="../images/Multiple_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>.
</p>
<h4><a name="Prob.3E.7Ct.7C"></a><span class="mw-headline">Prob&gt;|t|</span></h4>
<p>The probability that <img src="../images/Multiple_Regression_Results/math-e65765bedcabe42c66ec93228769e82a.png?v=0" title="H_0" alt="H_0" class="tex"/> in the <i>t</i> test is true. 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-3e5335a131705b3ef0bb8b250033167d.png?v=0" title="prob=2(1-tcdf(|t|,df_{Error}))\,\!" alt="prob=2(1-tcdf(|t|,df_{Error}))\,\!" class="tex"/>
</th>
<td>
<p>(17)  
</p>
</td></tr></table>
<p>where <img src="../images/Multiple_Regression_Results/math-0e2972b44120411286c26cb3f7efd6fc.png?v=0" title="tcdf(|t|,df_{Error})" alt="tcdf(|t|,df_{Error})" class="tex"/> computes the cumulative distribution function of the Student's t distribution at the values |t|, with <b>degree of freedom of error</b> <img src="../images/Multiple_Regression_Results/math-20c9dbcbb2c4fbfd59f632e721a494c4.png?v=0" title="(df_{Error})" alt="(df_{Error})" class="tex"/>.
</p>
<h4><a name="LCL_and_UCL"></a><span class="mw-headline">LCL and UCL</span></h4>
<p>From the <i>t</i>-value, we can calculate the <img src="../images/Multiple_Regression_Results/math-0cdaba532a6f8ad706e6b8e5e97f0d36.png?v=0" title="(1-\alpha )\times 100\%" alt="(1-\alpha )\times 100\%" class="tex"/>  <b>Confidence Interval</b> for each parameter by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-a27fc4294ee80efb51793fbcd4e23df2.png?v=0" title="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" alt="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" class="tex"/>
</th>
<td>
<p>(18)  
</p>
</td></tr></table>
<p>where <img src="../images/Multiple_Regression_Results/math-fd9dfeda668ac9b0c5ef311ffdca8f71.png?v=0" title="UCL" alt="UCL" class="tex"/> and <img src="../images/Multiple_Regression_Results/math-a4df64dc5e32716f6de0ec15b0340950.png?v=0" title="LCL" alt="LCL" class="tex"/> is short for the <b>Upper Confidence Interval</b> and <b>Lower Confidence Interval</b>, respectively.
</p>
<h4><a name="CI_Half_Width"></a><span class="mw-headline">CI Half Width</span></h4>
<p>The Confidence Interval Half Width is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-77ec7c61ac9d06bfc9ad4c0ae47b85ce.png?v=0" title="CI=\frac{UCL-LCL}2" alt="CI=\frac{UCL-LCL}2" class="tex"/>
</th>
<td>
<p>(19)  
</p>
</td></tr></table>
<h3><a name="Fit_Statistics"></a><span class="mw-headline">Fit Statistics</span></h3>
<p>Some fit statistics formulas are summary here: 
</p>
<dl><dd><a  class="image"><img alt="Mfitted-stats.png" src="../images/Multiple_Regression_Results/Mfitted-stats.png?v=63253" width="285"  /></a></dd></dl>
<h4><a name="Degree_of_Freedom"></a><span class="mw-headline">Degree of Freedom</span></h4>
<p>The degree of freedom for (Error) variation. Please refer to the <a href="../../UserGuide/UserGuide/Multiple_Regression_Results.html#ANOVA_Table" title="UserGuide:Multiple Regression Results">ANOVA</a> table for more details.
</p>
<h4><a name="Reduced_Chi-Sqr"></a><span class="mw-headline">Reduced Chi-Sqr</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-311ddcb93ec3330a08a572efc3a55aa2.png?v=0" title="\sigma^2=\frac{RSS}{n^{*}-k}" alt="\sigma^2=\frac{RSS}{n^{*}-k}" class="tex"/>
</th>
<td>
<p>(20)  
</p>
</td></tr></table>
<h4><a name="Residual_Sum_of_Squares"></a><span class="mw-headline">Residual Sum of Squares</span></h4>
<p>The residual sum of squares, see formula (8).
</p>
<h4><a name="R-Square_.28COD.29"></a><span class="mw-headline">R-Square (COD)</span></h4>
<p>The goodness of fit can be evaluated by <b>Coefficient of Determination (COD)</b>, <img src="../images/Multiple_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/>, which is given by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-79eb5491ba841bf79f5acb17cb226da4.png?v=0" title="R^2=\frac{Explained\, variation}{Total \, variation}=1-\frac{RSS}{TSS}" alt="R^2=\frac{Explained\, variation}{Total \, variation}=1-\frac{RSS}{TSS}" class="tex"/>
</th>
<td>
<p>(21)  
</p>
</td></tr></table>
<h4><a name="Adj._R-Square"></a><span class="mw-headline">Adj. R-Square</span></h4>
<p>The adjusted <img src="../images/Multiple_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> is used to adjust the <img src="../images/Multiple_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> value for the degree of freedom. It can be computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-3dce278823ecfde5fe7c017ab3681a62.png?v=0" title="\bar R^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" alt="\bar R^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" class="tex"/>
</th>
<td>
<p>(22)  
</p>
</td></tr></table>
<h4><a name="R_Value"></a><span class="mw-headline">R Value</span></h4>
<p>Then we can compute the <i>R</i>-value, which is simply the square root of <img src="../images/Multiple_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/>: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-8dea6550a6f24458c884bb59c192c935.png?v=0" title="R=\sqrt{R^2}" alt="R=\sqrt{R^2}" class="tex"/>
</th>
<td>
<p>(23)  
</p>
</td></tr></table>
<h4><a name="Root-MSE_.28SD.29"></a><span class="mw-headline">Root-MSE (SD)</span></h4>
<p>Root Mean Square of the Error, or residual standard deviation, which equals to: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-df94fd49f1b3b071f40637576e7aeb16.png?v=0" title="RootMSE=\sqrt{\frac{RSS}{n^*-k}}" alt="RootMSE=\sqrt{\frac{RSS}{n^*-k}}" class="tex"/>
</th>
<td>
<p>(24)  
</p>
</td></tr></table>
<h4><a name="Norm_of_Residuals"></a><span class="mw-headline">Norm of Residuals</span></h4>
<p>Equals to square root of RSS: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-edb52b53a5f0e033c1872d55206bf72f.png?v=0" title="Norm \,of \,Residuals=\sqrt{RSS}" alt="Norm \,of \,Residuals=\sqrt{RSS}" class="tex"/>
</th>
<td>
<p>(25)  
</p>
</td></tr></table>
<h2><a name="ANOVA_Table"></a><span class="mw-headline">ANOVA Table</span></h2>
<p>The ANOVA table of linear fitting is: 
</p>
<table class="simple">

<tr>
<th>
</th>
<th>df
</th>
<th>Sum of Squares
</th>
<th>Mean Square
</th>
<th>F Value
</th>
<th>Prob &gt; F
</th></tr>
<tr>
<th>Model
</th>
<td><i>k</i>
</td>
<td><i><img src="../images/Multiple_Regression_Results/math-a09c3010146eb14dac4eace6117896de.png?v=0" title="SS_{reg} = TSS-RSS" alt="SS_{reg} = TSS-RSS" class="tex"/> </i>
</td>
<td><i><img src="../images/Multiple_Regression_Results/math-191540b039674e13f1b8b80a26fc70b9.png?v=0" title="MS_{reg} = SS_{reg} / k " alt="MS_{reg} = SS_{reg} / k " class="tex"/></i>
</td>
<td><i><img src="../images/Multiple_Regression_Results/math-d5ea59127172a534421d4b08fd6d3135.png?v=0" title="MS_{reg} / MSE " alt="MS_{reg} / MSE " class="tex"/></i>
</td>
<td><i>p-value </i>
</td></tr>
<tr>
<th>Error
</th>
<td><i>n* - k</i>
</td>
<td><i><img src="../images/Multiple_Regression_Results/math-bf1981220040a8ac147698c85d55334f.png?v=0" title="RSS" alt="RSS" class="tex"/></i>
</td>
<td><i><img src="../images/Multiple_Regression_Results/math-41f5e35998833eae24be6c6ebb886ac4.png?v=0" title="MSE = RSS / (n^* - k)" alt="MSE = RSS / (n^* - k)" class="tex"/></i>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<th>Total
</th>
<td><i>n*</i>
</td>
<td><i><img src="../images/Multiple_Regression_Results/math-877b1540498bd253859cdebf2274ec8d.png?v=0" title="TSS" alt="TSS" class="tex"/></i>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></table>
<table class="note">

<tr>
<td><b>Note:</b> If intercept is included in the model, n*=n-1. Otherwise, n*=n and the total sum of squares is uncorrected.
</td></tr></table>
<p>Where the total sum of square, TSS, is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-4d28d19e4d080da070b15388db314470.png?v=0" title="TSS =\sum_{i=1}^nw_i(y_i -\frac{\sum_{i=1}^n w_i y_i} {\sum_{i=1}^n w_i})^2" alt="TSS =\sum_{i=1}^nw_i(y_i -\frac{\sum_{i=1}^n w_i y_i} {\sum_{i=1}^n w_i})^2" class="tex"/>       <i>(corrected)</i>
</th>
<td>(26)
</td></tr>
<tr>
<th><img src="../images/Multiple_Regression_Results/math-7c28116e7d91e4726fe4c0d26fa4e93e.png?v=0" title="TSS=\sum_{i=1}^n w_iy_i^2" alt="TSS=\sum_{i=1}^n w_iy_i^2" class="tex"/>  (uncorrected)
</th></tr></table>
<p>The F value here is a test of whether the fitting model differs significantly from the model y=constant. 
</p><p>Additionally, the <i>p</i>-value, or significance level, is reported with an <i>F</i>-test. We can reject the null hypothesis if the <i>p</i>-value is less than <img src="../images/Multiple_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>, which means that  fitting model differs significantly from the model y=constant.
</p><p>If fixing the intercept at a certain value, the p value for <i>F</i>-test is not meaningful, and it is different from that in multiple linear regression without the intercept constraint.
</p>
<h2><a name="Lack_of_fit_table"></a><span class="mw-headline">Lack of fit table</span></h2>
<p>To run the lack of fit test, you need to have repeated observations, namely, "replicate data" , so that at least one of the X values is repeated within the dataset, or within multiple datasets when concatenate fit mode is selected.
</p><p>Notations used for fit with replicates data:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-9ec98a656b3500239d77f631b67811cc.png?v=0" title="y_{ij}" alt="y_{ij}" class="tex"/> is the jth measurement made at the ith x-value in the data set<br />
</th></tr>
<tr>
<th><img src="../images/Multiple_Regression_Results/math-dbebef52b4445e3b11d20855d6e8d5dd.png?v=0" title="\bar{y}_{i}" alt="\bar{y}_{i}" class="tex"/> is the average of all of the y values at the ith x-value<br />
</th></tr>
<tr>
<th><img src="../images/Multiple_Regression_Results/math-0ff3619d52350e78a9bb731ef2c23991.png?v=0" title="\hat{y}_{ij}" alt="\hat{y}_{ij}" class="tex"/> is the predicted response for the jth measurement made at the ith x-value<br />
</th></tr></table>
<p>The sum of square in table below is expressed by:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-5c76b3b72fc70ebfe8fed6e8daf6a7c7.png?v=0" title="RSS=\sum_{i}\sum_{j}(y_{ij}-\hat{y}_{ij})^2" alt="RSS=\sum_{i}\sum_{j}(y_{ij}-\hat{y}_{ij})^2" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Multiple_Regression_Results/math-3e215ad1665f7e78c1af881e4c6e4ac6.png?v=0" title="LFSS=\sum_{i}\sum_{j}(\bar{y}_{i}-\hat{y}_{ij})^2" alt="LFSS=\sum_{i}\sum_{j}(\bar{y}_{i}-\hat{y}_{ij})^2" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Multiple_Regression_Results/math-b8b02bc61db58ee0bcf8927677ab3c59.png?v=0" title="PESS=\sum_{i}\sum_{j}(y_{ij}-\bar{y}_{i})^2" alt="PESS=\sum_{i}\sum_{j}(y_{ij}-\bar{y}_{i})^2" class="tex"/>
</th></tr></table>
<p>The Lack of fit table of linear fitting is: 
</p>
<table class="simple">

<tr>
<th>
</th>
<th>DF
</th>
<th>Sum of Squares
</th>
<th>Mean Square
</th>
<th>F Value
</th>
<th>Prob &gt; F
</th></tr>
<tr>
<th>Lack of Fit
</th>
<td><i>c-k-1</i>
</td>
<td><i>LFSS </i>
</td>
<td><i>MSLF = LFSS / (c - k - 1) </i>
</td>
<td><i>MSLF / MSPE</i>
</td>
<td><i>p-value </i>
</td></tr>
<tr>
<th>Pure Error
</th>
<td><i>n - c</i>
</td>
<td><i>PESS</i>
</td>
<td><i>MSPE = PESS / (n - c)</i>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<th>Error
</th>
<td><i>n*-k</i>
</td>
<td><i>RSS</i>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></table>
<table class="note">

<tr>
<td><b>Note</b>:<br />
<p>If intercept is included in the model, <i>n*=n-1</i>. Otherwise, <i>n*=n</i> and the total sum of squares is uncorrected. If the slope is fixed, <i><img src="../images/Multiple_Regression_Results/math-dfc3cbf5cc5bf940257cfcf8c86ddec0.png?v=0" title="df_{Model}" alt="df_{Model}" class="tex"/></i> = 0.  
</p><p><i>c</i> denotes the number of distinct x values. If intercept is fixed, DF for Lack of Fit is c-k.
</p>
</td></tr></table>
<h2><a name="Covariance_and_Correlation_Matrix"></a><span class="mw-headline">Covariance and Correlation Matrix</span></h2>
<p>The covariance matrix for the multiple linear regression can be calculated as 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-ed8ca3163756ee0bf47ac95149ddcacc.png?v=0" title="Cov(\beta _i,\beta _j)=\sigma ^2(X^{\prime }X)^{-1}" alt="Cov(\beta _i,\beta _j)=\sigma ^2(X^{\prime }X)^{-1}" class="tex"/>
</th>
<td>
<p>(27)  
</p>
</td></tr></table>
<p>The correlation between any two parameters is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-7b81d2f96b516f716d87ef2399134c1f.png?v=0" title="\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}}" alt="\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}}" class="tex"/>
</th>
<td>
<p>(28)  
</p>
</td></tr></table>
<h2><a name="Residual_Analysis"></a><span class="mw-headline">Residual Analysis</span></h2>
<p><img src="../images/Multiple_Regression_Results/math-af2593b49488f8bc44396795792c2d79.png?v=0" title="r_i" alt="r_i" class="tex"/> stands for the Regular Residual <img src="../images/Multiple_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/>.
</p>
<h3><a name="Standardized"></a><span class="mw-headline">Standardized</span></h3>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-edba44ad3ec53a729a081e552ffd8d52.png?v=0" title="r_i^{\prime }=\frac{r_i}s_\varepsilon" alt="r_i^{\prime }=\frac{r_i}s_\varepsilon" class="tex"/>
</th>
<td>
<p>(29)
</p>
</td></tr></table>
<h3><a name="Studentized"></a><span class="mw-headline">Studentized</span></h3>
<p>Also known as internally studentized residual.
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-5b4f1a55a60f41732cd45963fc4d7899.png?v=0" title="r_i^{\prime }=\frac{r_i}{s_\varepsilon\sqrt{1-h_i}}" alt="r_i^{\prime }=\frac{r_i}{s_\varepsilon\sqrt{1-h_i}}" class="tex"/>
</th>
<td>
<p>(30)
</p>
</td></tr></table>
<h3><a name="Studentized_deleted"></a><span class="mw-headline">Studentized deleted</span></h3>
<p>Also known as externally studentized residual.
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-33555ae8465559528419ee80ff636f91.png?v=0" title="r_i^{\prime }=\frac{r_i}{s_{\varepsilon-i}\sqrt{1-h_i}}" alt="r_i^{\prime }=\frac{r_i}{s_{\varepsilon-i}\sqrt{1-h_i}}" class="tex"/>
</th>
<td>
<p>(31)
</p>
</td></tr></table>
<p>In the equations for the <b>Studentized</b> and <b>Studentized deleted</b> residuals, <img src="../images/Multiple_Regression_Results/math-57339b77b3af15427b7154f4daf8a223.png?v=0" title="h_i" alt="h_i" class="tex"/> is the <i>i</i>th diagonal element of the matrix <img src="../images/Multiple_Regression_Results/math-44c29edb103a2872f519ad0c9a0fdaaa.png?v=0" title="P" alt="P" class="tex"/>:
</p>
<table class="formula">

<tr>
<th><img src="../images/Multiple_Regression_Results/math-609f90ce395d2346200e55f1a632bab0.png?v=0" title="P=X(X&#39;X)^{-1}X^{\prime }" alt="P=X(X&#39;X)^{-1}X^{\prime }" class="tex"/>
</th>
<td>
<p>(32)
</p>
</td></tr></table>
<p><img src="../images/Multiple_Regression_Results/math-7af4042b185319dd95cc2dcb158a88b2.png?v=0" title="s_{\varepsilon-i}" alt="s_{\varepsilon-i}" class="tex"/> means the variance is calculated based on all points but exclude the <i>i</i>th.
</p>
<h2><a name="Plots"></a><span class="mw-headline">Plots</span></h2>
<h3><a name="Partial_Leverage_Plots"></a><span class="mw-headline">Partial Leverage Plots</span></h3>
<p>In multiple regression, partial leverage plots can be used to study the relationship between the independent variable and a given dependent variable. In the plot, the partial residual of Y is plotted against the partial residual of X, or the intercept. The partial residual of a certain variable is the regression residual with that variable omitted in the model. 
</p><p>Take the model <img src="../images/Multiple_Regression_Results/math-1806999a4d966f305fa7fccc2cd26b87.png?v=0" title="y=\beta _0+\beta _1x_1+\beta _2x_2\,\!" alt="y=\beta _0+\beta _1x_1+\beta _2x_2\,\!" class="tex"/> for example: the partial leverage plot for <img src="../images/Multiple_Regression_Results/math-d60f5062564e1ece65993038b62484fa.png?v=0" title="x_1\,\!" alt="x_1\,\!" class="tex"/> is created by plotting the regression residual of <img src="../images/Multiple_Regression_Results/math-fba661434598f68730f693373f85c059.png?v=0" title="y=\beta _0+\beta _2x_2\,\!" alt="y=\beta _0+\beta _2x_2\,\!" class="tex"/> against the residual of <img src="../images/Multiple_Regression_Results/math-5cdf71b9084554798997f31cf4a2302d.png?v=0" title="x_1=\beta _0+\beta _2x_2\,\!" alt="x_1=\beta _0+\beta _2x_2\,\!" class="tex"/>.
</p>
<h3><a name="Resudial_Type"></a><span class="mw-headline">Resudial Type</span></h3>
<p>Select one residual type among <b>Regular</b>, <b>Standardized</b>, <b>Studentized</b>, <b>Studentized Deleted</b> for Plots.
</p>
<h3><a name="Residual_vs._Independent"></a><span class="mw-headline">Residual vs. Independent</span></h3>
<p>Scatter plot of residual <img src="../images/Multiple_Regression_Results/math-9b207167e5381c47682c6b4f58a623fb.png?v=0" title="res" alt="res" class="tex"/> vs. indenpendent variable <img src="../images/Multiple_Regression_Results/math-75087036f44bb88958df97586ae3bd1e.png?v=0" title="x_1,x_2,\dots,x_k" alt="x_1,x_2,\dots,x_k" class="tex"/>, each plot is locate in a seperate graphs.
</p>
<h3><a name="Residual_vs._Predicted_Value"></a><span class="mw-headline">Residual vs. Predicted Value</span></h3>
<p>Scatter plot of residual <img src="../images/Multiple_Regression_Results/math-9b207167e5381c47682c6b4f58a623fb.png?v=0" title="res" alt="res" class="tex"/> vs. fitted results <img src="../images/Multiple_Regression_Results/math-afe7f90dd34deaabbae2c26aab299e8c.png?v=0" title="\hat{Y}" alt="\hat{Y}" class="tex"/>.
</p>
<h3><a name="Residual_vs._Order_of_the_Data"></a><span class="mw-headline">Residual vs. Order of the Data</span></h3>
<p><img src="../images/Multiple_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/> vs. sequence number <img src="../images/Multiple_Regression_Results/math-865c0c0b4ab0e063e5caa3387c1a8741.png?v=0" title="i" alt="i" class="tex"/>
</p>
<h3><a name="Histogram_of_the_Residual"></a><span class="mw-headline">Histogram of the Residual</span></h3>
<p>The Histogram plot of the Residual <img src="../images/Multiple_Regression_Results/math-9b207167e5381c47682c6b4f58a623fb.png?v=0" title="res" alt="res" class="tex"/>
</p>
<h3><a name="Residual_Lag_Plot"></a><span class="mw-headline">Residual Lag Plot</span></h3>
<p>Residuals <img src="../images/Multiple_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/> vs. lagged residual <img src="../images/Multiple_Regression_Results/math-00407b58eb35fecbfea6dc5d4413e493.png?v=0" title="res_{(i&#8211;1)}" alt="res_{(i&#8211;1)}" class="tex"/>.
</p>
<h3><a name="Normal_Probability_Plot_of_Residuals"></a><span class="mw-headline">Normal Probability Plot of Residuals</span></h3>
<p>A normal probability plot of the residuals can be used to check whether the variance is normally distributed as well. If the resulting plot is approximately linear, we proceed to assume that the error terms are normally distributed. The plot is based on the percentiles versus ordered residual, the  percentiles is estimated by 
</p><p><img src="../images/Multiple_Regression_Results/math-ec995bff3922b2290f85bed243de5eb3.png?v=0" title="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" alt="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" class="tex"/> 
</p><p>where <i>n</i> is the total number of dataset and  <i>i</i> is the <i>i</i> th data. Also refer to <a href="../../UserGuide/UserGuide/Probability_Plot_and_Q-Q_Plot.html" title="UserGuide:Probability Plot and Q-Q Plot">Probability Plot and Q-Q Plot</a>
</p>





