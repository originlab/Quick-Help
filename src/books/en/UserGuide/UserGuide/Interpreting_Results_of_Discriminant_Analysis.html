<h1 class="firstHeading">17.7.4.2 Interpreting Results of Discriminant Analysis</h1><p class='urlname' style='display: none'>DiscAnalysis-Result</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Discriminant_Report_Sheet"><span class="tocnumber">1</span> <span class="toctext">Discriminant Report Sheet</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Descriptive_Statistics"><span class="tocnumber">1.1</span> <span class="toctext">Descriptive Statistics</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#Covariance_Matrix_.28Total.29"><span class="tocnumber">1.2</span> <span class="toctext">Covariance Matrix (Total)</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Correlation_Matrix_.28Total.29"><span class="tocnumber">1.3</span> <span class="toctext">Correlation Matrix (Total)</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Group_Distance_Matrix"><span class="tocnumber">1.4</span> <span class="toctext">Group Distance Matrix</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Univariate_ANOVA"><span class="tocnumber">1.5</span> <span class="toctext">Univariate ANOVA</span></a></li>
<li class="toclevel-2 tocsection-7"><a href="#Equality_Test_of_Covariance_Matrices"><span class="tocnumber">1.6</span> <span class="toctext">Equality Test of Covariance Matrices</span></a>
<ul>
<li class="toclevel-3 tocsection-8"><a href="#LOG_of_Determinants"><span class="tocnumber">1.6.1</span> <span class="toctext">LOG of Determinants</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="#Likelihood-ratio_Test"><span class="tocnumber">1.6.2</span> <span class="toctext">Likelihood-ratio Test</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-10"><a href="#Pooled_Within-group_Covariance.2FCorrelation_Matrix"><span class="tocnumber">1.7</span> <span class="toctext">Pooled Within-group Covariance/Correlation Matrix</span></a></li>
<li class="toclevel-2 tocsection-11"><a href="#Within-group_Covariance_Matrix"><span class="tocnumber">1.8</span> <span class="toctext">Within-group Covariance Matrix</span></a></li>
<li class="toclevel-2 tocsection-12"><a href="#Canonical_Discriminant_Analysis"><span class="tocnumber">1.9</span> <span class="toctext">Canonical Discriminant Analysis</span></a>
<ul>
<li class="toclevel-3 tocsection-13"><a href="#Eigenvalues"><span class="tocnumber">1.9.1</span> <span class="toctext">Eigenvalues</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#Wilks.27_Lamba_Test"><span class="tocnumber">1.9.2</span> <span class="toctext">Wilks' Lamba Test</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Standardized_Canonical_Coefficients"><span class="tocnumber">1.9.3</span> <span class="toctext">Standardized Canonical Coefficients</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="#Unstandardized_Canonical_Coefficients"><span class="tocnumber">1.9.4</span> <span class="toctext">Unstandardized Canonical Coefficients</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#Canonical_Structure_Matix"><span class="tocnumber">1.9.5</span> <span class="toctext">Canonical Structure Matix</span></a></li>
<li class="toclevel-3 tocsection-18"><a href="#Canonical_Group_Means"><span class="tocnumber">1.9.6</span> <span class="toctext">Canonical Group Means</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-19"><a href="#Coefficients_of_Linear_Discriminant_Function"><span class="tocnumber">1.10</span> <span class="toctext">Coefficients of Linear Discriminant Function</span></a></li>
<li class="toclevel-2 tocsection-20"><a href="#Classification_Summary_for_Training_Data"><span class="tocnumber">1.11</span> <span class="toctext">Classification Summary for Training Data</span></a>
<ul>
<li class="toclevel-3 tocsection-21"><a href="#Classification_Count"><span class="tocnumber">1.11.1</span> <span class="toctext">Classification Count</span></a></li>
<li class="toclevel-3 tocsection-22"><a href="#Error_Rate"><span class="tocnumber">1.11.2</span> <span class="toctext">Error Rate</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-23"><a href="#Cross-validation_Summary_for_Training_Data"><span class="tocnumber">1.12</span> <span class="toctext">Cross-validation Summary for Training Data</span></a></li>
<li class="toclevel-2 tocsection-24"><a href="#Classification_Summary_for_Test_Data"><span class="tocnumber">1.13</span> <span class="toctext">Classification Summary for Test Data</span></a></li>
<li class="toclevel-2 tocsection-25"><a href="#Classification_Summary_Plot"><span class="tocnumber">1.14</span> <span class="toctext">Classification Summary Plot</span></a></li>
<li class="toclevel-2 tocsection-26"><a href="#Classification_Fit_Plot"><span class="tocnumber">1.15</span> <span class="toctext">Classification Fit Plot</span></a></li>
<li class="toclevel-2 tocsection-27"><a href="#Canonical_Score_Plot"><span class="tocnumber">1.16</span> <span class="toctext">Canonical Score Plot</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-28"><a href="#Training.2FTest_Result"><span class="tocnumber">2</span> <span class="toctext">Training/Test Result</span></a>
<ul>
<li class="toclevel-2 tocsection-29"><a href="#Classification"><span class="tocnumber">2.1</span> <span class="toctext">Classification</span></a></li>
<li class="toclevel-2 tocsection-30"><a href="#Post_Probabilities"><span class="tocnumber">2.2</span> <span class="toctext">Post Probabilities</span></a></li>
<li class="toclevel-2 tocsection-31"><a href="#Atypicality_Index"><span class="tocnumber">2.3</span> <span class="toctext">Atypicality Index</span></a></li>
<li class="toclevel-2 tocsection-32"><a href="#Distance"><span class="tocnumber">2.4</span> <span class="toctext">Distance</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-33"><a href="#Canonical_Scores"><span class="tocnumber">3</span> <span class="toctext">Canonical Scores</span></a></li>
</ul>
</div>

<h2><a name="Discriminant_Report_Sheet"></a><span class="mw-headline">Discriminant Report Sheet</span></h2>
<h3><a name="Descriptive_Statistics"></a><span class="mw-headline">Descriptive Statistics</span></h3>
<p>The descriptive statistics table is useful in determining the nature of variables. We will know magnitude and missing values of data. Inspection of means and SDs can reveal univariate/variance difference between the groups.
</p>
<h3><a name="Covariance_Matrix_.28Total.29"></a><span class="mw-headline">Covariance Matrix (Total)</span></h3>
<p>The <b>Covariance Matrix (Total)</b> provide the covariance matrix of whole observations by treating all observations as from a single sample
</p>
<h3><a name="Correlation_Matrix_.28Total.29"></a><span class="mw-headline">Correlation Matrix (Total)</span></h3>
<p>The table can be used to reveal the relationship between each variables.
</p>
<h3><a name="Group_Distance_Matrix"></a><span class="mw-headline">Group Distance Matrix</span></h3>
<p>The <b>Group Distance Matrix</b> provides the Mahalanobis distances between group means.
</p>
<table class="note">

<tr>
<td><b>Note:</b> The group means are values computed in the <b><a href="#Descriptive_Statistics">Descriptive Statistics table</a></b>,  which are different from the <a href="#Canonical_Group_Means">Canonical Group Means</a>
</td></tr></table>
<h3><a name="Univariate_ANOVA"></a><span class="mw-headline">Univariate ANOVA</span></h3>
<p>The table is to test the difference in group means for each variables. If the value of <b>Prob&gt;F</b> is smaller than 0.05, it means the means of each group are significant different. Please note that if the variables are related, the result of table is not reliable . This univariate perspective does not account for any share variance(correlation) among the variables.
</p>
<h3><a name="Equality_Test_of_Covariance_Matrices"></a><span class="mw-headline">Equality Test of Covariance Matrices</span></h3>
<p>Discriminant analysis assumes covariance matrices are equivalent. If the assumption is not satisfied, there are several options to consider, including elimination of outliers, data transformation, and use of the separate covariance matrices instead of the pool one normally used in discriminant analysis, i.e. <b>Quadratic</b> method 
</p>
<h4><a name="LOG_of_Determinants"></a><span class="mw-headline">LOG of Determinants</span></h4>
<p>The table output the natural log of the determinants of each group's covariance matrix and the pooled within-group covariance. Ideally the determinants should be almost equal to one another for the assumption of equality of covariance matrices.
</p>
<h4><a name="Likelihood-ratio_Test"></a><span class="mw-headline">Likelihood-ratio Test</span></h4>
<p>The Likelihood-ratio test is to test whether the population covariance matrices within groups are equal. If the <b>p-value</b> &gt; 0.05, we can say the covariance matrices are equal. Please note that the data is assumed to follow a multivariate Normal distribution with the variance-covariance matrix of the group. However, because discriminant analysis is rather robust against violation of these assumptions, as a rule of thumb we generally don't get too concerned with significant results for this test.
</p>
<h3><a name="Pooled_Within-group_Covariance.2FCorrelation_Matrix"></a><span class="mw-headline">Pooled Within-group Covariance/Correlation Matrix</span></h3>
<p>The Pooled Within-group Correlation matrix provides bivariate correlations between all variables. It can be used to detect potential problems with multicolliearity, Please pay attention if several correlation coefficient are larger than 0.8.
</p>
<h3><a name="Within-group_Covariance_Matrix"></a><span class="mw-headline">Within-group Covariance Matrix</span></h3>
<p>Separate covariance matrices for each group.
</p>
<h3><a name="Canonical_Discriminant_Analysis"></a><span class="mw-headline">Canonical Discriminant Analysis</span></h3>
<h4><a name="Eigenvalues"></a><span class="mw-headline">Eigenvalues</span></h4>
<p>The Eigenvalues table outputs the eigenvalues of the discriminant functions, it also reveal the canonical correlation for the discriminant function. The larger the eigenvalue is, the more amount of variance shared the linear combination of variables. The eigenvalues are sorted in descending order of importance. So the first one always explains that majority of variance in the relationship. 
</p><p>The second columns of the table, <b>Percentage of Variance</b> reveal the importance of the discriminant function. and the third column, <b>Cumulative</b> provides the cumulative percetage of the varaiance as each function is added the to table. If there are several discriminant functions, we can say the first few with comulative percetages largher than 90% are most important in the analysis.
</p><p>The fourth column, <b>Canonical Correlation</b> provides the canonical correlation coefficient for each function. We can say the canonical correlation value is the r value between discriminat scores on the function and each group. It also can be used to compare the importance of each discriminant function.
</p>
<h4><a name="Wilks.27_Lamba_Test"></a><span class="mw-headline">Wilks' Lamba Test</span></h4>
<p>Wilks' Lambda test is to test which variable contribute significance in discriminat function. The closer Wilks' lambda is to 0, the more the variable contributes to the discriminant function. The table also provide a Chi-Square statsitic to test the significance of Wilk's Lambda. If the p-value if less than 0.05, we can conclude that the corresponding function explain the group membership well.
</p>
<h4><a name="Standardized_Canonical_Coefficients"></a><span class="mw-headline">Standardized Canonical Coefficients</span></h4>
<p>The standardized canonical discriminant coefficients can be used to rank the importance of each variables. A high standardized discriminant function coefficient might mean that the groups differ a lot on that variable
</p>
<h4><a name="Unstandardized_Canonical_Coefficients"></a><span class="mw-headline">Unstandardized Canonical Coefficients</span></h4>
<p>The unstandardized canonical coefficients is the estimate of parameters, <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-4bd1241d43b60e0e4190660b97d2f686.png?v=0" title="C_i" alt="C_i" class="tex"/> of the equation below
</p>
<table class="formula">

<tr>
<th> <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-d5f79805b30f2240fbd388bfdbe902e1.png?v=0" title="D_j = C_0 + C_1X_1j + C_2X_2j + ... +C_nX_nj " alt="D_j = C_0 + C_1X_1j + C_2X_2j + ... +C_nX_nj " class="tex"/>
</th>
<td> (1)
</td></tr></table>
<p><br />
where 
</p>
<ul><li> <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-5b58cc0cefa1115cdeb54f391b25591d.png?v=0" title="D_j" alt="D_j" class="tex"/> is the discriminant score for <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-c3476ea2ed6144bb64b9d14534f34230.png?v=0" title="jth" alt="jth" class="tex"/> observation. </li>
<li> <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-d80fda5bd18bd90b83fb2b82d319e87a.png?v=0" title="X_ij" alt="X_ij" class="tex"/>  is the <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-c3476ea2ed6144bb64b9d14534f34230.png?v=0" title="jth" alt="jth" class="tex"/> observation for the <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-811ed2017ce127575a960555aa6179d7.png?v=0" title="ith" alt="ith" class="tex"/> variable</li></ul>
<p>The purpose of canonical discriminant analysis is to find out the best coefficient estimation to maximize the difference in mean discriminant score between groups.
</p>
<h4><a name="Canonical_Structure_Matix"></a><span class="mw-headline">Canonical Structure Matix</span></h4>
<p>The canonical structure matrix reveals the correlations between each variables in the model and the discriminant functions. We can say they are factor loadings of the variables on each discriminant function.  It allows us to compare correlations and see how closely a variable is related to each function. Generally, any variables with a correlation of 0.3 or more is considered to be important. 
</p><p>The canonical structure matrix should be used to assign meaningful labels to the discriminant functions. The standardized discriminant function coefficients should be used to assess the importance of each independent variable's unique contribution to the discriminant function.
</p>
<h4><a name="Canonical_Group_Means"></a><span class="mw-headline">Canonical Group Means</span></h4>
<p>The Canonical group means is also called group centroids, are the mean for each group's canonical observation scores which are computed by <b>equation (1)</b>. The larger the difference between the canonical group means, the better the predictive power of the canonical discriminant function in classifying observations.
</p>
<h3><a name="Coefficients_of_Linear_Discriminant_Function"></a><span class="mw-headline">Coefficients of Linear Discriminant Function</span></h3>
<p>The <b>Coefficients of Linear Discriminant Function</b> table interprets the Fisher's theory, so is only available when <b>Linear</b> mode is selected for <b>Discriminant Function</b>
</p><p>The linear discriminant functions, also called "classification functions" ,for each observation, have following form
</p>
<table class="formula">

<tr>
<th> <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-05f8874658ad18517465794a8162b11b.png?v=0" title="C_k = C_{k0} + C_{k1}X_1 + C_{k2}X_2 + ... +C_{km}X_m " alt="C_k = C_{k0} + C_{k1}X_1 + C_{k2}X_2 + ... +C_{km}X_m " class="tex"/>
</th>
<td> (2)
</td></tr></table>
<p><br />
where 
</p>
<ul><li> <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-72abb72bf267ec1843c375f2d32f4e09.png?v=0" title="C_k" alt="C_k" class="tex"/> is the classification score for group <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-8ce4b16b22b58894aa86c421e8759df3.png?v=0" title="k" alt="k" class="tex"/></li>
<li> <img src="../images/Interpreting_Results_of_Discriminant_Analysis/math-683e43b184b24ea5529774cc3c40664c.png?v=0" title="C&#39;s" alt="C&#39;s" class="tex"/>  are the coefficients in table</li></ul>
<p><br />
For one observation, we can compute it's score for each group by the coefficients according to equation (2). The observation should be assign to the group with highest score. 
</p><p>In addition, the coefficients are helpful in deciding which variable affects more in classification. Comparing the values between groups, the higher coefficient means the variable attributes more for that group.
</p>
<h3><a name="Classification_Summary_for_Training_Data"></a><span class="mw-headline">Classification Summary for Training Data</span></h3>
<h4><a name="Classification_Count"></a><span class="mw-headline">Classification Count</span></h4>
<p>The rows in the <b>Classification Count table</b> are the observed groups of the observations and the columns are the predicted groups. Values in the diagonal of the table reflect the correct classification of observations into groups.
</p>
<h4><a name="Error_Rate"></a><span class="mw-headline">Error Rate</span></h4>
<p>The <b>Error Rate</b> table lists the prior probability of each groups and the rate for misclassification.
</p>
<h3><a name="Cross-validation_Summary_for_Training_Data"></a><span class="mw-headline">Cross-validation Summary for Training Data</span></h3>
<p>In cross-validation, each training data is treated as the test data, exclude it from training data to judge which group it should be classified to, and then verify whether the classification is correct or not. The <b>Classification Count</b> and the <b>Error Rate</b> table has the same meaning as <b>Classification Summary for Training Data</b> branch
</p>
<h3><a name="Classification_Summary_for_Test_Data"></a><span class="mw-headline">Classification Summary for Test Data</span></h3>
<p><b>The Classification Summary for Test Data</b> table summarizes how to test data are classified. List how many test data in each groups and it's corresponding percent.
</p>
<h3><a name="Classification_Summary_Plot"></a><span class="mw-headline">Classification Summary Plot</span></h3>
<p>The <b>Classification Summary Plot</b> virtually shows the observed group v.s. predicted groups. The more the grouped color for the bar, the correcter the classification is.
</p>
<h3><a name="Classification_Fit_Plot"></a><span class="mw-headline">Classification Fit Plot</span></h3>
<p>Values in the diagonal of the classification table reflect the correct classification of individuals into groups by plotting the observation's posterior probability v.s their their scores on the discriminant dimensions. We should pay attention to the outliers in the plot, it shows the observation that might be misclassified to.
</p>
<h3><a name="Canonical_Score_Plot"></a><span class="mw-headline">Canonical Score Plot</span></h3>
<p>The canonical score plot shows how the first two canonical function classify observation between groups by plotting the observation score, computed via <b>Equation (1)</b>. The plot provides a succinct summary of the separation of the observations. The clearer the observations are grouping to, the better the discriminant model is.
</p>
<table class="note">

<tr>
<td><b>Note:</b> We only provides canonical score plot for the first two canonical functions, as they are also the two reflects the most variance in discriminant model. However, if you want to plot canonical score plot for other canonical functions, Please plot it by yourself with the data in <b>Canonical Scores</b> sheet
</td></tr></table>
<h2><a name="Training.2FTest_Result"></a><span class="mw-headline">Training/Test Result</span></h2>
<h3><a name="Classification"></a><span class="mw-headline">Classification</span></h3>
<p>We will show the source training data, observed group and predicted group in the Training Results. From the <b>From Group</b> column and <b>Allocated to Group</b> column, we can conclude the <b><a href="#Classification_Summary_for_Training_Data">Classification Summary for Training Data</a></b>
</p>
<h3><a name="Post_Probabilities"></a><span class="mw-headline">Post Probabilities</span></h3>
<p>The <b>Post Probabilities</b> indicates the probability that the observation in the group. The observation will be located to a group with the highest posterior probability.
</p>
<h3><a name="Atypicality_Index"></a><span class="mw-headline">Atypicality Index</span></h3>
<p>The <b>atypicality index</b> presents the probabilities of obtaining an observation more typical of predicted group than the observed group. If most value in the atypicality index column are close to 1, it means the observations may come from a grouping not represented in the training set.
</p>
<h3><a name="Distance"></a><span class="mw-headline">Distance</span></h3>
<p>Distance is the Mahalanobis distrances from each of group means to the observation. The observation is classified to the group to which it is closest, i.e. the distance value is the smallest
</p>
<h2><a name="Canonical_Scores"></a><span class="mw-headline">Canonical Scores</span></h2>
<p>The <b>Canonical Scores</b> sheet list the observations in training and test data set and their corresponding canonical scores computed by <b>Equation (1)</b>
</p>





