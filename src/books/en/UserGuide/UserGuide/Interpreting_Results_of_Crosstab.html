<h1 class="firstHeading">17.1.4.2 Interpreting Results and Chi-square of Cross Tabulation</h1><p class='urlname' style='display: none'>crosstab-Results</p>
<p><br />
</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Contingency_Table"><span class="tocnumber">1</span> <span class="toctext">Contingency Table</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Chi-Square_Tests"><span class="tocnumber">2</span> <span class="toctext">Chi-Square Tests</span></a></li>
<li class="toclevel-1 tocsection-3"><a href="#Fisher.27s_Exact_Table"><span class="tocnumber">3</span> <span class="toctext">Fisher's Exact Table</span></a></li>
<li class="toclevel-1 tocsection-4"><a href="#Measures_of_Association"><span class="tocnumber">4</span> <span class="toctext">Measures of Association</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Measures_for_Nominal_Variables"><span class="tocnumber">4.1</span> <span class="toctext">Measures for Nominal Variables</span></a></li>
<li class="toclevel-2 tocsection-6"><a href="#Measures_for_Ordinal_Variables"><span class="tocnumber">4.2</span> <span class="toctext">Measures for Ordinal Variables</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-7"><a href="#Agreement_Statistic"><span class="tocnumber">5</span> <span class="toctext">Agreement Statistic</span></a>
<ul>
<li class="toclevel-2 tocsection-8"><a href="#Kappa_Test"><span class="tocnumber">5.1</span> <span class="toctext">Kappa Test</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Bowker.27s_Test"><span class="tocnumber">5.2</span> <span class="toctext">Bowker's Test</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Odds_Ratio_.26_Relative_Risk"><span class="tocnumber">6</span> <span class="toctext">Odds Ratio &amp; Relative Risk</span></a></li>
<li class="toclevel-1 tocsection-11"><a href="#CMH_Table"><span class="tocnumber">7</span> <span class="toctext">CMH Table</span></a>
<ul>
<li class="toclevel-2 tocsection-12"><a href="#Conditional_Independence_Test"><span class="tocnumber">7.1</span> <span class="toctext">Conditional Independence Test</span></a></li>
<li class="toclevel-2 tocsection-13"><a href="#Odds_Ratio_Homogeneity_Tests"><span class="tocnumber">7.2</span> <span class="toctext">Odds Ratio Homogeneity Tests</span></a></li>
<li class="toclevel-2 tocsection-14"><a href="#Common_Odds_Ratio"><span class="tocnumber">7.3</span> <span class="toctext">Common Odds Ratio</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-15"><a href="#Mosaic_Plot"><span class="tocnumber">8</span> <span class="toctext">Mosaic Plot</span></a></li>
</ul>
</div>

<h2><a name="Contingency_Table"></a><span class="mw-headline">Contingency Table</span></h2>
<p>Contingency Table gives the information about the frequency distribution of the variables, including counts, percentages and residuals. 
Counts, Row%, Col% and Total% helps user to compare the levels across the groups. 
</p><p>Residuals are statistics to test the independency of the column and row variable.The more the value is close to zero, the more likely the column and row variable has no association
</p><p>Adjusted residual is the most useful residual as it is standardized to N(0,1), for comparing between cells. If the value  is larger than 1.96 or less than -1.96, the observed count is significantly larger than or less than expected. The larger the value is, the more likely the column variable is associate with the row variable.
</p>
<h2><a name="Chi-Square_Tests"></a><span class="mw-headline">Chi-Square Tests</span></h2>
<p>The Chi-Square tests provides results to test the hypothesis that the row and column variables are independent. 
</p><p>Chi-Square Tests Table displays <b>ChiSquare, </b>DF<b> and </b>Prob &gt; ChiSq<b>(the p-value). </b>
</p><p>If <b>Prob &gt; ChiSq</b> is less than the significant level, we can say at the significant level, there is significant evidence of association between the row and column variables. Else, we can say at the significant level, there is no significant evidence of association between the row and column variables.
</p><p>Four tests are available.
</p>
<ul><li>Pearson Chi-Square: 
<dl><dd> It is the most widely used Chi-Square test. The test statistic is calculated by summing the squared deviations between observed and expected counts divided by expected counts. It has an approximately chi-squared distribution under large samples. So the test result is made by reference to the chi-squared distribution.</dd></dl></li>
<li>Likelihood Ratio
<dl><dd> Likelihood Ratio builds on the likelihood of the data under the null hypothesis of independence. It is used to compare the goodness of fit of the null model with the alternative model. The test statistic also has an approximately chi-squared distribution. It usually comes to similar result as Pearson Chi-Square.</dd></dl></li>
<li> Continuity Correction
<dl><dd>It is also referred to as Yates Continuity Correction, and is available only for a 2*2 table in Origin. If the expected number of observations in any category is too small(e,g,less than 5), the asymptotic chi-squared distribution is not quite correct, Pearson Chi-Square and Likelihood Ratio's results cannot be trusted and the Continuity correction is recommended. It is similar to the Pearson chi-square, except that it is adjusted for the continuity of the chi-squared distribution.</dd></dl></li>
<li>Linear Association
<dl><dd>It is available only for numeric data. The Chi-Square tests above do not take the ordering of the rows or columns into account, but Linear Association can do it. It is based on the Pearson correlation coefficient, and it has an approximately chi-squared distribution on 1 df.</dd></dl></li></ul>
<p><br />
</p>
<table class="note">

<tr>
<td><b>Notes:</b> if the expected number of observations in any category is too small(e,g,less than 5), <b>Pearson Chi-Square</b> and <b>Likelihood Ratio'</b>s results cannot be trusted.
</td></tr></table>
<h2><a name="Fisher.27s_Exact_Table"></a><span class="mw-headline">Fisher's Exact Table</span></h2>
<p>If the expected number of observations in any category is too small(e,g,less than 5), Chi-Square Tests may not be appropriate while Fisher's Exact Table is recommended. 
</p><p>Three tests are available, left-sided, right-sided and two-sided test. It enable user to know which A*B level combination is more likely to occur. You can look at the <b>Conclusion</b> column for the details. (A is for the row variable and B is for the column variable)
</p>
<table class="note">

<tr>
<td><b>Notes:</b> Note that Fisher's Exact test is available only for a 2*2 table
</td></tr></table>
<h2><a name="Measures_of_Association"></a><span class="mw-headline">Measures of Association</span></h2>
<p>Please look at the <a href="../../UserGuide/Category/Cross_Tabulation.html#Measures_for_Nominal_Variables" title="Category:Cross Tabulation">introduction page</a> for what situation the statistics should be used in
</p>
<h4><a name="Measures_for_Nominal_Variables"></a><span class="mw-headline">Measures for Nominal Variables</span></h4>
<ul><li> Phi
<dl><dd> For a 2*2 table, the range of the Phi is [-1,1]. For tables larger than 2*2, the range of the Phi is [0,M] (See <a href="../../UserGuide/UserGuide/Algorithm(CrossTabs).html" title="UserGuide:Algorithm(CrossTabs)">algorithm page</a> for M]). A larger value indicates the stronger association of two variables.</dd></dl></li>
<li> Contingency coefficient
<dl><dd> The range of value is [0,1). A larger value indicates the stronger association of two variables.</dd></dl></li>
<li> Cramer's V
<dl><dd> The values range from 0 to 1. A larger value indicates the stronger association of two variables.</dd></dl></li>
<li> Lambda
<dl><dd> Please look at the notes below for more information of C|R, R|C and Symmetric. A larger value indicates a stronger association</dd></dl></li>
<li> Uncertainty Coefficient
<dl><dd> Please look at the notes below for more information of C|R, R|C and Symmetric. A larger value indicates a stronger association</dd></dl></li></ul>
<table class="note">

<tr>
<td><b>Notes:</b>
<ul><li> C|R:
<dl><dd> The row variable(R) is regarded as an independent variable, while the column variable(C) is regarded as dependent variable. The value indicates by what percentage do we reduce our error when using the R to predict the C</dd></dl></li>
<li> R|C
<dl><dd> The column variable(C) is regarded as an independent variable, while the row variable(R) is regarded as dependent variable. The value indicates by what percentage do we reduce our error when using the C to predict the R</dd></dl></li>
<li> Symmetric:
<dl><dd> The variables are not be classified as independent and dependent. That is, it can only to measure the strength of association between the two variables but it can not predict how one variable affects another one</dd></dl></li></ul>
</td></tr></table>
<h4><a name="Measures_for_Ordinal_Variables"></a><span class="mw-headline">Measures for Ordinal Variables</span></h4>
<ul><li> Gamma</li>
<li> Take a range of values from -1 to +1. If it is positive, this means that the increase of one variable is likely to cause the increase of the other variable. While a negative value indicates a reverse relationship. The more the values is close to 0, the weaker the relationship is. </li>
<li> Kendall's tau-b and tau-c
<dl><dd> Similar to <b>Gamma</b> and with same results explanation.</dd></dl></li>
<li> Somer's D
<dl><dd> Please look at the notes below for more information of C|R, R|C and Symmetric. A larger value indicates a stronger association</dd></dl></li></ul>
<table class="note">

<tr>
<td><b>Notes:</b>
<ul><li> C|R:
<dl><dd> The row variable(R) is regarded as an independent variable, while the column variable(C) is regarded as dependent variable. The value indicates the strength of association while C depends on R.</dd></dl></li>
<li> R|C
<dl><dd> The column variable(C) is regarded as an independent variable, while the row variable(R) is regarded as dependent variable. The value indicates the strength of association while R depends on C.</dd></dl></li>
<li> Symmetric:
<dl><dd> The variables are not be classified as independent and dependent. That is, it can only to measure the strength of association between the two variables but it can not indicate how one variable affects another one</dd></dl></li></ul>
</td></tr></table>
<h2><a name="Agreement_Statistic"></a><span class="mw-headline">Agreement Statistic</span></h2>
<p>Please look at the <a href="../../UserGuide/Category/Cross_Tabulation.html#Measures_for_Nominal_Variables" title="Category:Cross Tabulation">introduction page</a> for what situation the statistics should be used in
</p>
<h3><a name="Kappa_Test"></a><span class="mw-headline">Kappa Test</span></h3>
<p>Kappa Test table displays the value of Kappa, standard error(SE), lower confidence limit(LCL) and upper confidence limit(UCL),Z value, Prob&gt;Z(the p-value for a one-sided test for Kappa),Prob&gt;|Z|(the p-value for a two-sided test for Kappa). 
</p><p>From the Kappa value, user will know the level of agreement the two rater agree to each other.
</p>
<dl><dd><ul><li> <b>&lt;=0</b>: no agreement</li>
<li> <b>0 - 0.4</b>: poor agreement</li>
<li> <b>0.4 - 0.59</b>: fair agreement</li>
<li> <b>0.6 - 0.74</b>: good agreement</li>
<li> <b>&gt; 0.75</b>: excellent agreement</li>
<li> <b>1</b>: complete agreement</li></ul></dd></dl>
<p>In the mean time, Kappa Test table also provide results for testing the hypothesis that Kappa equals to zero.
</p>
<dl><dd><ul><li>If "Prob&gt;Z" less than significant level, we can say that at the significant level, Kappa is significantly larger than zero. Else we can say  at the significant level,Kappa is significantly equals to zero.</li>
<li>If "Prob&gt;|Z|" less than significant level, we can say that at the significant level, Kappa is significantly different from zero. Else we can say  at the significant level,Kappa is significantly equals to zero.</li></ul></dd></dl>
<h3><a name="Bowker.27s_Test"></a><span class="mw-headline">Bowker's Test</span></h3>
<p>Bowker's Test table displays Chi-Square value, its DF and "Prob&gt;ChiSq"(p-value for the Bowker's test). It tests the equality of proportion in all matched-pairs cells that are symmetric around the diagonal (<img src="../images/Interpreting_Results_of_Crosstab/math-680315992f2510c407f4d63a43aa479f.png?v=0" title="P_{ij}=P_{ji}" alt="P_{ij}=P_{ji}" class="tex"/>)
</p>
<dl><dd><ul><li>If "Prob&gt;ChiSq" less than significant level, we can say that at the significant level, the frequency counts table is significantly asymmetric, that is,  <img src="../images/Interpreting_Results_of_Crosstab/math-3090c15be16c62f6a38d9628077d1878.png?v=0" title="P_{ij} \ne P_{ji}" alt="P_{ij} \ne P_{ji}" class="tex"/>. Else, we can say that at the significant level, the frequency counts table is Not significantly asymmetric, that is, <img src="../images/Interpreting_Results_of_Crosstab/math-680315992f2510c407f4d63a43aa479f.png?v=0" title="P_{ij}=P_{ji}" alt="P_{ij}=P_{ji}" class="tex"/></li></ul></dd></dl>
<h2><a name="Odds_Ratio_.26_Relative_Risk"></a><span class="mw-headline">Odds Ratio &amp; Relative Risk</span></h2>
<p>Odds Ratio &amp; Relative Risk is available only for a 2*2 table. Odds Ratio measures the ratio of the odds that an event or result will occur to the odds of the event not happening. Relative Risk measures the ratio of the odds of an event occurring in an group to the odds of the event occurring in a comparison group.
</p><p>Odds Ratio &amp; Relative Risk table displays the value, lower confidence limit(LCL) and upper confidence limit(UCL). Supposed Relative Risk =RR=P(a|b)/P(a|c), If RR=1, we can say that the probability of causing outcome a is the same in b and c; else if RR&gt;1, we can say that the probability of causing outcome a is greater in b than in c;else, we can say that the probability of causing outcome a is smaller in b than in c.
</p>
<h2><a name="CMH_Table"></a><span class="mw-headline">CMH Table</span></h2>
<p>Results of <b>Cochran-Mantel-Haenszel</b> tests. It is to test whether there is any relationship between the row and column variable after controlling for the layer variable
</p>
<h3><a name="Conditional_Independence_Test"></a><span class="mw-headline">Conditional Independence Test</span></h3>
<p>It is tested by Mantel-Haenszel statistic. The Mantel-Haenszel statistic tests the hypothesis that there is no significant association between the row and column variable, by controlling for the layer variable. Conditional Independence Test table displays Chi-Square value, its DF and "Prob&gt;ChiSq"(p-value for the Conditional Independence Test). 
</p>
<dl><dd><ul><li>If "Prob&gt;ChiSq" less than significant level, we can say that at the significant level, there is significant association between the row and column variable in at least one layer.Else,we can say that at the significant level, there is no significant association between the row and column variable in any layer.</li></ul></dd></dl>
<h3><a name="Odds_Ratio_Homogeneity_Tests"></a><span class="mw-headline">Odds Ratio Homogeneity Tests</span></h3>
<p>It is tested by Breslow-Day statistic and Tarone's statistic. They all test the hypothesis that odds ratio between the row and column variable is the same at each level of layer variable. 
</p><p>Odds Ratio Homogeneity Tests table displays Chi-Square value, its DF and "Prob&gt;ChiSq"(p-value for the Odds Ratio Homogeneity Tests).
For Breslow-Day statistic and Tarone's statistic,
</p>
<dl><dd><ul><li>If "Prob&gt;ChiSq" less than significant level, we can say that at the significant level, odds ratio is significantly different among layers. Else,we can say that at the significant level, odds ratio is NOT significantly different among layers.</li></ul></dd></dl>
<h3><a name="Common_Odds_Ratio"></a><span class="mw-headline">Common Odds Ratio</span></h3>
<p>The common odds ratio across layer variable is estimated by Mantel-Haenszel estimate. Common Odds Ratio table displays estimate of common odds ratio, "ln(estimate)" (The natural log of the estimated common odds ratio) and its standard error, lower confidence limit(LCL) and upper confidence limit(UCL).
</p>
<h2><a name="Mosaic_Plot"></a><span class="mw-headline">Mosaic Plot</span></h2>
<p>A mosaic plot is divided into rectangles, so that the area of each rectangle is proportional to the proportions of the Y variable in each level of the X variable.
</p>





