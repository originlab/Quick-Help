<h1 class="firstHeading">15.4.1 Additional Information of R-square</h1><p class='urlname' style='display: none'>Details_of_R_square</p>
<p>How good is the fit? One obvious metric is how close the fitted curve is to the actual data points. From the previous <a href="../../UserGuide/Category/Interpreting_Regression_Results.html#Illustration_of_the_Least-Squares_Method" title="Category:Interpreting Regression Results">section</a>, we know that the residual sum of square (<i>RSS</i>) or the reduced chi-square value is a quantitative value that can be used to evaluate this kind of distance. However, the value of residual sum of square (<i>RSS</i>) varies from dataset to dataset, making it necessary to rescale this value to a uniform range. On the other hand, one may want to use the mean of y value to describe the data feature. If this is the case, the fitted curve is a horizontal line<img src="../images/Details_of_R_square/math-70816b25c4d584534e7df04fc17bf02f.png?v=0" title="y=\overline{y}" alt="y=\overline{y}" class="tex"/>, and the predictor x, cannot linearly predict the y value.  To verify this, we first calculate the variation between data points and the mean, the "total sum of squares" about the mean, by
</p><p><img src="../images/Details_of_R_square/math-d13055e359b42226d0b8ae6ba915df63.png?v=0" title="TSS=\sum_{i=1}^n(y_i-\overline{y})^2 \,\!" alt="TSS=\sum_{i=1}^n(y_i-\overline{y})^2 \,\!" class="tex"/>
</p><p>In least-squares fitting, the <i>TSS</i> can be divided into two parts: the variation explained by regression and that not explained by regression: 
</p>
<table class="noborder">

<tr>
<td>
<ul><li>The regression sum of squares, <i>SSreg</i>, is the portion of the variation that is explained by the regression model.</li></ul>
</td>
<td>
<p><img src="../images/Details_of_R_square/math-f4247da7269e1f6bd96b1cb786a8daa7.png?v=0" title="SSreg=\sum_{i=1}^n(\widehat{y_i}-\overline{y})^2 \,\!" alt="SSreg=\sum_{i=1}^n(\widehat{y_i}-\overline{y})^2 \,\!" class="tex"/>
</p>
</td></tr>
<tr>
<td>
<ul><li>The residual sum of squares, <i>RSS</i>, is the portion that is not explained by the regression model.</li></ul>
</td>
<td>
<p><img src="../images/Details_of_R_square/math-634bbb984efc22841c46916fdb141a1e.png?v=0" title="RSS=\sum_{i=1}^n(y_i-\widehat{y_i})^2 \,\!" alt="RSS=\sum_{i=1}^n(y_i-\widehat{y_i})^2 \,\!" class="tex"/>
</p>
</td></tr></table>
<p>Clearly, the closer the data points are to the fitted curve, the smaller the <i>RSS</i> and the greater the proportion of the total variation that is represented by the <i>SSreg</i>.  Thus, the ratio of <i>SSreg</i> to <i>TSS</i> can be used as one measure of the quality of the regression model.  This quantity -- termed the <b>coefficient of determination</b> -- is computed as:
</p><p><img src="../images/Details_of_R_square/math-c816efca2c109e12f1095236ff58d882.png?v=0" title="R^2=\frac{SSreg}{TSS}=1-\frac{RSS}{TSS} \,\!" alt="R^2=\frac{SSreg}{TSS}=1-\frac{RSS}{TSS} \,\!" class="tex"/>
</p><p>From the above equation, we can see that when using a good fitting model, <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> should vary between 0 and 1. A value close to 1 indicates that the fit is a good one.
</p><p>Mathematically speaking, the degrees of freedom will affect <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/>. That is, when adding variables in the model, <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> will rise, but this does not imply a better fit. To avoid this effect, we can look at the adjusted <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/>:
</p><p><img src="../images/Details_of_R_square/math-56ed207e01d9d11e34c2fa201b98e00b.png?v=0" title="\overline{R}^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}} \,\!" alt="\overline{R}^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}} \,\!" class="tex"/>
</p><p>From the equation, we can see that adjusted <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> overcomes the rise in <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/>, especially when fitting a small sample size <i>(n)</i> by multiple predictor (k) model. Though we usually term the coefficient of determination as "<i>R</i>-square", it is actually not a "square" value of <i>R</i>. For most cases, it is a value between 0 and 1, but you may also find negative R^2 when the fit is poor. This occurs because the equation to calculate <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> is <img src="../images/Details_of_R_square/math-fe4da21819f56a1c315ec606c5fd159f.png?v=0" title="R^2 = 1 - RSS / TSS" alt="R^2 = 1 - RSS / TSS" class="tex"/>.  The second term will be greater then 1, when a bad model is used.
</p><p>However, using <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> or adjusted <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> is not sufficient.  For example, in the following graph, the fitted curve shown in plot B-D might have a high <img src="../images/Details_of_R_square/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> value, but apparently the models are wrong. So it is necessary to diagnose the regression result by the <a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html" title="UserGuide:Graphic Residual Analysis">Residual Analysis</a>.
</p>
<dl><dd><a  class="image"><img alt="Goodness of Fit.jpg" src="../images/Details_of_R_square/Goodness_of_Fit.jpg?v=12412" width="468"  /></a></dd></dl>
<h2><a name="R-Square_in_Linear_Fit"></a><span class="mw-headline">R-Square in Linear Fit</span></h2>
<h3><a name="Linear_Fit_for_Intercept_Included"></a><span class="mw-headline">Linear Fit for Intercept Included</span></h3>
<p>When intercept is included in linear fit , it follows the relation:
</p>
<dl><dd><img src="../images/Details_of_R_square/math-f27785e30b46e8d63fc44e61bc5097d7.png?v=0" title="\sum_{i=1}^n (y_i-\bar{y})^2 = \sum_{i=1}^n (y_i-f(x_i))^2 + \sum_{i=1}^n (f(x_i)-\bar{y})^2" alt="\sum_{i=1}^n (y_i-\bar{y})^2 = \sum_{i=1}^n (y_i-f(x_i))^2 + \sum_{i=1}^n (f(x_i)-\bar{y})^2" class="tex"/></dd></dl>
<p>where <img src="../images/Details_of_R_square/math-d50c99fc89a5682178a3b7e60966f560.png?v=0" title="(x_i, y_i) \; i=1..n \;" alt="(x_i, y_i) \; i=1..n \;" class="tex"/> are fitting data,  <img src="../images/Details_of_R_square/math-bacfc7141fdfd692244b6c50891d1f7b.png?v=0" title="\bar{y}" alt="\bar{y}" class="tex"/> denotes the mean of the dependent variable and <img src="../images/Details_of_R_square/math-2545f6aff9e599f7b0731247906b6b00.png?v=0" title="f(x_i) \;" alt="f(x_i) \;" class="tex"/> is the fitted value.
</p><p>The left hand side in the above equation is the total sum of squares, i.e.
</p>
<dl><dd><img src="../images/Details_of_R_square/math-65662ccc390f7d7235cfde7790068791.png?v=0" title="TSS = \sum_{i=1}^n (y_i-\bar{y})^2" alt="TSS = \sum_{i=1}^n (y_i-\bar{y})^2" class="tex"/></dd></dl>
<p>The first term on the right is the residual sum of squares, i.e.
</p>
<dl><dd><img src="../images/Details_of_R_square/math-5b6fe5a5e38d5b1e0c1295c200aef14f.png?v=0" title="RSS = \sum_{i=1}^n (y_i-f(x_i))^2" alt="RSS = \sum_{i=1}^n (y_i-f(x_i))^2" class="tex"/></dd></dl>
<p>And the second term on the right is the sum of squares due to regression, i.e.
</p>
<dl><dd><img src="../images/Details_of_R_square/math-98c4dfa1b52ea241deb3e3f72a8ede46.png?v=0" title="SSR = \sum_{i=1}^n (f(x_i)-\bar{y})^2" alt="SSR = \sum_{i=1}^n (f(x_i)-\bar{y})^2" class="tex"/></dd></dl>
<p>Hence <i><b>TSS = RSS + SSR</b></i>.
</p><p>And the coefficient of determination (<b>R-Square</b>) is defined by the ratio of SSR to TSS:
</p>
<dl><dd><img src="../images/Details_of_R_square/math-87012079a131e26d822f0af602776401.png?v=0" title="R^2=\frac{SSR}{TSS}=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^n (y_i-f(x_i))^2}{\sum_{i=1}^n (y_i-\bar{y})^2}" alt="R^2=\frac{SSR}{TSS}=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^n (y_i-f(x_i))^2}{\sum_{i=1}^n (y_i-\bar{y})^2}" class="tex"/></dd></dl>
<p>Therefore <b>R-Square</b> measures the proportion of variation of the dependent variable about the mean explained by the fitting when intercept is included.
</p>
<h3><a name="Linear_Fit_for_Fixed_Intercept"></a><span class="mw-headline">Linear Fit for Fixed Intercept</span></h3>
<p>However, when intercept is fixed in linear fit, the above relation in <a href="#Linear_Fit_for_Intercept_Included">Linear Fit for Intercept Included</a> is not satisfied. For a poor fit, it may result in a negative <b>R-Square</b> value using the definition in <a href="#Linear_Fit_for_Intercept_Included">Linear Fit for Intercept Included</a>. And this is not reasonable.
</p><p>When intercept is fixed in linear fit, it follows the relation below:
</p>
<dl><dd><img src="../images/Details_of_R_square/math-4bb63ecc585d86a8829dec4019fe998f.png?v=0" title="\sum_{i=1}^n y_i^2 = \sum_{i=1}^n (y_i-f(x_i))^2 + \sum_{i=1}^n (f(x_i))^2" alt="\sum_{i=1}^n y_i^2 = \sum_{i=1}^n (y_i-f(x_i))^2 + \sum_{i=1}^n (f(x_i))^2" class="tex"/></dd></dl>
<p>Then <b>TSS</b> and <b>SSR</b> need be redefined, and <b>RSS</b> is unchanged.
</p>
<dl><dd><img src="../images/Details_of_R_square/math-898716dfe4c0756a5032aff8d5101712.png?v=0" title="TSS = \sum_{i=1}^n y_i^2" alt="TSS = \sum_{i=1}^n y_i^2" class="tex"/></dd></dl>
<dl><dd><img src="../images/Details_of_R_square/math-120a95f050e837d97b73fecc20722464.png?v=0" title="SSR = \sum_{i=1}^n (f(x_i))^2" alt="SSR = \sum_{i=1}^n (f(x_i))^2" class="tex"/></dd></dl>
<p>And the coefficient of determination (<b>R-Square</b>) is redefined as follows:
</p>
<dl><dd><img src="../images/Details_of_R_square/math-5b936b2331157d838945fac4f2fa4bda.png?v=0" title="R^2=\frac{SSR}{TSS}=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^n (y_i-f(x_i))^2}{\sum_{i=1}^n y_i^2}" alt="R^2=\frac{SSR}{TSS}=1-\frac{RSS}{TSS}=1-\frac{\sum_{i=1}^n (y_i-f(x_i))^2}{\sum_{i=1}^n y_i^2}" class="tex"/></dd></dl>
<p>In this way, the <b>R-Square</b> value will always be non-negative. And <b>R-Square</b> measures the proportion of variation of the dependent variable around the value zero explained by the fitting when intercept is fixed.
</p>





