<h1 class="firstHeading">17.5.5.2 Algorithms (Two-Sample Kolmogorov-Smirnov Test)</h1><p class='urlname' style='display: none'>KS-Test-Algorithm</p>
<p>The procedure below draws on NAG algorithms. 
</p><p>Consider two independent samples X and Y, with the size of <img src="../images/Algorithm_(kstest2)/math-8d88a43872d6db6535d8672a15f09ce2.png?v=0" title="n_1\,\!" alt="n_1\,\!" class="tex"/> and <img src="../images/Algorithm_(kstest2)/math-ede03c8aef1b07c898c7747b489fd765.png?v=0" title="n_2\,\! " alt="n_2\,\! " class="tex"/>.Denoted as <img src="../images/Algorithm_(kstest2)/math-d79752c45b6e0288c460390908ff4157.png?v=0" title="x_1,x_2,\ldots ,x_{n_1}\,\!" alt="x_1,x_2,\ldots ,x_{n_1}\,\!" class="tex"/> and <img src="../images/Algorithm_(kstest2)/math-ba3bf26bc53cf72dd2f7e804e8351ba7.png?v=0" title="y_1,y_2,\ldots ,y_{n_1}\,\!" alt="y_1,y_2,\ldots ,y_{n_1}\,\!" class="tex"/> respectively. Let F(x) and G(x) represent their respective, unknown distribution functions. Also let <img src="../images/Algorithm_(kstest2)/math-2eaa8777a1f571cbad7bfca3adf83c34.png?v=0" title=" S_1(x)\,\! " alt=" S_1(x)\,\! " class="tex"/> and <img src="../images/Algorithm_(kstest2)/math-17118f898c20f6dbf66463ce22714faa.png?v=0" title=" S_2(x)\,\! " alt=" S_2(x)\,\! " class="tex"/> denote the values of sample empirical distribution functions.
</p><p>The null hypothesis&#160;:F(x)=G(x)
</p><p>The alternative hypothesis <img src="../images/Algorithm_(kstest2)/math-fabff59271b950125b7a360fba21de2c.png?v=0" title="H_1\,\!" alt="H_1\,\!" class="tex"/>:F(x)&lt;&gt;G(x)  the associated p-value is a two-tailed probability;
</p><p>or <img src="../images/Algorithm_(kstest2)/math-fabff59271b950125b7a360fba21de2c.png?v=0" title="H_1\,\!" alt="H_1\,\!" class="tex"/>:F(x)&gt;G(x)   the associated p-value is an upper-tailed probability, 
</p><p>or  <img src="../images/Algorithm_(kstest2)/math-fabff59271b950125b7a360fba21de2c.png?v=0" title="H_1\,\!" alt="H_1\,\!" class="tex"/>: F(x)&lt;G(x)   the associated p-value is a lower-tailed probability
</p><p>For the first case of <img src="../images/Algorithm_(kstest2)/math-fabff59271b950125b7a360fba21de2c.png?v=0" title="H_1\,\!" alt="H_1\,\!" class="tex"/>,  the statistics <img src="../images/Algorithm_(kstest2)/math-a9aca3b08301eb7213e2408c0ab85104.png?v=0" title="D_{n_1,n_2} \,\!" alt="D_{n_1,n_2} \,\!" class="tex"/> represents the largest absolute deviation of the two empirical distribution functions.
</p><p>For the second case of <img src="../images/Algorithm_(kstest2)/math-fabff59271b950125b7a360fba21de2c.png?v=0" title="H_1\,\!" alt="H_1\,\!" class="tex"/>, the statistics <img src="../images/Algorithm_(kstest2)/math-4cd60644c5463cc66c7631c167ed3936.png?v=0" title="D_{n_1,n_2}^{+} \,\!" alt="D_{n_1,n_2}^{+} \,\!" class="tex"/> represents the largest positive deviation between the empirical distribution function of the first sample and the empirical distribution function of the second sample, that is <img src="../images/Algorithm_(kstest2)/math-9cf98ed19b4ee4732ba5e4524e54a3f8.png?v=0" title="D_{n_1,n_2}^{+}=\max \{S_1(x)-S_2(x),0\}\,\!" alt="D_{n_1,n_2}^{+}=\max \{S_1(x)-S_2(x),0\}\,\!" class="tex"/> .
</p><p>For the third case of <img src="../images/Algorithm_(kstest2)/math-fabff59271b950125b7a360fba21de2c.png?v=0" title="H_1\,\!" alt="H_1\,\!" class="tex"/>, the statistics <img src="../images/Algorithm_(kstest2)/math-8af9c1a0d88a260554650e06fd515607.png?v=0" title="D_{n_1,n_2}^{-} \,\!" alt="D_{n_1,n_2}^{-} \,\!" class="tex"/> represents the largest positive deviation between the empirical distribution function of the second sample and the empirical distribution function of the first sample, that is  <img src="../images/Algorithm_(kstest2)/math-6a6a55b10a26fafb59197daab9f6d53b.png?v=0" title="D_{n_1,n_2}^{-}=\max \{S_2(x)-S_1(x),0\}\,\!" alt="D_{n_1,n_2}^{-}=\max \{S_2(x)-S_1(x),0\}\,\!" class="tex"/> . 
</p><p>KS-test2 also returns the standard statistics <img src="../images/Algorithm_(kstest2)/math-3e29b728262edf3639e74270b72e638a.png?v=0" title="Z=\sqrt{(n_1*n_2)/(n_1+n_2)}*D\,\!" alt="Z=\sqrt{(n_1*n_2)/(n_1+n_2)}*D\,\!" class="tex"/>,
</p><p>where <img src="../images/Algorithm_(kstest2)/math-a7c6c783c5d03fc91d0594b217f56580.png?v=0" title="D\,\!" alt="D\,\!" class="tex"/> maybe <img src="../images/Algorithm_(kstest2)/math-3f75caf92b4b313ec5ec353b2df6a0b0.png?v=0" title="D_{n_1,n_2}\,\!" alt="D_{n_1,n_2}\,\!" class="tex"/>,<img src="../images/Algorithm_(kstest2)/math-4cd60644c5463cc66c7631c167ed3936.png?v=0" title="D_{n_1,n_2}^{+} \,\!" alt="D_{n_1,n_2}^{+} \,\!" class="tex"/>, <img src="../images/Algorithm_(kstest2)/math-8af9c1a0d88a260554650e06fd515607.png?v=0" title="D_{n_1,n_2}^{-} \,\!" alt="D_{n_1,n_2}^{-} \,\!" class="tex"/>depending on the choice of the alternative hypothesis. 
</p><p>The distribution of the statistic <img src="../images/Algorithm_(kstest2)/math-07b1048bc60901fe82394ae47282671c.png?v=0" title="Z\,\!" alt="Z\,\!" class="tex"/> converges asymptotically to a distribution given by Smirnov as <img src="../images/Algorithm_(kstest2)/math-8d88a43872d6db6535d8672a15f09ce2.png?v=0" title="n_1\,\!" alt="n_1\,\!" class="tex"/> and <img src="../images/Algorithm_(kstest2)/math-2f6fac59fce80e76d698bcf5ea77bab6.png?v=0" title="n_2\,\!" alt="n_2\,\!" class="tex"/> increase. The probability, under the null hypothesis, of obtaining a value of the test statistic as extreme as that observed, is computed. 
</p><p>If <img src="../images/Algorithm_(kstest2)/math-caeb72e22c815c3c25d6a8b427c8aaad.png?v=0" title="max(n_1,n_2)\leq 2500\,\!" alt="max(n_1,n_2)\leq 2500\,\!" class="tex"/> and <img src="../images/Algorithm_(kstest2)/math-be2983946ebf66a1fd9de27574f27b22.png?v=0" title="n_1*n_2\leq 10000\,\!" alt="n_1*n_2\leq 10000\,\!" class="tex"/> then an exact method is given by Kim and Jinrich. Otherwise <img src="../images/Algorithm_(kstest2)/math-c0f582773fdbd168bbab09a1e6159c46.png?v=0" title="p\,\!" alt="p\,\!" class="tex"/>  is computed using the approximations suggested by Kim and Jenrich (1973)
</p><p>Note that the method used only exact for continuous theoretical distributions. 
</p><p>This method computes the two-sided probability. The one-sided probabilities are estimated by having the two-sided probability. This is a good estimate for small <img src="../images/Algorithm_(kstest2)/math-c0f582773fdbd168bbab09a1e6159c46.png?v=0" title="p\,\!" alt="p\,\!" class="tex"/>, that is <img src="../images/Algorithm_(kstest2)/math-cd60a65bcbb8b47dc37b5c879ed79777.png?v=0" title="p\leq 0.10\,\!" alt="p\leq 0.10\,\!" class="tex"/>, but it becomes very poor for larger <img src="../images/Algorithm_(kstest2)/math-c0f582773fdbd168bbab09a1e6159c46.png?v=0" title="p\,\!" alt="p\,\!" class="tex"/>.
</p><p>For more details of the algorithm, please refer to <a class="external text" href="http://www.originlab.com/pdfs/nagcl09/manual/pdf/g08/g08cdc.pdf" target="_blank"><b>nag_2_sample_ks_test (g08cdc)</b></a> .
</p>





