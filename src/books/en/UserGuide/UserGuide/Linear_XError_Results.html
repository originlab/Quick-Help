<h1 class="firstHeading">15.2.6 Algorithms (Fit Linear with X Error)</h1><p class='urlname' style='display: none'>Ref-Linear-XErr</p>
<div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#The_Fitting_Model"><span class="tocnumber">1</span> <span class="toctext">The Fitting Model</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Fit_Control"><span class="tocnumber">2</span> <span class="toctext">Fit Control</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Computation_Method"><span class="tocnumber">2.1</span> <span class="toctext">Computation Method</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Quantities_.28York_Method.29"><span class="tocnumber">3</span> <span class="toctext">Quantities (York Method)</span></a>
<ul>
<li class="toclevel-2 tocsection-5"><a href="#Fit_Parameters"><span class="tocnumber">3.1</span> <span class="toctext">Fit Parameters</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="#Fitted_Value_and_Standard_Errors"><span class="tocnumber">3.1.1</span> <span class="toctext">Fitted Value and Standard Errors</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#t-Value_and_Confidence_Level"><span class="tocnumber">3.1.2</span> <span class="toctext">t-Value and Confidence Level</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="#Prob.3E.7Ct.7C"><span class="tocnumber">3.1.3</span> <span class="toctext">Prob&gt;|t|</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="#LCL_and_UCL"><span class="tocnumber">3.1.4</span> <span class="toctext">LCL and UCL</span></a></li>
<li class="toclevel-3 tocsection-10"><a href="#CI_Half_Width"><span class="tocnumber">3.1.5</span> <span class="toctext">CI Half Width</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-11"><a href="#Fit_Statistics"><span class="tocnumber">3.2</span> <span class="toctext">Fit Statistics</span></a>
<ul>
<li class="toclevel-3 tocsection-12"><a href="#Degrees_of_Freedom"><span class="tocnumber">3.2.1</span> <span class="toctext">Degrees of Freedom</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#Residual_Sum_of_Squares"><span class="tocnumber">3.2.2</span> <span class="toctext">Residual Sum of Squares</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#Reduced_Chi-Sqr"><span class="tocnumber">3.2.3</span> <span class="toctext">Reduced Chi-Sqr</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Pearson.27s_r"><span class="tocnumber">3.2.4</span> <span class="toctext">Pearson's r</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="#Root-MSE_.28SD.29"><span class="tocnumber">3.2.5</span> <span class="toctext">Root-MSE (SD)</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#Covariance_and_Correlation_Matrix"><span class="tocnumber">3.2.6</span> <span class="toctext">Covariance and Correlation Matrix</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-18"><a href="#Quantities_.28FV_Method.29"><span class="tocnumber">4</span> <span class="toctext">Quantities (FV Method)</span></a></li>
<li class="toclevel-1 tocsection-19"><a href="#Quantities_.28Deming_Method.29"><span class="tocnumber">5</span> <span class="toctext">Quantities (Deming Method)</span></a>
<ul>
<li class="toclevel-2 tocsection-20"><a href="#Fit_Parameters_2"><span class="tocnumber">5.1</span> <span class="toctext">Fit Parameters</span></a>
<ul>
<li class="toclevel-3 tocsection-21"><a href="#Fitted_Value_and_Standard_Errors_2"><span class="tocnumber">5.1.1</span> <span class="toctext">Fitted Value and Standard Errors</span></a></li>
<li class="toclevel-3 tocsection-22"><a href="#t-Value_and_Confidence_Level_2"><span class="tocnumber">5.1.2</span> <span class="toctext">t-Value and Confidence Level</span></a></li>
<li class="toclevel-3 tocsection-23"><a href="#Prob.3E.7Ct.7C_2"><span class="tocnumber">5.1.3</span> <span class="toctext">Prob&gt;|t|</span></a></li>
<li class="toclevel-3 tocsection-24"><a href="#LCL_and_UCL_2"><span class="tocnumber">5.1.4</span> <span class="toctext">LCL and UCL</span></a></li>
<li class="toclevel-3 tocsection-25"><a href="#CI_Half_Width_2"><span class="tocnumber">5.1.5</span> <span class="toctext">CI Half Width</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-26"><a href="#Fit_Statistics_2"><span class="tocnumber">5.2</span> <span class="toctext">Fit Statistics</span></a>
<ul>
<li class="toclevel-3 tocsection-27"><a href="#Degrees_of_Freedom_2"><span class="tocnumber">5.2.1</span> <span class="toctext">Degrees of Freedom</span></a></li>
<li class="toclevel-3 tocsection-28"><a href="#Residual_Sum_of_Squares_2"><span class="tocnumber">5.2.2</span> <span class="toctext">Residual Sum of Squares</span></a></li>
<li class="toclevel-3 tocsection-29"><a href="#Reduced_Chi-Sqr_2"><span class="tocnumber">5.2.3</span> <span class="toctext">Reduced Chi-Sqr</span></a></li>
<li class="toclevel-3 tocsection-30"><a href="#Pearson.27s_r_2"><span class="tocnumber">5.2.4</span> <span class="toctext">Pearson's r</span></a></li>
<li class="toclevel-3 tocsection-31"><a href="#Root-MSE_.28SD.29_2"><span class="tocnumber">5.2.5</span> <span class="toctext">Root-MSE (SD)</span></a></li>
<li class="toclevel-3 tocsection-32"><a href="#Covariance_and_Correlation_Matrix_2"><span class="tocnumber">5.2.6</span> <span class="toctext">Covariance and Correlation Matrix</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-33"><a href="#Finding_X.2FY"><span class="tocnumber">6</span> <span class="toctext">Finding X/Y</span></a></li>
<li class="toclevel-1 tocsection-34"><a href="#Residual_Plots"><span class="tocnumber">7</span> <span class="toctext">Residual Plots</span></a>
<ul>
<li class="toclevel-2 tocsection-35"><a href="#Residual_vs._Independent"><span class="tocnumber">7.1</span> <span class="toctext">Residual vs. Independent</span></a></li>
<li class="toclevel-2 tocsection-36"><a href="#Residual_vs._Predicted_Value"><span class="tocnumber">7.2</span> <span class="toctext">Residual vs. Predicted Value</span></a></li>
<li class="toclevel-2 tocsection-37"><a href="#Residual_vs._Order_of_the_Data"><span class="tocnumber">7.3</span> <span class="toctext">Residual vs. Order of the Data</span></a></li>
<li class="toclevel-2 tocsection-38"><a href="#Histogram_of_the_Residual"><span class="tocnumber">7.4</span> <span class="toctext">Histogram of the Residual</span></a></li>
<li class="toclevel-2 tocsection-39"><a href="#Residual_Lag_Plot"><span class="tocnumber">7.5</span> <span class="toctext">Residual Lag Plot</span></a></li>
<li class="toclevel-2 tocsection-40"><a href="#Normal_Probability_Plot_of_Residuals"><span class="tocnumber">7.6</span> <span class="toctext">Normal Probability Plot of Residuals</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-41"><a href="#Reference"><span class="tocnumber">8</span> <span class="toctext">Reference</span></a></li>
</ul>
</div>

<h2><a name="The_Fitting_Model"></a><span class="mw-headline">The Fitting Model</span></h2>
<p>For given dataset <img src="../images/Linear_XError_Results/math-170b5dfcdac249cee2093c4610a34edc.png?v=0" title="(X_i,Y_i), (\sigma_{x_i},\sigma_{y_i}), i=1,2,\ldots n" alt="(X_i,Y_i), (\sigma_{x_i},\sigma_{y_i}), i=1,2,\ldots n" class="tex"/>, where X is the independent variable and Y is the dependent variable, and <img src="../images/Linear_XError_Results/math-afea82b04c63850353a3fb8edecf44c4.png?v=0" title="(\sigma_{x_i},\sigma_{y_i})" alt="(\sigma_{x_i},\sigma_{y_i})" class="tex"/> are Errors for X, Y, respectively.  -- Fit Linear with X Error fits the data to a model of the following form: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-6f0706d5e7020e7b145421a393c3ca56.png?v=0" title="y=\beta _0+\beta _1x+\varepsilon" alt="y=\beta _0+\beta _1x+\varepsilon" class="tex"/>
</th>
<td>
<p>(1)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-8da8ff23a00993b1a55ee023f9ba0db6.png?v=0" title="\left\{\begin{matrix}&#10;x_i=X_i+\sigma_{x_i}\\ &#10;y_i=Y_i+\sigma_{y_i}&#10;\end{matrix}\right." alt="\left\{\begin{matrix}&#10;x_i=X_i+\sigma_{x_i}\\ &#10;y_i=Y_i+\sigma_{y_i}&#10;\end{matrix}\right." class="tex"/>
</th>
<td>
<p>(2)   
</p>
</td></tr></table>
<h2><a name="Fit_Control"></a><span class="mw-headline">Fit Control</span></h2>
<h3><a name="Computation_Method"></a><span class="mw-headline">Computation Method</span></h3>
<ul>
<li><b>York Method</b><br />
<dl><dd>York Method is the computation method of D. York, described in <i>Unified equations for the slope, intercept, and standard error of the best straight line</i></dd></dl>
<li><b>FV Method</b><br />
<dl><dd>FV Method is the computation method of Giovanni Fasano &amp; Roberto Vio, described in <i>Fittng a Straight Line with Errors on Both Coordinates</i>.</dd></dl>
<li><b>Deming Method</b><br />
<dl><dd>Deming regression is the maximum likelihood estimation of an errors-in-variables model, the X/Y errors are assumed to be independent identically distributed.</dd></dl>
<li><b>Correlation Between X and Y Errors</b>
<dl><dd>Correlation Between X and Y Errors <img src="../images/Linear_XError_Results/math-af2593b49488f8bc44396795792c2d79.png?v=0" title="r_i" alt="r_i" class="tex"/> (For York method only)</dd></dl>
<li><b>Standard Deviation of X/Y</b>
<dl><dd>Standard Deviation of X/Y (For Deming method only)</dd></dl>
</ul>
<h2><a name="Quantities_.28York_Method.29"></a><span class="mw-headline">Quantities (York Method)</span></h2>
<p>When you perform a linear fit, you generate an <a class="external text" href="../../UserGuide/UserGuide/Analysis_Report_Sheets_and_Columns.html">analysis report sheet</a> listing computed quantities.  The Parameters table reports model slope and intercept (numbers in parentheses show how the quantities are derived): 
</p>
<h3><a name="Fit_Parameters"></a><span class="mw-headline">Fit Parameters</span></h3>
<p><a  class="image"><img alt="York Error.png" src="../images/Linear_XError_Results/York_Error.png?v=66414" width="607"  /></a>
</p>
<h4><a name="Fitted_Value_and_Standard_Errors"></a><span class="mw-headline">Fitted Value and Standard Errors</span></h4>
<p>Define <img src="../images/Linear_XError_Results/math-540bdf656419f2a308d7ad1571c664a0.png?v=0" title="W_i" alt="W_i" class="tex"/> which involves the weight (error) for both x and y;
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-4641c94cc1de495f820ae7e04f6f773b.png?v=0" title="W_i = \frac{\omega_{x_i}\omega_{y_i}}{\omega_{x_i}+\beta_1^2\omega_{y_i}-2\beta_1 r_ia_i} =\frac{1}{\sigma_{y_i}^2+\beta_1^2\sigma_{x_i}^2 - 2\beta_1 r_i \sigma_{x_i} \sigma_{y_i}}" alt="W_i = \frac{\omega_{x_i}\omega_{y_i}}{\omega_{x_i}+\beta_1^2\omega_{y_i}-2\beta_1 r_ia_i} =\frac{1}{\sigma_{y_i}^2+\beta_1^2\sigma_{x_i}^2 - 2\beta_1 r_i \sigma_{x_i} \sigma_{y_i}}" class="tex"/>
</th>
<td>
<p>(3)  
</p>
</td></tr></table>
<p>Therein, <img src="../images/Linear_XError_Results/math-04e382d5a60b9af5117b1c5ae29d51fb.png?v=0" title="\omega_{x_i}=\frac{1}{\sigma_{x_i}^2}, \ \omega_{y_i}=\frac{1}{\sigma_{y_i}^2}" alt="\omega_{x_i}=\frac{1}{\sigma_{x_i}^2}, \ \omega_{y_i}=\frac{1}{\sigma_{y_i}^2}" class="tex"/> are weights of <img src="../images/Linear_XError_Results/math-6e6fd7f03c691b16aaea1383f4c04a4a.png?v=0" title="(X_i, Y_i)" alt="(X_i, Y_i)" class="tex"/>, <img src="../images/Linear_XError_Results/math-af2593b49488f8bc44396795792c2d79.png?v=0" title="r_i" alt="r_i" class="tex"/> is <b>Correlation between X and Y Errors</b> (i.e. <img src="../images/Linear_XError_Results/math-e01da6570365bb9bcdd31bf5d02c17e4.png?v=0" title="\sigma_{x_i}" alt="\sigma_{x_i}" class="tex"/> and <img src="../images/Linear_XError_Results/math-51def96939771aad32a8144ed6298ea7.png?v=0" title="\sigma_{y_i}" alt="\sigma_{y_i}" class="tex"/>), and <img src="../images/Linear_XError_Results/math-77943a8eacd84180926cb531c129e23d.png?v=0" title="\alpha_i=\sqrt{\omega_{x_i} \omega_{y_i}}" alt="\alpha_i=\sqrt{\omega_{x_i} \omega_{y_i}}" class="tex"/>.
</p><p>The slope of the fitted line for <img src="../images/Linear_XError_Results/math-6e6fd7f03c691b16aaea1383f4c04a4a.png?v=0" title="(X_i, Y_i)" alt="(X_i, Y_i)" class="tex"/> with no weighting (errors) is the initial value for <img src="../images/Linear_XError_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png?v=0" title="\beta_1" alt="\beta_1" class="tex"/>. They should be solved iteratively, until successive estimates of <img src="../images/Linear_XError_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png?v=0" title="\beta_1" alt="\beta_1" class="tex"/> agree within desired tolerance.
</p><p>The concise equations which estimate parameters <img src="../images/Linear_XError_Results/math-9dabc344cb060be9355c54cc39a038db.png?v=0" title="\hat{\beta_0}" alt="\hat{\beta_0}" class="tex"/> and <img src="../images/Linear_XError_Results/math-4db6565d3866d2baeaf7a66f389744f9.png?v=0" title="\hat{\beta_1}" alt="\hat{\beta_1}" class="tex"/> for the best-fit line with X_Y errors are:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-68bd787a470493a132e4266994ea8e05.png?v=0" title="\hat{\beta_0}=\bar{Y}-\hat{\beta_1}\bar{X}" alt="\hat{\beta_0}=\bar{Y}-\hat{\beta_1}\bar{X}" class="tex"/>
</th>
<td>
<p>(4)  
</p>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-8d5bb50bef2c77df3ef066239cf869f9.png?v=0" title="\hat{\beta_1}=\frac{\sum{W_i b_i V_i}}{\sum{W_i b_i U_i}}" alt="\hat{\beta_1}=\frac{\sum{W_i b_i V_i}}{\sum{W_i b_i U_i}}" class="tex"/>
</th>
<td>
<p>(5)  
</p>
</td></tr></table>
<p>where <img src="../images/Linear_XError_Results/math-5dee071df132e443c3cee0626dffcb37.png?v=0" title="\bar{X} = \frac{ \sum{W_i X_i} }{ \sum{W_i} }, \ \bar{Y} = \frac{ \sum{W_i Y_i} }{ \sum{Y_i} }" alt="\bar{X} = \frac{ \sum{W_i X_i} }{ \sum{W_i} }, \ \bar{Y} = \frac{ \sum{W_i Y_i} }{ \sum{Y_i} }" class="tex"/>.
</p><p>U and V are the deviation for X and Y:
</p><p><img src="../images/Linear_XError_Results/math-8a09d2f12018fd535029e8c26fc1b583.png?v=0" title="\left\{\begin{matrix}&#10;U=X-\bar{X}\\&#10;V=Y-\bar{Y}&#10;\end{matrix}\right.&#10;" alt="\left\{\begin{matrix}&#10;U=X-\bar{X}\\&#10;V=Y-\bar{Y}&#10;\end{matrix}\right.&#10;" class="tex"/>
</p><p>and 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-739e4473271361ddc22a8a83eebe1710.png?v=0" title="b_i=W_i \left[\frac{U_i}{\omega_{y_i}}+\frac{\hat{\beta_1}}{\omega_{x_i}}{V_i}-(\beta U_i+V_i)\frac{r_i}{\alpha_i} \right]" alt="b_i=W_i \left[\frac{U_i}{\omega_{y_i}}+\frac{\hat{\beta_1}}{\omega_{x_i}}{V_i}-(\beta U_i+V_i)\frac{r_i}{\alpha_i} \right]" class="tex"/>
</th>
<td>
<p>(6)  
</p>
</td></tr></table>
<p>The corresponding variation <img src="../images/Linear_XError_Results/math-10e16c6a764d367ca5077a54bf156f7e.png?v=0" title="\sigma^2" alt="\sigma^2" class="tex"/> and standard error <img src="../images/Linear_XError_Results/math-03c7c0ace395d80182db07ae2c30f034.png?v=0" title="s" alt="s" class="tex"/> for parameter is:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-40ef0222004a7df6913a5b312f797702.png?v=0" title="\sigma_{\hat{\beta_0}}^2=\frac{1}{\sum{W_i}}+\bar{x}^2\sigma_{\hat{\beta_1}}^2" alt="\sigma_{\hat{\beta_0}}^2=\frac{1}{\sum{W_i}}+\bar{x}^2\sigma_{\hat{\beta_1}}^2" class="tex"/>
</th>
<td>
<p>(7)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-2ea99c183266ba8295bceec8e5a6d093.png?v=0" title="\sigma_{\hat{\beta_1}}^2=\frac{1}{\sum{W_i u_i^2}}" alt="\sigma_{\hat{\beta_1}}^2=\frac{1}{\sum{W_i u_i^2}}" class="tex"/>
</th>
<td>
<p>(8)  
</p>
</td></tr></table>
<p>where <img src="../images/Linear_XError_Results/math-ad19c90fa0b0d544d1e448948119e9cf.png?v=0" title="\bar{x} = \frac{ \sum{W_i x_i} }{ \sum{W_i} }" alt="\bar{x} = \frac{ \sum{W_i x_i} }{ \sum{W_i} }" class="tex"/>, <img src="../images/Linear_XError_Results/math-1ba8aaab47179b3d3e24b0ccea9f4e30.png?v=0" title="x_i" alt="x_i" class="tex"/> is the expectation value of <img src="../images/Linear_XError_Results/math-a97118fb9e8d7e006a466bfc0771f888.png?v=0" title="X_i" alt="X_i" class="tex"/>, and <img src="../images/Linear_XError_Results/math-07a832aec0fd752f69267520da32aea3.png?v=0" title="u_i=x_i - \bar{x}" alt="u_i=x_i - \bar{x}" class="tex"/>.
</p><p>The standard error for parameters is final given by:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-8799bdc2ab7c55a491ea8eb1db6fb88c.png?v=0" title="\varepsilon_{\hat{\beta_0}}=\sqrt{\sigma _{\hat{\beta_0}}^2}\sqrt{\frac{S}{n-2}}" alt="\varepsilon_{\hat{\beta_0}}=\sqrt{\sigma _{\hat{\beta_0}}^2}\sqrt{\frac{S}{n-2}}" class="tex"/>
</th>
<td>
<p>(9)  
</p>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-aa7c7a7784c8c3747374fe60e336fe06.png?v=0" title="\varepsilon_{\hat{\beta_1}}=\sqrt{\sigma _{\hat{\beta_1}}^2}\sqrt{\frac{S}{n-2}}" alt="\varepsilon_{\hat{\beta_1}}=\sqrt{\sigma _{\hat{\beta_1}}^2}\sqrt{\frac{S}{n-2}}" class="tex"/>
</th>
<td>
<p>(10)  
</p>
</td></tr></table>
<p>where <img src="../images/Linear_XError_Results/math-5dbc98dcc983a70728bd082d1a47546e.png?v=0" title="S" alt="S" class="tex"/> is:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-a4414d3acaf054a46d95a8c4b3ad9489.png?v=0" title="S=\sum W_i(Y_i - \beta_1 X_i- \beta_0)^2" alt="S=\sum W_i(Y_i - \beta_1 X_i- \beta_0)^2" class="tex"/>
</th>
<td>
<p>(11)  
</p>
</td></tr></table>
<h4><a name="t-Value_and_Confidence_Level"></a><span class="mw-headline">t-Value and Confidence Level</span></h4>
<p>If the regression assumptions hold, we have: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-0ed681c4ca83831abc9e0afe2f959db2.png?v=0" title="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" class="tex"/>  and  <img src="../images/Linear_XError_Results/math-b502ef25a978991781d791a9f6275a83.png?v=0" title="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" class="tex"/>
</th>
<td>
<p>(12)  
</p>
</td></tr></table>
<p>The <i>t</i>-test can be used to examine whether the fitting parameters are significantly different from zero, which means that we can test whether  <img src="../images/Linear_XError_Results/math-ae198091d3f29f3101f5b0b86bf85b5d.png?v=0" title="\beta _0= 0\,\!" alt="\beta _0= 0\,\!" class="tex"/> (if true, this means that the fitted line passes through the origin) or <img src="../images/Linear_XError_Results/math-c1cb3b8aa03a36c23962a83cf7d2dc40.png?v=0" title="\beta _1= 0\,\!" alt="\beta _1= 0\,\!" class="tex"/>. The hypotheses of the <i>t</i>-tests are: 
</p>
<dl><dd><img src="../images/Linear_XError_Results/math-d15540fa2e02fa998eea73fd85bf1952.png?v=0" title="H_0&#160;: \beta _0= 0\,\! " alt="H_0&#160;: \beta _0= 0\,\! " class="tex"/>                  <img src="../images/Linear_XError_Results/math-57dce98807a9a866ee09b5bbe88bb68e.png?v=0" title="H_0&#160;: \beta _1= 0\,\!" alt="H_0&#160;: \beta _1= 0\,\!" class="tex"/> </dd>
<dd><img src="../images/Linear_XError_Results/math-00f883e098653776a99683aae70c3783.png?v=0" title="H_\alpha &#160;: \beta _0  \neq 0\,\!" alt="H_\alpha &#160;: \beta _0  \neq 0\,\!" class="tex"/>         <img src="../images/Linear_XError_Results/math-27aac7aa6524d0b486a1890feffff3e1.png?v=0" title="H_\alpha &#160;: \beta _1 \neq  0\,\!" alt="H_\alpha &#160;: \beta _1 \neq  0\,\!" class="tex"/>     </dd></dl>
<p>The <i>t</i>-values can be computed by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-6368690db66f6b16f642c2976ba74d59.png?v=0" title="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" alt="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" class="tex"/>  and <img src="../images/Linear_XError_Results/math-abcdef6859e44b55ed86ce261400d36c.png?v=0" title="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" alt="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" class="tex"/>
</th>
<td>
<p>(13)  
</p>
</td></tr></table>
<p>With the computed <i>t</i>-value, we can decide whether or not to reject the corresponding null hypothesis. Usually, for a given confidence level <img src="../images/Linear_XError_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>   , we can reject <img src="../images/Linear_XError_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> when <img src="../images/Linear_XError_Results/math-9d38e8e7363d66af41530c3fbdf494f5.png?v=0" title="|t|&gt;t_{\frac \alpha 2}" alt="|t|&gt;t_{\frac \alpha 2}" class="tex"/>. Additionally, the <i>p</i>-value, or significance level, is reported with a <i>t</i>-test. We also reject the null hypothesis <img src="../images/Linear_XError_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> if the <i>p</i>-value is less than <img src="../images/Linear_XError_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>.
</p>
<h4><a name="Prob.3E.7Ct.7C"></a><span class="mw-headline">Prob&gt;|t|</span></h4>
<p>The probability that <img src="../images/Linear_XError_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> in the <i>t</i> test above is true. 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-3e5335a131705b3ef0bb8b250033167d.png?v=0" title="prob=2(1-tcdf(|t|,df_{Error}))\,\!" alt="prob=2(1-tcdf(|t|,df_{Error}))\,\!" class="tex"/>
</th>
<td>
<p>(14)  
</p>
</td></tr></table>
<p>where <i>tcdf(t, df)</i> computes the lower tail probability for the Student's <i>t</i> distribution with <i>df</i> degree of freedom.
</p>
<h4><a name="LCL_and_UCL"></a><span class="mw-headline">LCL and UCL</span></h4>
<p>From the <i>t</i>-value, we can calculate the <img src="../images/Linear_XError_Results/math-0cdaba532a6f8ad706e6b8e5e97f0d36.png?v=0" title="(1-\alpha )\times 100\%" alt="(1-\alpha )\times 100\%" class="tex"/>  <b>Confidence Interval</b> for each parameter by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-a27fc4294ee80efb51793fbcd4e23df2.png?v=0" title="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" alt="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" class="tex"/>
</th>
<td>
<p>(15)  
</p>
</td></tr></table>
<p>where <img src="../images/Linear_XError_Results/math-fd9dfeda668ac9b0c5ef311ffdca8f71.png?v=0" title="UCL" alt="UCL" class="tex"/> and <img src="../images/Linear_XError_Results/math-a4df64dc5e32716f6de0ec15b0340950.png?v=0" title="LCL" alt="LCL" class="tex"/> is short for the <b>Upper Confidence Interval</b> and <b>Lower Confidence Interval</b>, respectively.
</p>
<h4><a name="CI_Half_Width"></a><span class="mw-headline">CI Half Width</span></h4>
<p>The Confidence Interval Half Width is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-77ec7c61ac9d06bfc9ad4c0ae47b85ce.png?v=0" title="CI=\frac{UCL-LCL}2" alt="CI=\frac{UCL-LCL}2" class="tex"/>
</th>
<td>
<p>(16)  
</p>
</td></tr></table>
<p>where UCL and LCL is the <b>Upper Confidence Interval</b> and <b>Lower Confidence Interval</b>, respectively.
</p><p>For more information <a class="external text" href="../../UserGuide/UserGuide/Linear_XError_Results.html#Reference">,see Reference 1 (below)</a>.
</p>
<h3><a name="Fit_Statistics"></a><span class="mw-headline">Fit Statistics</span></h3>
<p><a  class="image"><img alt="York Stats.png" src="../images/Linear_XError_Results/York_Stats.png?v=66434" width="252"  /></a>
</p>
<h4><a name="Degrees_of_Freedom"></a><span class="mw-headline">Degrees of Freedom</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-8dbf22cb95c3b3dc33d43f0dcb4582b7.png?v=0" title="df=n-2" alt="df=n-2" class="tex"/>
</th>
<td>
<p>(17) 
</p>
</td></tr></table>
<p>n is total number of points
</p>
<h4><a name="Residual_Sum_of_Squares"></a><span class="mw-headline">Residual Sum of Squares</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-c1140af1d30d301bb79328c09bdf9741.png?v=0" title="RSS=\sum^n_{i=1} \frac{(\beta_0+\beta_1 x_i - y_i)^2}{\sigma^2_{y_i}+\beta_1^2\sigma^2_{x_i}}" alt="RSS=\sum^n_{i=1} \frac{(\beta_0+\beta_1 x_i - y_i)^2}{\sigma^2_{y_i}+\beta_1^2\sigma^2_{x_i}}" class="tex"/>
</th>
<td>
<p>(18) 
</p>
</td></tr></table>
<h4><a name="Reduced_Chi-Sqr"></a><span class="mw-headline">Reduced Chi-Sqr</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-c0df1f687a05ad41611867fa7424fbeb.png?v=0" title="\sigma^2=\frac{RSS}{n-2}" alt="\sigma^2=\frac{RSS}{n-2}" class="tex"/>
</th>
<td>
<p>(19) 
</p>
</td></tr></table>
<h4><a name="Pearson.27s_r"></a><span class="mw-headline">Pearson's r</span></h4>
<p>In simple linear regression, the correlation coefficient between x and y, denoted by <i>r</i>, equals to: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-f7f1dc7cce20e9b9b19e47b012ee173d.png?v=0" title="r=R\,\!" alt="r=R\,\!" class="tex"/>   <i>if <img src="../images/Linear_XError_Results/math-3a1e29703e364979554e6bd76761bed6.png?v=0" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"/> is positive</i>
</th>
<td>
<p>(20)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-556cec8e44138ef9cf002c70434cb55d.png?v=0" title="r=-R\,\!" alt="r=-R\,\!" class="tex"/>   <i>if <img src="../images/Linear_XError_Results/math-3a1e29703e364979554e6bd76761bed6.png?v=0" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"/> is negative</i>
</th></tr></table>
<p><b><img src="../images/Linear_XError_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/></b> can be computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-9ce15fecaa71e5021c67564b9f212551.png?v=0" title="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" alt="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" class="tex"/>
</th>
<td>
<p>(21)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-143fa9a82ab7c4beeb5f48ff2f737e5f.png?v=0" title="TSS=\sum_{i=1}^n(y_i-\bar{y})^2" alt="TSS=\sum_{i=1}^n(y_i-\bar{y})^2" class="tex"/>
</th></tr></table>
<h4><a name="Root-MSE_.28SD.29"></a><span class="mw-headline">Root-MSE (SD)</span></h4>
<p>Root mean square of the error, or residual standard deviation,  which equals to: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-5d65f33336e7ea1ead5adec051bf5572.png?v=0" title="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" alt="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" class="tex"/>
</th>
<td>
<p>(22) 
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-c20f8bb6c5735ccda8a5e88cff767c30.png?v=0" title="df_{Error}=n-2" alt="df_{Error}=n-2" class="tex"/>
</th></tr></table>
<h4><a name="Covariance_and_Correlation_Matrix"></a><span class="mw-headline">Covariance and Correlation Matrix</span></h4>
<p>The Covariance matrix of linear regression is calculated by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-b0bcaee4a98e9106fc977c7ab6cf5b79.png?v=0" title="&#10;\begin{pmatrix}&#10;Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\&#10;Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1)&#10;\end{pmatrix}=\sigma ^2\frac 1{SXX}\begin{pmatrix} \sum \frac{x_i^2}n &amp; -\bar x \\-\bar x &amp; 1 \end{pmatrix}" alt="&#10;\begin{pmatrix}&#10;Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\&#10;Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1)&#10;\end{pmatrix}=\sigma ^2\frac 1{SXX}\begin{pmatrix} \sum \frac{x_i^2}n &amp; -\bar x \\-\bar x &amp; 1 \end{pmatrix}" class="tex"/>
</th>
<td>
<p>(23)  
</p>
</td></tr></table>
<p>The correlation between any two parameters is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-b029662d99694045f6e48fab550e2b23.png?v=0" title="&#10;\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " alt="&#10;\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " class="tex"/>
</th>
<td>
<p>(24)  
</p>
</td></tr></table>
<h2><a name="Quantities_.28FV_Method.29"></a><span class="mw-headline">Quantities (FV Method)</span></h2>
<p>FV Method is the computation method of Giovanni Fasano &amp; Roberto Vio, described in <i>Fittng a Straight Line with Errors on Both Coordinates</i>.
</p><p>The weighting is defined as:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-9cf5235a06cda7abe1a09a6f38559885.png?v=0" title="W_i=\frac{1}{\beta_1^2\sigma_{x_{i}}^2+\sigma_{y_{i}}^2}" alt="W_i=\frac{1}{\beta_1^2\sigma_{x_{i}}^2+\sigma_{y_{i}}^2}" class="tex"/>
</th>
<td>
<p>(25) 
</p>
</td></tr></table>
<p>The slope of the fitted line for <img src="../images/Linear_XError_Results/math-6e6fd7f03c691b16aaea1383f4c04a4a.png?v=0" title="(X_i, Y_i)" alt="(X_i, Y_i)" class="tex"/> with no weighting (errors) is <img src="../images/Linear_XError_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png?v=0" title="\beta_1" alt="\beta_1" class="tex"/>.
</p><p>Let
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-b51de36fa852cbc25f343e160be11748.png?v=0" title="\bar{x}=\frac{\sum{W_i x_i}}{\sum W_i}" alt="\bar{x}=\frac{\sum{W_i x_i}}{\sum W_i}" class="tex"/>
</th>
<td>
<p>(26) 
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-3c0019f33af7b65eabc78ad4fcee6048.png?v=0" title="\bar{y}=\frac{\sum{W_i y_i}}{\sum W_i}" alt="\bar{y}=\frac{\sum{W_i y_i}}{\sum W_i}" class="tex"/>
</th>
<td>
<p>(27) 
</p>
</td></tr></table>
<p>by minimizing the sum <img src="../images/Linear_XError_Results/math-7609b577fc585f58d9bc5f5c8ceb96d0.png?v=0" title="K^2=\sum{W_i (y_i-\beta_0-\beta_1 x_i)^2}" alt="K^2=\sum{W_i (y_i-\beta_0-\beta_1 x_i)^2}" class="tex"/>, we can get the estimate value <img src="../images/Linear_XError_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png?v=0" title="\beta_0" alt="\beta_0" class="tex"/> and <img src="../images/Linear_XError_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png?v=0" title="\beta_1" alt="\beta_1" class="tex"/> by setting the partial derivatives to 0.
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-bc7d13600bde08b898a24ed825398bc5.png?v=0" title="\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}" alt="\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}" class="tex"/>
</th>
<td>
<p>(28) 
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-5c5608c30636231eeb691a4efcb34493.png?v=0" title="a\hat{\beta_1}^2+b\hat{\beta_1}-c=0" alt="a\hat{\beta_1}^2+b\hat{\beta_1}-c=0" class="tex"/>
</th>
<td>
<p>(29) 
</p>
</td></tr></table>
<p>where
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-b96e947ec0b5a7c613457b9ea2d7ac02.png?v=0" title="a=\sum{W_i^2\sigma_{x_i}^2(y_i-\bar{y_i})(x_i-\bar{x_i})}" alt="a=\sum{W_i^2\sigma_{x_i}^2(y_i-\bar{y_i})(x_i-\bar{x_i})}" class="tex"/>
</th>
<td>
<p>(30) 
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-dcc2ce5179902638125db68c2d6ea365.png?v=0" title="b=\sum{W_i^2[\sigma_{y_i}^2(x_i-\bar{x_i})^2-\sigma_{x_i}^2(y_i-\bar{y_i})^2]}" alt="b=\sum{W_i^2[\sigma_{y_i}^2(x_i-\bar{x_i})^2-\sigma_{x_i}^2(y_i-\bar{y_i})^2]}" class="tex"/>
</th>
<td>
<p>(31) 
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-08271c784412d72127509ff01d0f968a.png?v=0" title="c=\sum{W_i^2\sigma_{y_i}^2(y_i-\bar{y_i})(x_i-\bar{x_i})}" alt="c=\sum{W_i^2\sigma_{y_i}^2(y_i-\bar{y_i})(x_i-\bar{x_i})}" class="tex"/>
</th>
<td>
<p>(32) 
</p>
</td></tr></table>
<p><img src="../images/Linear_XError_Results/math-4db6565d3866d2baeaf7a66f389744f9.png?v=0" title="\hat{\beta_1}" alt="\hat{\beta_1}" class="tex"/> should be solved iteratively, until successive estimates of <img src="../images/Linear_XError_Results/math-4db6565d3866d2baeaf7a66f389744f9.png?v=0" title="\hat{\beta_1}" alt="\hat{\beta_1}" class="tex"/> agree within desired tolerance.
</p><p>For each parameter standard error, please refer to <a href="../../UserGuide/UserGuide/Linear_Regression_Results.html#Simple_Linear_Regression_Model" title="UserGuide:Linear Regression Results">Linear Regression Model</a>
</p><p>For more information <a class="external text" href="../../UserGuide/UserGuide/Linear_XError_Results.html#Reference">,see Reference 2 (below)</a>.
</p>
<h2><a name="Quantities_.28Deming_Method.29"></a><span class="mw-headline">Quantities (Deming Method)</span></h2>
<p>When you perform a linear fit, you generate an <a class="external text" href="../../UserGuide/UserGuide/Analysis_Report_Sheets_and_Columns.html">analysis report sheet</a> listing computed quantities.  The Parameters table reports model slope and intercept (numbers in parentheses show how the quantities are derived): 
</p>
<h3><a name="Fit_Parameters_2"></a><span class="mw-headline">Fit Parameters</span></h3>
<p><a  class="image"><img alt="Deming Error.png" src="../images/Linear_XError_Results/Deming_Error.png?v=66568" width="607"  /></a>
</p><p>Deming regression is used for situation where both x and y are subjected to measurement error.
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-6f0706d5e7020e7b145421a393c3ca56.png?v=0" title="y=\beta _0+\beta _1x+\varepsilon" alt="y=\beta _0+\beta _1x+\varepsilon" class="tex"/>
</th>
<td>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-8da8ff23a00993b1a55ee023f9ba0db6.png?v=0" title="\left\{\begin{matrix}&#10;x_i=X_i+\sigma_{x_i}\\ &#10;y_i=Y_i+\sigma_{y_i}&#10;\end{matrix}\right." alt="\left\{\begin{matrix}&#10;x_i=X_i+\sigma_{x_i}\\ &#10;y_i=Y_i+\sigma_{y_i}&#10;\end{matrix}\right." class="tex"/>
</th>
<td>
</td></tr></table>
<p>Assume <img src="../images/Linear_XError_Results/math-e01da6570365bb9bcdd31bf5d02c17e4.png?v=0" title="\sigma_{x_i}" alt="\sigma_{x_i}" class="tex"/> are independent identically distributed with <img src="../images/Linear_XError_Results/math-4e0901de034a98e2154ac97b77c7b03d.png?v=0" title="\sigma_{x_i} \sim \mathcal{N}(0,\sigma^2)" alt="\sigma_{x_i} \sim \mathcal{N}(0,\sigma^2)" class="tex"/>, and that <img src="../images/Linear_XError_Results/math-51def96939771aad32a8144ed6298ea7.png?v=0" title="\sigma_{y_i}" alt="\sigma_{y_i}" class="tex"/> are independent identically distributed with <img src="../images/Linear_XError_Results/math-34b628842156af60063c3ee3bd6c2ddf.png?v=0" title="\sigma_{y_i} \sim \mathcal{N}(0,\lambda \sigma^2)" alt="\sigma_{y_i} \sim \mathcal{N}(0,\lambda \sigma^2)" class="tex"/>, where <img src="../images/Linear_XError_Results/math-4efc64d95120928b599f8c26260906bb.png?v=0" title="\mathcal{N}(0,\sigma^2)" alt="\mathcal{N}(0,\sigma^2)" class="tex"/> denotes the normal distribution with mean 0 and standard deviation <img src="../images/Linear_XError_Results/math-a2ab7d71a0f07f388ff823293c147d21.png?v=0" title="\sigma" alt="\sigma" class="tex"/>. If <img src="../images/Linear_XError_Results/math-0f95d2f2265095c8032bb38dd0790e74.png?v=0" title="\lambda=1" alt="\lambda=1" class="tex"/>, it’s orthogonal regression.
The weighted sum of squared residuals of the model is minimized:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-fe8384be6fd283bb5de696a280b51535.png?v=0" title="RSS=\sum^n_{i=1}\left ((x_i-X_i)^2+\frac{(y_i-\beta_0-\beta_1X_i)^2}{\lambda}\right)" alt="RSS=\sum^n_{i=1}\left ((x_i-X_i)^2+\frac{(y_i-\beta_0-\beta_1X_i)^2}{\lambda}\right)" class="tex"/>
</th>
<td>
<p>(33)
</p>
</td></tr></table>
<h4><a name="Fitted_Value_and_Standard_Errors_2"></a><span class="mw-headline">Fitted Value and Standard Errors</span></h4>
<p>We can solve the parameters:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-c53c84a2f0e13365be6e8065a25e7bf7.png?v=0" title="\hat{\beta_1}=\frac{SYY-\lambda SXX+\sqrt{(SYY-\lambda SXX)^2+4\lambda SXY^2}}{2SXY}" alt="\hat{\beta_1}=\frac{SYY-\lambda SXX+\sqrt{(SYY-\lambda SXX)^2+4\lambda SXY^2}}{2SXY}" class="tex"/>
</th>
<td>
<p>(34)  
</p>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-bc7d13600bde08b898a24ed825398bc5.png?v=0" title="\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}" alt="\hat{\beta_0}=\bar{y}-\hat{\beta_1}\bar{x}" class="tex"/>
</th>
<td>
<p>(35)  
</p>
</td></tr></table>
<p>where:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-d81bb6610b25c48703ab9159111acedd.png?v=0" title="\bar{x}=\frac{1}{n}\sum_{i=1}^2{x_i}, \bar{y}=\frac{1}{n}\sum_{i=1}^n{y_i}" alt="\bar{x}=\frac{1}{n}\sum_{i=1}^2{x_i}, \bar{y}=\frac{1}{n}\sum_{i=1}^n{y_i}" class="tex"/>
</th>
<td>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-4910699030b9a518c1d2b906e5ad20fe.png?v=0" title="u_i=x_i-\bar{x}" alt="u_i=x_i-\bar{x}" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-0724e00259fc5379b680c7cd71f5236a.png?v=0" title="v_i=y_i-\bar{y}" alt="v_i=y_i-\bar{y}" class="tex"/>
</th>
<td>
</td></tr></table>
<p>and:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-e498d793b11998cf2c483b0e48283e2c.png?v=0" title="SXX=\sum_{i=1}^n u_i^2" alt="SXX=\sum_{i=1}^n u_i^2" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-eb43ff9d88ffadb8d766bcd9733f182b.png?v=0" title="SYY=\sum_{i=1}^n v_i^2" alt="SYY=\sum_{i=1}^n v_i^2" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-5689ac81685be69663ca6d33159f0c93.png?v=0" title="SXY=\sum_{i=1}^n u_iv_i" alt="SXY=\sum_{i=1}^n u_iv_i" class="tex"/>
</th>
<td>
</td></tr></table>
<p>The corresponding variation for parameters is:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-8a1845ed44ea8ea860f03425357bfd46.png?v=0" title="\sigma^2_{\hat \beta _0}=\frac{1}{nw}+2(\bar{x}+2\bar{z})\bar{z}Q+(\bar{x}+2\bar{z})^2 \sigma_{\bar{\beta_1}}^2" alt="\sigma^2_{\hat \beta _0}=\frac{1}{nw}+2(\bar{x}+2\bar{z})\bar{z}Q+(\bar{x}+2\bar{z})^2 \sigma_{\bar{\beta_1}}^2" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-cbe84ea88ec11012ef86e702fc7ee54a.png?v=0" title="\sigma^2_{\hat \beta _1}=Q^2w^2\sigma^2\sum^n_{i=1}(\lambda u_i^2+v_i^2)" alt="\sigma^2_{\hat \beta _1}=Q^2w^2\sigma^2\sum^n_{i=1}(\lambda u_i^2+v_i^2)" class="tex"/>
</th>
<td>
</td></tr></table>
<p>The standard error for parameters can be estimated by:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-6632645192059c2282b23580a508a66e.png?v=0" title="\varepsilon _{\hat \beta _0}=\sqrt{\sigma^2_{\hat \beta _0}}" alt="\varepsilon _{\hat \beta _0}=\sqrt{\sigma^2_{\hat \beta _0}}" class="tex"/>
</th>
<td>
<p>(37)
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-49d10be0d0c9f5af09d8f54a0f0c07a9.png?v=0" title="\varepsilon _{\hat \beta _1}=\sqrt{\sigma^2_{\hat \beta _1}}" alt="\varepsilon _{\hat \beta _1}=\sqrt{\sigma^2_{\hat \beta _1}}" class="tex"/>
</th>
<td>
<p>(38) 
</p>
</td></tr></table>
<p>and
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-42ef22057c063c56ff7aaebd77af194e.png?v=0" title="w=\frac{1}{\sigma^2(\lambda+\hat{\beta_1}^2)}" alt="w=\frac{1}{\sigma^2(\lambda+\hat{\beta_1}^2)}" class="tex"/>
</th>
<td>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-3e7fef69e46279e49712a17851cc28d1.png?v=0" title="z_i=w\sigma^2(\lambda u_i+\hat{\beta_1} v_i)" alt="z_i=w\sigma^2(\lambda u_i+\hat{\beta_1} v_i)" class="tex"/>
</th>
<td>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-bca17961c43098a1fa2d3ee1a32879b1.png?v=0" title="\bar{z}=\frac{1}{n}\sum_{i=1}^n z_i" alt="\bar{z}=\frac{1}{n}\sum_{i=1}^n z_i" class="tex"/>
</th>
<td>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-ea78829455669cb7414deb799ae782b9.png?v=0" title="Q=\frac{1}{w \sum_{i=1}^n \left(\frac{u_iv_i}{\hat{\beta_1}}+4(z_i-\bar{z})(z_i-u_i)\right)}" alt="Q=\frac{1}{w \sum_{i=1}^n \left(\frac{u_iv_i}{\hat{\beta_1}}+4(z_i-\bar{z})(z_i-u_i)\right)}" class="tex"/>
</th>
<td>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-0dec0855e663537c4e849312f0364319.png?v=0" title="\sigma=\sqrt{\frac{\sum^n_{i=1}(x_i-X_i)^2+\frac{\sum^n_{i=1}(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)^2}{\lambda}}{n-2}}" alt="\sigma=\sqrt{\frac{\sum^n_{i=1}(x_i-X_i)^2+\frac{\sum^n_{i=1}(y_i-\hat{\beta_0}-\hat{\beta_1}x_i)^2}{\lambda}}{n-2}}" class="tex"/>
</th>
<td>
</td></tr></table>
<h4><a name="t-Value_and_Confidence_Level_2"></a><span class="mw-headline">t-Value and Confidence Level</span></h4>
<p>If the regression assumptions hold, we have: 
</p>
<table class="formula">

<tr>
<td><img src="../images/Linear_XError_Results/math-0ed681c4ca83831abc9e0afe2f959db2.png?v=0" title="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" class="tex"/>  and  <img src="../images/Linear_XError_Results/math-b502ef25a978991781d791a9f6275a83.png?v=0" title="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" class="tex"/>
</td>
<td>
</td></tr></table>
<p>The <i>t</i>-test can be used to examine whether the fitting parameters are significantly different from zero, which means that we can test whether  <img src="../images/Linear_XError_Results/math-ae198091d3f29f3101f5b0b86bf85b5d.png?v=0" title="\beta _0= 0\,\!" alt="\beta _0= 0\,\!" class="tex"/> (if true, this means that the fitted line passes through the origin) or <img src="../images/Linear_XError_Results/math-c1cb3b8aa03a36c23962a83cf7d2dc40.png?v=0" title="\beta _1= 0\,\!" alt="\beta _1= 0\,\!" class="tex"/>. The hypotheses of the <i>t</i>-tests are: 
</p>
<dl><dd><img src="../images/Linear_XError_Results/math-d15540fa2e02fa998eea73fd85bf1952.png?v=0" title="H_0&#160;: \beta _0= 0\,\! " alt="H_0&#160;: \beta _0= 0\,\! " class="tex"/>                  <img src="../images/Linear_XError_Results/math-57dce98807a9a866ee09b5bbe88bb68e.png?v=0" title="H_0&#160;: \beta _1= 0\,\!" alt="H_0&#160;: \beta _1= 0\,\!" class="tex"/> </dd>
<dd><img src="../images/Linear_XError_Results/math-00f883e098653776a99683aae70c3783.png?v=0" title="H_\alpha &#160;: \beta _0  \neq 0\,\!" alt="H_\alpha &#160;: \beta _0  \neq 0\,\!" class="tex"/>         <img src="../images/Linear_XError_Results/math-27aac7aa6524d0b486a1890feffff3e1.png?v=0" title="H_\alpha &#160;: \beta _1 \neq  0\,\!" alt="H_\alpha &#160;: \beta _1 \neq  0\,\!" class="tex"/>     </dd></dl>
<p>The <i>t</i>-values can be computed by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-6368690db66f6b16f642c2976ba74d59.png?v=0" title="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" alt="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" class="tex"/>  and <img src="../images/Linear_XError_Results/math-abcdef6859e44b55ed86ce261400d36c.png?v=0" title="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" alt="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" class="tex"/>
</th>
<td>
<p>(38)  
</p>
</td></tr></table>
<p>With the computed <i>t</i>-value, we can decide whether or not to reject the corresponding null hypothesis. Usually, for a given confidence level <img src="../images/Linear_XError_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>   , we can reject <img src="../images/Linear_XError_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> when <img src="../images/Linear_XError_Results/math-9d38e8e7363d66af41530c3fbdf494f5.png?v=0" title="|t|&gt;t_{\frac \alpha 2}" alt="|t|&gt;t_{\frac \alpha 2}" class="tex"/>. Additionally, the <i>p</i>-value, or significance level, is reported with a <i>t</i>-test. We also reject the null hypothesis <img src="../images/Linear_XError_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> if the <i>p</i>-value is less than <img src="../images/Linear_XError_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>.
</p>
<h4><a name="Prob.3E.7Ct.7C_2"></a><span class="mw-headline">Prob&gt;|t|</span></h4>
<p>The probability that <img src="../images/Linear_XError_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> in the <i>t</i> test above is true. 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-3e5335a131705b3ef0bb8b250033167d.png?v=0" title="prob=2(1-tcdf(|t|,df_{Error}))\,\!" alt="prob=2(1-tcdf(|t|,df_{Error}))\,\!" class="tex"/>
</th>
<td>
<p>(39)  
</p>
</td></tr></table>
<p>where <i>tcdf(t, df)</i> computes the lower tail probability for the Student's <i>t</i> distribution with <i>df</i> degree of freedom.
</p>
<h4><a name="LCL_and_UCL_2"></a><span class="mw-headline">LCL and UCL</span></h4>
<p>From the <i>t</i>-value, we can calculate the <img src="../images/Linear_XError_Results/math-0cdaba532a6f8ad706e6b8e5e97f0d36.png?v=0" title="(1-\alpha )\times 100\%" alt="(1-\alpha )\times 100\%" class="tex"/>  <b>Confidence Interval</b> for each parameter by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-a27fc4294ee80efb51793fbcd4e23df2.png?v=0" title="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" alt="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" class="tex"/>
</th>
<td>
<p>(40)  
</p>
</td></tr></table>
<p>where <img src="../images/Linear_XError_Results/math-fd9dfeda668ac9b0c5ef311ffdca8f71.png?v=0" title="UCL" alt="UCL" class="tex"/> and <img src="../images/Linear_XError_Results/math-a4df64dc5e32716f6de0ec15b0340950.png?v=0" title="LCL" alt="LCL" class="tex"/> is short for the <b>Upper Confidence Interval</b> and <b>Lower Confidence Interval</b>, respectively.
</p>
<h4><a name="CI_Half_Width_2"></a><span class="mw-headline">CI Half Width</span></h4>
<p>The Confidence Interval Half Width is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-77ec7c61ac9d06bfc9ad4c0ae47b85ce.png?v=0" title="CI=\frac{UCL-LCL}2" alt="CI=\frac{UCL-LCL}2" class="tex"/>
</th>
<td>
<p>(41)  
</p>
</td></tr></table>
<p>where UCL and LCL is the <b>Upper Confidence Interval</b> and <b>Lower Confidence Interval</b>, respectively.
</p><p>For more information <a class="external text" href="../../UserGuide/UserGuide/Linear_XError_Results.html#Reference">,see Reference 1 (below)</a>.
</p>
<h3><a name="Fit_Statistics_2"></a><span class="mw-headline">Fit Statistics</span></h3>
<p><a  class="image"><img alt="Deming Stats.png" src="../images/Linear_XError_Results/Deming_Stats.png?v=66628" width="251"  /></a>
</p>
<h4><a name="Degrees_of_Freedom_2"></a><span class="mw-headline">Degrees of Freedom</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-8dbf22cb95c3b3dc33d43f0dcb4582b7.png?v=0" title="df=n-2" alt="df=n-2" class="tex"/>
</th>
<td>
<p>(42) 
</p>
</td></tr></table>
<p>n is total number of points
</p>
<h4><a name="Residual_Sum_of_Squares_2"></a><span class="mw-headline">Residual Sum of Squares</span></h4>
<p>See formula (33)
</p>
<h4><a name="Reduced_Chi-Sqr_2"></a><span class="mw-headline">Reduced Chi-Sqr</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-c0df1f687a05ad41611867fa7424fbeb.png?v=0" title="\sigma^2=\frac{RSS}{n-2}" alt="\sigma^2=\frac{RSS}{n-2}" class="tex"/>
</th>
<td>
<p>(43) 
</p>
</td></tr></table>
<h4><a name="Pearson.27s_r_2"></a><span class="mw-headline">Pearson's r</span></h4>
<p>In simple linear regression, the correlation coefficient between x and y, denoted by <i>r</i>, equals to: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-f7f1dc7cce20e9b9b19e47b012ee173d.png?v=0" title="r=R\,\!" alt="r=R\,\!" class="tex"/>   <i>if <img src="../images/Linear_XError_Results/math-3a1e29703e364979554e6bd76761bed6.png?v=0" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"/> is positive</i>
</th>
<td>
<p>(44)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-556cec8e44138ef9cf002c70434cb55d.png?v=0" title="r=-R\,\!" alt="r=-R\,\!" class="tex"/>   <i>if <img src="../images/Linear_XError_Results/math-3a1e29703e364979554e6bd76761bed6.png?v=0" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"/> is negative</i>
</th></tr></table>
<p><b><img src="../images/Linear_XError_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/></b> can be computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-9ce15fecaa71e5021c67564b9f212551.png?v=0" title="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" alt="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" class="tex"/>
</th>
<td>
<p>(45)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-143fa9a82ab7c4beeb5f48ff2f737e5f.png?v=0" title="TSS=\sum_{i=1}^n(y_i-\bar{y})^2" alt="TSS=\sum_{i=1}^n(y_i-\bar{y})^2" class="tex"/>
</th></tr></table>
<h4><a name="Root-MSE_.28SD.29_2"></a><span class="mw-headline">Root-MSE (SD)</span></h4>
<p>Root mean square of the error, which equals to: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-5d65f33336e7ea1ead5adec051bf5572.png?v=0" title="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" alt="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" class="tex"/>
</th>
<td>
<p>(46) 
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_XError_Results/math-c20f8bb6c5735ccda8a5e88cff767c30.png?v=0" title="df_{Error}=n-2" alt="df_{Error}=n-2" class="tex"/>
</th></tr></table>
<h4><a name="Covariance_and_Correlation_Matrix_2"></a><span class="mw-headline">Covariance and Correlation Matrix</span></h4>
<p>The Covariance matrix of linear regression is calculated by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-30e3c5c9127c7982f5851e6a515d2bf5.png?v=0" title="&#10;\begin{pmatrix}&#10;Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\&#10;Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1)&#10;\end{pmatrix}=\begin{pmatrix} \ \sigma^2_{\hat{\beta_0}}  &amp; -\bar{x}\sigma^2_{\hat \beta _1} \\-\bar{x}\sigma^2_{\hat \beta _1} &amp;\sigma^2_{\hat{\beta_1}} \end{pmatrix}" alt="&#10;\begin{pmatrix}&#10;Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\&#10;Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1)&#10;\end{pmatrix}=\begin{pmatrix} \ \sigma^2_{\hat{\beta_0}}  &amp; -\bar{x}\sigma^2_{\hat \beta _1} \\-\bar{x}\sigma^2_{\hat \beta _1} &amp;\sigma^2_{\hat{\beta_1}} \end{pmatrix}" class="tex"/>
</th>
<td>
<p>(47)  
</p>
</td></tr></table>
<p>The correlation between any two parameters is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_XError_Results/math-b029662d99694045f6e48fab550e2b23.png?v=0" title="&#10;\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " alt="&#10;\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " class="tex"/>
</th>
<td>
<p>(48)  
</p>
</td></tr></table>
<h2><a name="Finding_X.2FY"></a><span class="mw-headline"><a href="../../UserGuide/UserGuide/Finding_Y_X_from_X_Y_Standard_Curves.html" title="UserGuide:Finding Y X from X Y Standard Curves">Finding X/Y</a></span></h2>
<h2><a name="Residual_Plots"></a><span class="mw-headline">Residual Plots</span></h2>
<h3><a name="Residual_vs._Independent"></a><span class="mw-headline">Residual vs. Independent</span></h3>
<p>Scatter plot of residual <img src="../images/Linear_XError_Results/math-9b207167e5381c47682c6b4f58a623fb.png?v=0" title="res" alt="res" class="tex"/> vs. indenpendent variable <img src="../images/Linear_XError_Results/math-75087036f44bb88958df97586ae3bd1e.png?v=0" title="x_1,x_2,\dots,x_k" alt="x_1,x_2,\dots,x_k" class="tex"/>, each plot is located in a seperate graphs.
</p>
<h3><a name="Residual_vs._Predicted_Value"></a><span class="mw-headline">Residual vs. Predicted Value</span></h3>
<p>Scatter plot of residual <img src="../images/Linear_XError_Results/math-9b207167e5381c47682c6b4f58a623fb.png?v=0" title="res" alt="res" class="tex"/> vs. fitted results <img src="../images/Linear_XError_Results/math-c9e53cbdffe795c0913cd927f13ffb9b.png?v=0" title="\hat{y_i}" alt="\hat{y_i}" class="tex"/>
</p>
<h3><a name="Residual_vs._Order_of_the_Data"></a><span class="mw-headline">Residual vs. Order of the Data</span></h3>
<p><img src="../images/Linear_XError_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/> vs. sequence number <img src="../images/Linear_XError_Results/math-865c0c0b4ab0e063e5caa3387c1a8741.png?v=0" title="i" alt="i" class="tex"/>
</p>
<h3><a name="Histogram_of_the_Residual"></a><span class="mw-headline">Histogram of the Residual</span></h3>
<p>The Histogram plot of the Residual <img src="../images/Linear_XError_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/>
</p>
<h3><a name="Residual_Lag_Plot"></a><span class="mw-headline">Residual Lag Plot</span></h3>
<p>Residuals <img src="../images/Linear_XError_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/> vs. lagged residual <img src="../images/Linear_XError_Results/math-00407b58eb35fecbfea6dc5d4413e493.png?v=0" title="res_{(i&#8211;1)}" alt="res_{(i&#8211;1)}" class="tex"/>.
</p>
<h3><a name="Normal_Probability_Plot_of_Residuals"></a><span class="mw-headline">Normal Probability Plot of Residuals</span></h3>
<p>A normal probability plot of the residuals can be used to check whether the variance is normally distributed as well. If the resulting plot is approximately linear, we proceed to assume that the error terms are normally distributed. The plot is based on the percentiles versus ordered residual, and the percentiles is estimated by 
</p><p><img src="../images/Linear_XError_Results/math-ec995bff3922b2290f85bed243de5eb3.png?v=0" title="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" alt="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" class="tex"/> 
</p><p>where <i>n</i> is the total number of dataset and  <i>i</i> is the <i>i</i> th data.
Also refer to <a href="../../UserGuide/UserGuide/Probability_Plot_and_Q-Q_Plot.html" title="UserGuide:Probability Plot and Q-Q Plot">Probability Plot and Q-Q Plot</a>
</p>
<h2><a name="Reference"></a><span class="mw-headline">Reference</span></h2>
<ol><li> York D, Unified equations for the slope, intercept, and standard error of the best straight line, American Journal of Physics, Volume 72, Issue 3, pp. 367-375 (2004).</li>
<li> G. Fasano and R. Vio, "Fitting straight lines with errors on both coordinates", Newsletter of Working Group for Modern Astronomical Methodology, No. 7, 2-7, Sept. 1988.</li></ol>






