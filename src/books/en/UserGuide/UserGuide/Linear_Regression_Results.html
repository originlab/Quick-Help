<h1 class="firstHeading">15.2.5 Algorithms (Linear Regression)</h1><p class='urlname' style='display: none'>LR-Algorithm</p>
<div class="toclimit-3"><div id="toc" class="toc"><div id="toctitle"><h2>Contents</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#The_Linear_Regression_Model"><span class="tocnumber">1</span> <span class="toctext">The Linear Regression Model</span></a>
<ul>
<li class="toclevel-2 tocsection-2"><a href="#Simple_Linear_Regression_Model"><span class="tocnumber">1.1</span> <span class="toctext">Simple Linear Regression Model</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-3"><a href="#Fit_Control"><span class="tocnumber">2</span> <span class="toctext">Fit Control</span></a>
<ul>
<li class="toclevel-2 tocsection-4"><a href="#Errors_as_Weight"><span class="tocnumber">2.1</span> <span class="toctext">Errors as Weight</span></a>
<ul>
<li class="toclevel-3 tocsection-5"><a href="#No_Weighting"><span class="tocnumber">2.1.1</span> <span class="toctext">No Weighting</span></a></li>
<li class="toclevel-3 tocsection-6"><a href="#Direct_Weighting"><span class="tocnumber">2.1.2</span> <span class="toctext">Direct Weighting</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Instrumental"><span class="tocnumber">2.1.3</span> <span class="toctext">Instrumental</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-8"><a href="#Fix_Intercept_.28at.29"><span class="tocnumber">2.2</span> <span class="toctext">Fix Intercept (at)</span></a></li>
<li class="toclevel-2 tocsection-9"><a href="#Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"><span class="tocnumber">2.3</span> <span class="toctext">Scale Error with sqrt(Reduced Chi-Sqr)</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#Fit_Results"><span class="tocnumber">3</span> <span class="toctext">Fit Results</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#Fit_Parameters"><span class="tocnumber">3.1</span> <span class="toctext">Fit Parameters</span></a>
<ul>
<li class="toclevel-3 tocsection-12"><a href="#Fitted_value"><span class="tocnumber">3.1.1</span> <span class="toctext">Fitted value</span></a></li>
<li class="toclevel-3 tocsection-13"><a href="#The_Parameter_Standard_Errors"><span class="tocnumber">3.1.2</span> <span class="toctext">The Parameter Standard Errors</span></a></li>
<li class="toclevel-3 tocsection-14"><a href="#t-Value_and_Confidence_Level"><span class="tocnumber">3.1.3</span> <span class="toctext">t-Value and Confidence Level</span></a></li>
<li class="toclevel-3 tocsection-15"><a href="#Prob.3E.7Ct.7C"><span class="tocnumber">3.1.4</span> <span class="toctext">Prob&gt;|t|</span></a></li>
<li class="toclevel-3 tocsection-16"><a href="#LCL_and_UCL"><span class="tocnumber">3.1.5</span> <span class="toctext">LCL and UCL</span></a></li>
<li class="toclevel-3 tocsection-17"><a href="#CI_Half_Width"><span class="tocnumber">3.1.6</span> <span class="toctext">CI Half Width</span></a></li>
</ul>
</li>
<li class="toclevel-2 tocsection-18"><a href="#Fit_Statistics"><span class="tocnumber">3.2</span> <span class="toctext">Fit Statistics</span></a>
<ul>
<li class="toclevel-3 tocsection-19"><a href="#Degrees_of_Freedom"><span class="tocnumber">3.2.1</span> <span class="toctext">Degrees of Freedom</span></a></li>
<li class="toclevel-3 tocsection-20"><a href="#Residual_Sum_of_Squares"><span class="tocnumber">3.2.2</span> <span class="toctext">Residual Sum of Squares</span></a></li>
<li class="toclevel-3 tocsection-21"><a href="#Reduced_Chi-Sqr"><span class="tocnumber">3.2.3</span> <span class="toctext">Reduced Chi-Sqr</span></a></li>
<li class="toclevel-3 tocsection-22"><a href="#R-Square_.28COD.29"><span class="tocnumber">3.2.4</span> <span class="toctext">R-Square (COD)</span></a></li>
<li class="toclevel-3 tocsection-23"><a href="#Adj._R-Square"><span class="tocnumber">3.2.5</span> <span class="toctext">Adj. R-Square</span></a></li>
<li class="toclevel-3 tocsection-24"><a href="#R_Value"><span class="tocnumber">3.2.6</span> <span class="toctext">R Value</span></a></li>
<li class="toclevel-3 tocsection-25"><a href="#Pearson.27s_r"><span class="tocnumber">3.2.7</span> <span class="toctext">Pearson's r</span></a></li>
<li class="toclevel-3 tocsection-26"><a href="#Root-MSE_.28SD.29"><span class="tocnumber">3.2.8</span> <span class="toctext">Root-MSE (SD)</span></a></li>
<li class="toclevel-3 tocsection-27"><a href="#Norm_of_Residuals"><span class="tocnumber">3.2.9</span> <span class="toctext">Norm of Residuals</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-28"><a href="#ANOVA_Table"><span class="tocnumber">4</span> <span class="toctext">ANOVA Table</span></a></li>
<li class="toclevel-1 tocsection-29"><a href="#Lack_of_fit_table"><span class="tocnumber">5</span> <span class="toctext">Lack of fit table</span></a></li>
<li class="toclevel-1 tocsection-30"><a href="#Covariance_and_Correlation_Matrix"><span class="tocnumber">6</span> <span class="toctext">Covariance and Correlation Matrix</span></a></li>
<li class="toclevel-1 tocsection-31"><a href="#Outliers"><span class="tocnumber">7</span> <span class="toctext">Outliers</span></a></li>
<li class="toclevel-1 tocsection-32"><a href="#Residual_Analysis"><span class="tocnumber">8</span> <span class="toctext">Residual Analysis</span></a>
<ul>
<li class="toclevel-2 tocsection-33"><a href="#Standardized"><span class="tocnumber">8.1</span> <span class="toctext">Standardized</span></a></li>
<li class="toclevel-2 tocsection-34"><a href="#Studentized"><span class="tocnumber">8.2</span> <span class="toctext">Studentized</span></a></li>
<li class="toclevel-2 tocsection-35"><a href="#Studentized_deleted"><span class="tocnumber">8.3</span> <span class="toctext">Studentized deleted</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-36"><a href="#Confidence_and_Prediction_Bands"><span class="tocnumber">9</span> <span class="toctext">Confidence and Prediction Bands</span></a></li>
<li class="toclevel-1 tocsection-37"><a href="#Confidence_Ellipses"><span class="tocnumber">10</span> <span class="toctext">Confidence Ellipses</span></a></li>
<li class="toclevel-1 tocsection-38"><a href="#Finding_Y.2FX_from_X.2FY"><span class="tocnumber">11</span> <span class="toctext">Finding Y/X from X/Y</span></a></li>
<li class="toclevel-1 tocsection-39"><a href="#Residual_Plots"><span class="tocnumber">12</span> <span class="toctext">Residual Plots</span></a>
<ul>
<li class="toclevel-2 tocsection-40"><a href="#Resudial_Type"><span class="tocnumber">12.1</span> <span class="toctext">Resudial Type</span></a></li>
<li class="toclevel-2 tocsection-41"><a href="#Residual_vs._Independent"><span class="tocnumber">12.2</span> <span class="toctext">Residual vs. Independent</span></a></li>
<li class="toclevel-2 tocsection-42"><a href="#Residual_vs._Predicted_Value"><span class="tocnumber">12.3</span> <span class="toctext">Residual vs. Predicted Value</span></a></li>
<li class="toclevel-2 tocsection-43"><a href="#Residual_vs._Order_of_the_Data"><span class="tocnumber">12.4</span> <span class="toctext">Residual vs. Order of the Data</span></a></li>
<li class="toclevel-2 tocsection-44"><a href="#Histogram_of_the_Residual"><span class="tocnumber">12.5</span> <span class="toctext">Histogram of the Residual</span></a></li>
<li class="toclevel-2 tocsection-45"><a href="#Residual_Lag_Plot"><span class="tocnumber">12.6</span> <span class="toctext">Residual Lag Plot</span></a></li>
<li class="toclevel-2 tocsection-46"><a href="#Normal_Probability_Plot_of_Residuals"><span class="tocnumber">12.7</span> <span class="toctext">Normal Probability Plot of Residuals</span></a></li>
</ul>
</li>
</ul>
</div>
</div> 
<h2><a name="The_Linear_Regression_Model"></a><span class="mw-headline">The Linear Regression Model</span></h2>
<h3><a name="Simple_Linear_Regression_Model"></a><span class="mw-headline">Simple Linear Regression Model</span></h3>
<p>For a given dataset <img src="../images/Linear_Regression_Results/math-f243bd7bc9626358c4cbdace83fad108.png?v=0" title="(x_i,y_i),i=1,2,\ldots n" alt="(x_i,y_i),i=1,2,\ldots n" class="tex"/> -- where x is the independent variable and y is the dependent variable, <img src="../images/Linear_Regression_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png?v=0" title="\beta_0" alt="\beta_0" class="tex"/> and <img src="../images/Linear_Regression_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png?v=0" title="\beta_1" alt="\beta_1" class="tex"/> are parameters, and <img src="../images/Linear_Regression_Results/math-3ac22ebe353c690d089056a1a61e884d.png?v=0" title="\varepsilon_i" alt="\varepsilon_i" class="tex"/> is a random error term with mean <img src="../images/Linear_Regression_Results/math-e08f05e02ca7379fa770d67c7f995ec4.png?v=0" title="E\left \{\varepsilon_i\right \}=0" alt="E\left \{\varepsilon_i\right \}=0" class="tex"/> and variance <img src="../images/Linear_Regression_Results/math-57e78781f84e3e3175bcc12c95318cfb.png?v=0" title="Var\left \{\varepsilon_i\right \}=\sigma^2" alt="Var\left \{\varepsilon_i\right \}=\sigma^2" class="tex"/> -- linear regression fits the data to a model of the following form: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-e7275a1e9849b30e085befefdcf649e4.png?v=0" title="y_i=\beta _0+\beta _1x_i+\varepsilon_i" alt="y_i=\beta _0+\beta _1x_i+\varepsilon_i" class="tex"/>
</th>
<td>
<p>(1)
</p>
</td></tr></table>
<p>The least squares estimation is used to minimize the sum of the <i>n</i> squared deviations
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-88e33546e2dca0c68fbd64d62f733914.png?v=0" title="\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2" alt="\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2" class="tex"/>
</th>
<td>
<p>(2)
</p>
</td></tr></table>
<p>the estimated parameters of linear model can be computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-befdde5b640aa0a77538792c238ed5cd.png?v=0" title="\hat\beta _1=\frac{SXY}{SXX}" alt="\hat\beta _1=\frac{SXY}{SXX}" class="tex"/>
</th>
<td>
<p>(3)  
</p>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-fb3f1891e62df179b758521b10084aa6.png?v=0" title="\hat\beta _0=\bar y-\hat\beta _1\bar x " alt="\hat\beta _0=\bar y-\hat\beta _1\bar x " class="tex"/>
</th>
<td>
<p>(4)  
</p>
</td></tr></table>
<p>where: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-322527b1ed6496cd6cdf25a0002dfc7f.png?v=0" title="\bar x=\frac {1}{n}\sum_{i=1}^nx_i" alt="\bar x=\frac {1}{n}\sum_{i=1}^nx_i" class="tex"/>,<img src="../images/Linear_Regression_Results/math-595a61adb4402eacacded49c4e4bbf1a.png?v=0" title="\bar y=\frac {1}{n}\sum_{i=1}^ny_i" alt="\bar y=\frac {1}{n}\sum_{i=1}^ny_i" class="tex"/>
</th>
<td>
<p>(5)  
</p>
</td></tr></table>
<p>and
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-475a6b18f39e0bc196ab8aeee961b0f0.png?v=0" title="SXY=\sum_{i=1}^nx_iy_i\; \; \; \; \; \; \; SXX=\sum_{i=1}^nx_i^2" alt="SXY=\sum_{i=1}^nx_iy_i\; \; \; \; \; \; \; SXX=\sum_{i=1}^nx_i^2" class="tex"/>    (uncorrected)
</th>
<td>
<p>(6)  
</p>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-2b50c9938d5981cb89ffaec033abab1c.png?v=0" title="SXY=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)\; \; \; \; \; \; \; SXX=\sum_{i=1}^n(x_i-\bar x)^2" alt="SXY=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)\; \; \; \; \; \; \; SXX=\sum_{i=1}^n(x_i-\bar x)^2" class="tex"/>  (corrected)
</th>
<td>
<p>(7)  
</p>
</td></tr></table>
<table class="note">

<tr>
<td><b>Note:</b> When the intercept is excluded from the model, the coefficients are calculated using the <b>uncorrected</b> formula.
</td></tr></table>
<p>Therefore, we estimate the regression function as follows:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-4629c75f122b9d4b551eb493d45a295e.png?v=0" title="\hat{y}=\hat{\beta_0}+\hat{\beta_1}x" alt="\hat{y}=\hat{\beta_0}+\hat{\beta_1}x" class="tex"/>
</th>
<td>
<p>(8)  
</p>
</td></tr></table>
<p>the residual <img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/> is defined as:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-1eccdc0d4d12d0c892775e7682366d77.png?v=0" title="res_i=y_i-\hat{y_i}" alt="res_i=y_i-\hat{y_i}" class="tex"/>
</th>
<td>
<p>(9)  
</p>
</td></tr></table>
<p>formula in (2) is to be minimized equaling to residual sum of squares
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-3b4c813cc6108fdfb9d72a3346e2cd15.png?v=0" title="RSS=\sum_{i=1}^nres_i^2" alt="RSS=\sum_{i=1}^nres_i^2" class="tex"/>
</th>
<td>
<p>(10)  
</p>
</td></tr></table>
<p>when the least squares estimators <img src="../images/Linear_Regression_Results/math-9dabc344cb060be9355c54cc39a038db.png?v=0" title="\hat{\beta_0}" alt="\hat{\beta_0}" class="tex"/> and <img src="../images/Linear_Regression_Results/math-4db6565d3866d2baeaf7a66f389744f9.png?v=0" title="\hat{\beta_1}" alt="\hat{\beta_1}" class="tex"/> are used for estimating <img src="../images/Linear_Regression_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png?v=0" title="\beta_0" alt="\beta_0" class="tex"/> and  <img src="../images/Linear_Regression_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png?v=0" title="\beta_1" alt="\beta_1" class="tex"/>.
</p>
<h2><a name="Fit_Control"></a><span class="mw-headline">Fit Control</span></h2>
<h3><a name="Errors_as_Weight"></a><span class="mw-headline">Errors as Weight</span></h3>
<p>In above section, we assume that there is constant variance in the errors. However, when we fit the experimental data, we may need to take the instrument error (which reflect the accuracy and precision of a measuring instrument) into account in fitting process. Therefore, the assumption of constant variance in the errors is violated. Thus, we need to assume <img src="../images/Linear_Regression_Results/math-3ac22ebe353c690d089056a1a61e884d.png?v=0" title="\varepsilon_i" alt="\varepsilon_i" class="tex"/> to be normally distributed with nonconstant variance, and the errors act as <img src="../images/Linear_Regression_Results/math-10e16c6a764d367ca5077a54bf156f7e.png?v=0" title="\sigma^2" alt="\sigma^2" class="tex"/>, which can be used as weight in fitting. The weight is defined as:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-8845a41888129439e7f1b3fbdaeb2078.png?v=0" title="W=\begin{bmatrix}&#10; w_1&amp; 0 &amp; \dots &amp;0 \\ &#10;0 &amp; w_2 &amp; \dots &amp;0 \\ &#10; \vdots&amp; \vdots &amp;\ \ddots &amp;\vdots \\ &#10; 0&amp; 0 &amp;\dots  &amp; w_n&#10;\end{bmatrix}" alt="W=\begin{bmatrix}&#10; w_1&amp; 0 &amp; \dots &amp;0 \\ &#10;0 &amp; w_2 &amp; \dots &amp;0 \\ &#10; \vdots&amp; \vdots &amp;\ \ddots &amp;\vdots \\ &#10; 0&amp; 0 &amp;\dots  &amp; w_n&#10;\end{bmatrix}" class="tex"/>
</th>
<td>
</td></tr></table>
<p>The fitting model is changed into:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-b91af30efe2f541053551a9080cdf9de.png?v=0" title="\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\hat{\beta _0}+\hat{\beta _1}x_i)]^2" alt="\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\hat{\beta _0}+\hat{\beta _1}x_i)]^2" class="tex"/>
</th>
<td>
<p>(11)  
</p>
</td></tr></table>
<p>The weight factors <img src="../images/Linear_Regression_Results/math-aa38f107289d4d73d516190581397349.png?v=0" title="w_i" alt="w_i" class="tex"/> can be given by three formulas:
</p>
<h4><a name="No_Weighting"></a><span class="mw-headline">No Weighting</span></h4>
<p>The error bar will not be treated as weight in calculation.
</p>
<h4><a name="Direct_Weighting"></a><span class="mw-headline">Direct Weighting</span></h4>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-05c143b7a8ecc59d8edb905c4b6e3397.png?v=0" title="w_i=\sigma_i " alt="w_i=\sigma_i " class="tex"/>
</th>
<td>
<p>(12)
</p>
</td></tr></table>
<h4><a name="Instrumental"></a><span class="mw-headline">Instrumental</span></h4>
<p>As for Instrumental weight, the value is inversely proportional to the instrumental errors, so a trial with small errors will have a larger weight because it is rather precise than some other trials with larger errors.
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-63f1838aba2791cf68521fe43a435763.png?v=0" title="w_i=\frac 1{\sigma_i^2}" alt="w_i=\frac 1{\sigma_i^2}" class="tex"/>
</th>
<td>
<p>(13)
</p>
</td></tr></table>
<table class="note">

<tr>
<td><b>Note:</b> The errors as weight should be desiganited as "YError" column in worksheet.
</td>
<td>
</td></tr></table>
<h3><a name="Fix_Intercept_.28at.29"></a><span class="mw-headline">Fix Intercept (at)</span></h3>
<p>Fix intercept will set the y-intercept <img src="../images/Linear_Regression_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png?v=0" title="\beta_0" alt="\beta_0" class="tex"/> to a fixed value, meanwhile, the total degree of freedom will be n*=n-1 due to the intercept fixed.
</p>
<h3><a name="Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"></a><span class="mw-headline">Scale Error with sqrt(Reduced Chi-Sqr)</span></h3>
<p><b>Scale Error with sqrt(Reduced Chi-Sqr)</b> is available when fitting with weight. This option only affects the error on the parameters reported from the fitting process, and does not affect the fitting process or the data in any way. 
By default, it is checked, and <img src="../images/Linear_Regression_Results/math-10e16c6a764d367ca5077a54bf156f7e.png?v=0" title="\sigma^2" alt="\sigma^2" class="tex"/> is taken into account when calculate error on the parameters, otherwise,<img src="../images/Linear_Regression_Results/math-10e16c6a764d367ca5077a54bf156f7e.png?v=0" title="\sigma^2" alt="\sigma^2" class="tex"/> will not be taken into account for error calculation.
Take Covariance Matrix as an example:
Scale Error with sqrt(Reduced Chi-Sqr):
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-a219511b6fc22ebe963c31f36a1b4dd7.png?v=0" title="Cov(\beta _i,\beta _j)=\sigma^2 (X^{\prime }X)^{-1}" alt="Cov(\beta _i,\beta _j)=\sigma^2 (X^{\prime }X)^{-1}" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_Regression_Results/math-aa0f6ec7a20808737c0ce43c3cba8892.png?v=0" title="\sigma^2=\frac{RSS}{n^{*}-1}" alt="\sigma^2=\frac{RSS}{n^{*}-1}" class="tex"/>
</th>
<td>
<p>(14)
</p>
</td></tr></table>
<p>Do not Scale Error with sqrt(Reduced Chi-Sqr):
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-d654da511627b08f8bb2817849d7a889.png?v=0" title="Cov(\beta _i,\beta _j)=(X&#39;X)^{-1}\,\!" alt="Cov(\beta _i,\beta _j)=(X&#39;X)^{-1}\,\!" class="tex"/>
</th>
<td>
<p>(15)
</p>
</td></tr></table>
<p>For weighted fitting, <img src="../images/Linear_Regression_Results/math-6f369f83036bd2eab887744dd10a6eb7.png?v=0" title="(X&#39;WX)^{-1}\,\!" alt="(X&#39;WX)^{-1}\,\!" class="tex"/> is used instead of <img src="../images/Linear_Regression_Results/math-8f5640d983fa3d67b37e5356b6a62a78.png?v=0" title="(X&#39;X)^{-1}\,\!" alt="(X&#39;X)^{-1}\,\!" class="tex"/>.
</p>
<h2><a name="Fit_Results"></a><span class="mw-headline">Fit Results</span></h2>
<p>When you perform a linear fit, you generate an <a class="external text" href="../../UserGuide/UserGuide/Analysis_Report_Sheets_and_Columns.html">analysis report sheet</a> listing computed quantities.  The Parameters table reports model slope and intercept (numbers in parentheses show how the quantities are derived): 
</p>
<h3><a name="Fit_Parameters"></a><span class="mw-headline">Fit Parameters</span></h3>
<p><a  class="image"><img alt="Fitted-paramater.png" src="../images/Linear_Regression_Results/Fitted-paramater.png?v=63542" width="655"  /></a>
</p>
<h4><a name="Fitted_value"></a><span class="mw-headline">Fitted value</span></h4>
<p>See formula (3)&amp;(4)
</p>
<h4><a name="The_Parameter_Standard_Errors"></a><span class="mw-headline">The Parameter Standard Errors</span></h4>
<p>For each parameter, the standard error can be obtained by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-a2e4c535ba2702d3744f6a49bfa4ac1c.png?v=0" title="\varepsilon _{\hat \beta _0}=s_\varepsilon \sqrt{\frac{\sum x_i^2}{nSXX}}" alt="\varepsilon _{\hat \beta _0}=s_\varepsilon \sqrt{\frac{\sum x_i^2}{nSXX}}" class="tex"/>
</th>
<td>
<p>(16)  
</p>
</td></tr></table>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-01aff625e5ad40105a12347ba05339bc.png?v=0" title="\varepsilon _{\hat \beta _1}=\frac{s_\varepsilon }{\sqrt{SXX}}" alt="\varepsilon _{\hat \beta _1}=\frac{s_\varepsilon }{\sqrt{SXX}}" class="tex"/>
</th>
<td>
<p>(17)  
</p>
</td></tr></table>
<p>where the sample variance <img src="../images/Linear_Regression_Results/math-f5fccf3ce2663df316135e66a00c0a74.png?v=0" title="s_\varepsilon ^2" alt="s_\varepsilon ^2" class="tex"/> (or error mean square, <img src="../images/Linear_Regression_Results/math-a9fc1a03386ae38b64e06c8172994963.png?v=0" title="MSE" alt="MSE" class="tex"/>) can be estimated as follows: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-63b692ed73b623d4f2f13eddcc9c7fb0.png?v=0" title="s_\varepsilon ^2=\frac{RSS}{df_{Error}}=\frac{\sum_{i=1}^n (y_i-\hat y_i)^2}{n^{*}-1}" alt="s_\varepsilon ^2=\frac{RSS}{df_{Error}}=\frac{\sum_{i=1}^n (y_i-\hat y_i)^2}{n^{*}-1}" class="tex"/>
</th>
<td>
<p>(18)  
</p>
</td></tr></table>
<p>And RSS means the residual sum of square (or error sum of square, SSE), which is actually the sum of the squares of the vertical deviations from each data point to the fitted line. It can be computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-60dcb5431cb468da75a936e22c6b4470.png?v=0" title="RSS=\sum_{i=1}^n e_i=\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\beta _0+\beta _1x_i)]^2" alt="RSS=\sum_{i=1}^n e_i=\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\beta _0+\beta _1x_i)]^2" class="tex"/>
</th>
<td>
<p>(19)  
</p>
</td></tr></table>
<table class="note">

<tr>
<td><b>Note</b>&#160;: Regarding <img src="../images/Linear_Regression_Results/math-be2982d2ce999da5c6b4601e87933c95.png?v=0" title="n*" alt="n*" class="tex"/>, if intercept is included in the model, <img src="../images/Linear_Regression_Results/math-31a8d4eab62ae5b74866f9e002f08ce5.png?v=0" title="n*=n-1" alt="n*=n-1" class="tex"/>. Otherwise, <img src="../images/Linear_Regression_Results/math-b53ed76ba17eb8f2704fb7b8c9e36d44.png?v=0" title="n*=n" alt="n*=n" class="tex"/>.
</td></tr></table>
<h4><a name="t-Value_and_Confidence_Level"></a><span class="mw-headline">t-Value and Confidence Level</span></h4>
<p>If the regression assumptions hold, we have: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-0ed681c4ca83831abc9e0afe2f959db2.png?v=0" title="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" class="tex"/>  and  <img src="../images/Linear_Regression_Results/math-b502ef25a978991781d791a9f6275a83.png?v=0" title="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" class="tex"/>
</th>
<td>
<p>(20)  
</p>
</td></tr></table>
<p>The <i>t</i>-test can be used to examine whether the fitting parameters are significantly different from zero, which means that we can test whether  <img src="../images/Linear_Regression_Results/math-ae198091d3f29f3101f5b0b86bf85b5d.png?v=0" title="\beta _0= 0\,\!" alt="\beta _0= 0\,\!" class="tex"/> (if true, this means that the fitted line passes through the origin) or <img src="../images/Linear_Regression_Results/math-c1cb3b8aa03a36c23962a83cf7d2dc40.png?v=0" title="\beta _1= 0\,\!" alt="\beta _1= 0\,\!" class="tex"/>. The hypotheses of the <i>t</i>-tests are: 
</p>
<dl><dd><img src="../images/Linear_Regression_Results/math-d15540fa2e02fa998eea73fd85bf1952.png?v=0" title="H_0&#160;: \beta _0= 0\,\! " alt="H_0&#160;: \beta _0= 0\,\! " class="tex"/>                  <img src="../images/Linear_Regression_Results/math-57dce98807a9a866ee09b5bbe88bb68e.png?v=0" title="H_0&#160;: \beta _1= 0\,\!" alt="H_0&#160;: \beta _1= 0\,\!" class="tex"/> </dd>
<dd><img src="../images/Linear_Regression_Results/math-00f883e098653776a99683aae70c3783.png?v=0" title="H_\alpha &#160;: \beta _0  \neq 0\,\!" alt="H_\alpha &#160;: \beta _0  \neq 0\,\!" class="tex"/>         <img src="../images/Linear_Regression_Results/math-27aac7aa6524d0b486a1890feffff3e1.png?v=0" title="H_\alpha &#160;: \beta _1 \neq  0\,\!" alt="H_\alpha &#160;: \beta _1 \neq  0\,\!" class="tex"/>     </dd></dl>
<p>The <i>t</i>-values can be computed by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-6368690db66f6b16f642c2976ba74d59.png?v=0" title="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" alt="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" class="tex"/>  and <img src="../images/Linear_Regression_Results/math-abcdef6859e44b55ed86ce261400d36c.png?v=0" title="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" alt="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" class="tex"/>
</th>
<td>
<p>(21)  
</p>
</td></tr></table>
<p>With the computed <i>t</i>-value, we can decide whether or not to reject the corresponding null hypothesis. Usually, for a given confidence level <img src="../images/Linear_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>   , we can reject <img src="../images/Linear_Regression_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> when <img src="../images/Linear_Regression_Results/math-9d38e8e7363d66af41530c3fbdf494f5.png?v=0" title="|t|&gt;t_{\frac \alpha 2}" alt="|t|&gt;t_{\frac \alpha 2}" class="tex"/>. Additionally, the <i>p</i>-value, or significance level, is reported with a <i>t</i>-test. We also reject the null hypothesis <img src="../images/Linear_Regression_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> if the <i>p</i>-value is less than <img src="../images/Linear_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>  .
</p>
<h4><a name="Prob.3E.7Ct.7C"></a><span class="mw-headline">Prob&gt;|t|</span></h4>
<p>The probability that <img src="../images/Linear_Regression_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png?v=0" title="H_0 \,\!" alt="H_0 \,\!" class="tex"/> in the <i>t</i> test above is true. 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-3e5335a131705b3ef0bb8b250033167d.png?v=0" title="prob=2(1-tcdf(|t|,df_{Error}))\,\!" alt="prob=2(1-tcdf(|t|,df_{Error}))\,\!" class="tex"/>
</th>
<td>
<p>(22)  
</p>
</td></tr></table>
<p>where <i>tcdf(t, df)</i> computes the lower tail probability for the Student's <i>t</i> distribution with <i>df</i> degree of freedom.
</p>
<h4><a name="LCL_and_UCL"></a><span class="mw-headline">LCL and UCL</span></h4>
<p>From the <i>t</i>-value, we can calculate the <img src="../images/Linear_Regression_Results/math-0cdaba532a6f8ad706e6b8e5e97f0d36.png?v=0" title="(1-\alpha )\times 100\%" alt="(1-\alpha )\times 100\%" class="tex"/>  <b>Confidence Interval</b> for each parameter by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-a27fc4294ee80efb51793fbcd4e23df2.png?v=0" title="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" alt="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" class="tex"/>
</th>
<td>
<p>(23)  
</p>
</td></tr></table>
<p>where <img src="../images/Linear_Regression_Results/math-fd9dfeda668ac9b0c5ef311ffdca8f71.png?v=0" title="UCL" alt="UCL" class="tex"/> and <img src="../images/Linear_Regression_Results/math-a4df64dc5e32716f6de0ec15b0340950.png?v=0" title="LCL" alt="LCL" class="tex"/> is short for the <b>Upper Confidence Interval</b> and <b>Lower Confidence Interval</b>, respectively.
</p>
<h4><a name="CI_Half_Width"></a><span class="mw-headline">CI Half Width</span></h4>
<p>The Confidence Interval Half Width is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-77ec7c61ac9d06bfc9ad4c0ae47b85ce.png?v=0" title="CI=\frac{UCL-LCL}2" alt="CI=\frac{UCL-LCL}2" class="tex"/>
</th>
<td>
<p>(24)  
</p>
</td></tr></table>
<p>where UCL and LCL is the <b>Upper Confidence Interval</b> and <b>Lower Confidence Interval</b>, respectively.
</p>
<h3><a name="Fit_Statistics"></a><span class="mw-headline">Fit Statistics</span></h3>
<p>Key linear fit statistics are summarized in the Statistics table (numbers in parentheses show how quantities are computed): 
</p>
<dl><dd><a  class="image"><img alt="FitStats.png" src="../images/Linear_Regression_Results/FitStats.png?v=63216" width="313"  /></a></dd></dl>
<h4><a name="Degrees_of_Freedom"></a><span class="mw-headline">Degrees of Freedom</span></h4>
<p>The Error degrees of freedom. Please refer to the <a href="../../UserGuide/UserGuide/Linear_Regression_Results.html#ANOVA_Table" title="UserGuide:Linear Regression Results">ANOVA</a> table for more details.
</p>
<h4><a name="Residual_Sum_of_Squares"></a><span class="mw-headline">Residual Sum of Squares</span></h4>
<p>The residual sum of squares, see formula (19).
</p>
<h4><a name="Reduced_Chi-Sqr"></a><span class="mw-headline">Reduced Chi-Sqr</span></h4>
<p>See formula (14)
</p>
<h4><a name="R-Square_.28COD.29"></a><span class="mw-headline">R-Square (COD)</span></h4>
<p>The quality of linear regression can be measured by the <b>coefficient of determination (COD)</b>, or <b><img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/></b>, which can be computed as: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-9ce15fecaa71e5021c67564b9f212551.png?v=0" title="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" alt="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" class="tex"/>
</th>
<td>
<p>(25)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_Regression_Results/math-700ffa56572a1641462ec2aadfb69b87.png?v=0" title="TSS=\sum(y_i-\bar{y})^2" alt="TSS=\sum(y_i-\bar{y})^2" class="tex"/>
</th></tr></table>
<p>where <i>TSS</i> is the total sum of square, and <i>RSS</i> is the residual sum of square. The <img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/> is a value between 0 and 1. Generally speaking, if it is close to 1, the relationship between X and Y will be regarded as very strong and we can have a high degree of confidence in our regression model.
</p>
<h4><a name="Adj._R-Square"></a><span class="mw-headline">Adj. R-Square</span></h4>
<p>We can further calculate the <b>adjusted <img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/></b> as 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-68a88ab44a828cde18104979d28917fe.png?v=0" title="{\bar R}^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" alt="{\bar R}^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" class="tex"/>
</th>
<td>
<p>(26)  
</p>
</td></tr></table>
<h4><a name="R_Value"></a><span class="mw-headline">R Value</span></h4>
<p>The <i>R</i> value is the square root of <i><img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png?v=0" title="R^2" alt="R^2" class="tex"/></i>: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-8dea6550a6f24458c884bb59c192c935.png?v=0" title="R=\sqrt{R^2}" alt="R=\sqrt{R^2}" class="tex"/>
</th>
<td>
<p>(27)  
</p>
</td></tr></table>
<h4><a name="Pearson.27s_r"></a><span class="mw-headline">Pearson's r</span></h4>
<p>In simple linear regression, the correlation coefficient between x and y, denoted by <i>r</i>, equals to: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-f7f1dc7cce20e9b9b19e47b012ee173d.png?v=0" title="r=R\,\!" alt="r=R\,\!" class="tex"/>   <i>if <img src="../images/Linear_Regression_Results/math-3a1e29703e364979554e6bd76761bed6.png?v=0" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"/> is positive</i>
</th>
<td>
<p>(28)  
</p>
</td></tr>
<tr>
<th><img src="../images/Linear_Regression_Results/math-556cec8e44138ef9cf002c70434cb55d.png?v=0" title="r=-R\,\!" alt="r=-R\,\!" class="tex"/>   <i>if <img src="../images/Linear_Regression_Results/math-3a1e29703e364979554e6bd76761bed6.png?v=0" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"/> is negative</i>
</th></tr></table>
<h4><a name="Root-MSE_.28SD.29"></a><span class="mw-headline">Root-MSE (SD)</span></h4>
<p>Root mean square of the error, or residual standard deviation, which equals to: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-5d65f33336e7ea1ead5adec051bf5572.png?v=0" title="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" alt="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" class="tex"/>
</th>
<td>
<p>(29)  
</p>
</td></tr></table>
<h4><a name="Norm_of_Residuals"></a><span class="mw-headline">Norm of Residuals</span></h4>
<p>Equals to square root of RSS: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-edb52b53a5f0e033c1872d55206bf72f.png?v=0" title="Norm \,of \,Residuals=\sqrt{RSS}" alt="Norm \,of \,Residuals=\sqrt{RSS}" class="tex"/>
</th>
<td>
<p>(30)  
</p>
</td></tr></table>
<h2><a name="ANOVA_Table"></a><span class="mw-headline">ANOVA Table</span></h2>
<p>The ANOVA table of linear fitting is:
</p>
<table class="simple">

<tr>
<th>
</th>
<th>DF
</th>
<th>Sum of Squares
</th>
<th>Mean Square
</th>
<th>F Value
</th>
<th>Prob &gt; F
</th></tr>
<tr>
<th>Model
</th>
<td><i>1</i>
</td>
<td><i><img src="../images/Linear_Regression_Results/math-76d28c12ecef51969ed6e4493c3934cf.png?v=0" title="SS_{reg} = TSS - RSS" alt="SS_{reg} = TSS - RSS" class="tex"/> </i>
</td>
<td><i><img src="../images/Linear_Regression_Results/math-e5dc62d3c446d80f3ce90dab19161a25.png?v=0" title="MS_{reg} = SS_{reg} / 1 " alt="MS_{reg} = SS_{reg} / 1 " class="tex"/></i>
</td>
<td><i><img src="../images/Linear_Regression_Results/math-d5ea59127172a534421d4b08fd6d3135.png?v=0" title="MS_{reg} / MSE " alt="MS_{reg} / MSE " class="tex"/></i>
</td>
<td><i>p-value </i>
</td></tr>
<tr>
<th>Error
</th>
<td><i>n* - 1</i>
</td>
<td><i>RSS</i>
</td>
<td><i>MSE = RSS / (n* - 1)</i>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<th>Total
</th>
<td><i>n*</i>
</td>
<td><i>TSS</i>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></table>
<table class="note">

<tr>
<td><b>Note</b>: If intercept is included in the model, <i>n*=n-1</i>. Otherwise, <i>n*=n</i> and the total sum of squares is uncorrected. If the slope is fixed, <i><img src="../images/Linear_Regression_Results/math-dfc3cbf5cc5bf940257cfcf8c86ddec0.png?v=0" title="df_{Model}" alt="df_{Model}" class="tex"/></i> = 0.
</td></tr></table>
<p>Where the total sum of square, TSS, is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-4d28d19e4d080da070b15388db314470.png?v=0" title="TSS =\sum_{i=1}^nw_i(y_i -\frac{\sum_{i=1}^n w_i y_i} {\sum_{i=1}^n w_i})^2" alt="TSS =\sum_{i=1}^nw_i(y_i -\frac{\sum_{i=1}^n w_i y_i} {\sum_{i=1}^n w_i})^2" class="tex"/>       <i>(corrected)</i>
</th>
<td>(31)
</td></tr>
<tr>
<th><img src="../images/Linear_Regression_Results/math-7c28116e7d91e4726fe4c0d26fa4e93e.png?v=0" title="TSS=\sum_{i=1}^n w_iy_i^2" alt="TSS=\sum_{i=1}^n w_iy_i^2" class="tex"/>  (uncorrected)
</th></tr></table>
<p>The F value here is a test of whether the fitting model differs significantly from the model y=constant.
</p><p>The <i>p</i>-value, or significance level, is reported with an <i>F</i>-test. If the <i>p</i>-value is less than <img src="../images/Linear_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png?v=0" title="\alpha\,\!" alt="\alpha\,\!" class="tex"/>, the fitting model differs significantly from the model y=constant.
</p><p>If fixing the intercept at a certain value, the p value for <i>F</i>-test is not meaningful, and it is different from that in linear regression without the intercept constraint.
</p>
<h2><a name="Lack_of_fit_table"></a><span class="mw-headline">Lack of fit table</span></h2>
<p>To run the lack of fit test, you need to have repeated observations, namely, "replicate data" , so that at least one of the X values is repeated within the dataset, or within multiple datasets when concatenate fit mode is selected.
</p><p>Notations used for fit with replicates data:
</p>
<table class="formula">

<tr>
<td><img src="../images/Linear_Regression_Results/math-9ec98a656b3500239d77f631b67811cc.png?v=0" title="y_{ij}" alt="y_{ij}" class="tex"/> is the jth measurement made at the ith x-value in the data set<br />
</td></tr>
<tr>
<td><img src="../images/Linear_Regression_Results/math-dbebef52b4445e3b11d20855d6e8d5dd.png?v=0" title="\bar{y}_{i}" alt="\bar{y}_{i}" class="tex"/> is the average of all of the y values at the ith x-value<br />
</td></tr>
<tr>
<td><img src="../images/Linear_Regression_Results/math-0ff3619d52350e78a9bb731ef2c23991.png?v=0" title="\hat{y}_{ij}" alt="\hat{y}_{ij}" class="tex"/> is the predicted response for the jth measurement made at the ith x-value<br />
</td></tr></table>
<p>The sum of square in table below is expressed by:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-5c76b3b72fc70ebfe8fed6e8daf6a7c7.png?v=0" title="RSS=\sum_{i}\sum_{j}(y_{ij}-\hat{y}_{ij})^2" alt="RSS=\sum_{i}\sum_{j}(y_{ij}-\hat{y}_{ij})^2" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_Regression_Results/math-3e215ad1665f7e78c1af881e4c6e4ac6.png?v=0" title="LFSS=\sum_{i}\sum_{j}(\bar{y}_{i}-\hat{y}_{ij})^2" alt="LFSS=\sum_{i}\sum_{j}(\bar{y}_{i}-\hat{y}_{ij})^2" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_Regression_Results/math-b8b02bc61db58ee0bcf8927677ab3c59.png?v=0" title="PESS=\sum_{i}\sum_{j}(y_{ij}-\bar{y}_{i})^2" alt="PESS=\sum_{i}\sum_{j}(y_{ij}-\bar{y}_{i})^2" class="tex"/>
</th></tr></table>
<p>The Lack of fit table of linear fitting is: 
</p>
<table class="simple">

<tr>
<th>
</th>
<th>DF
</th>
<th>Sum of Squares
</th>
<th>Mean Square
</th>
<th>F Value
</th>
<th>Prob &gt; F
</th></tr>
<tr>
<th>Lack of Fit
</th>
<td><i>c-2</i>
</td>
<td><i>LFSS </i>
</td>
<td><i>MSLF = LFSS / (c - 2) </i>
</td>
<td><i>MSLF / MSPE</i>
</td>
<td><i>p-value </i>
</td></tr>
<tr>
<th>Pure Error
</th>
<td><i>n - c</i>
</td>
<td><i>PESS</i>
</td>
<td><i>MSPE = PESS / (n - c)</i>
</td>
<td>
</td>
<td>
</td></tr>
<tr>
<th>Error
</th>
<td><i>n*-1</i>
</td>
<td><i>RSS</i>
</td>
<td>
</td>
<td>
</td>
<td>
</td></tr></table>
<table class="note">

<tr>
<td><b>Note</b>:<br />
<p>If intercept is included in the model, <i>n*=n-1</i>. Otherwise, <i>n*=n</i> and the total sum of squares is uncorrected. If the slope is fixed, <i><img src="../images/Linear_Regression_Results/math-dfc3cbf5cc5bf940257cfcf8c86ddec0.png?v=0" title="df_{Model}" alt="df_{Model}" class="tex"/></i> = 0.  
</p><p><i>c</i> denotes the number of distinct x values. If intercept is fixed, DF for Lack of Fit is c-1.
</p>
</td></tr></table>
<h2><a name="Covariance_and_Correlation_Matrix"></a><span class="mw-headline">Covariance and Correlation Matrix</span></h2>
<p>The Covariance matrix of linear regression is calculated by: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-b0bcaee4a98e9106fc977c7ab6cf5b79.png?v=0" title="&#10;\begin{pmatrix}&#10;Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\&#10;Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1)&#10;\end{pmatrix}=\sigma ^2\frac 1{SXX}\begin{pmatrix} \sum \frac{x_i^2}n &amp; -\bar x \\-\bar x &amp; 1 \end{pmatrix}" alt="&#10;\begin{pmatrix}&#10;Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\&#10;Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1)&#10;\end{pmatrix}=\sigma ^2\frac 1{SXX}\begin{pmatrix} \sum \frac{x_i^2}n &amp; -\bar x \\-\bar x &amp; 1 \end{pmatrix}" class="tex"/>
</th>
<td>
<p>(32)  
</p>
</td></tr></table>
<p>The correlation between any two parameters is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-b029662d99694045f6e48fab550e2b23.png?v=0" title="&#10;\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " alt="&#10;\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " class="tex"/>
</th>
<td>
<p>(33)  
</p>
</td></tr></table>
<h2><a name="Outliers"></a><span class="mw-headline">Outliers</span></h2>
<p>The Outliers are those points whose absolute values in Studentized Residual plot are larger than 2.
</p><p><img src="../images/Linear_Regression_Results/math-3efc2a4ea2a6baf34c2d5f2bf5bd4765.png?v=0" title="abs(Studentized Residual)&gt;2" alt="abs(Studentized Residual)&gt;2" class="tex"/>
</p><p>Studentized Residual is introduced in <a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Detecting_outliers_by_transforming_residuals" title="UserGuide:Graphic Residual Analysis">Detecting outliers by transforming residuals</a>.
</p>
<h2><a name="Residual_Analysis"></a><span class="mw-headline">Residual Analysis</span></h2>
<p><img src="../images/Linear_Regression_Results/math-af2593b49488f8bc44396795792c2d79.png?v=0" title="r_i" alt="r_i" class="tex"/> stands for the Regular Residual <img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/>.
</p>
<h3><a name="Standardized"></a><span class="mw-headline">Standardized</span></h3>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-edba44ad3ec53a729a081e552ffd8d52.png?v=0" title="r_i^{\prime }=\frac{r_i}s_\varepsilon" alt="r_i^{\prime }=\frac{r_i}s_\varepsilon" class="tex"/>
</th>
<td>
<p>(34)
</p>
</td></tr></table>
<h3><a name="Studentized"></a><span class="mw-headline">Studentized</span></h3>
<p>Also known as internally studentized residual.
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-5b4f1a55a60f41732cd45963fc4d7899.png?v=0" title="r_i^{\prime }=\frac{r_i}{s_\varepsilon\sqrt{1-h_i}}" alt="r_i^{\prime }=\frac{r_i}{s_\varepsilon\sqrt{1-h_i}}" class="tex"/>
</th>
<td>
<p>(35)
</p>
</td></tr></table>
<h3><a name="Studentized_deleted"></a><span class="mw-headline">Studentized deleted</span></h3>
<p>Also known as externally studentized residual.
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-33555ae8465559528419ee80ff636f91.png?v=0" title="r_i^{\prime }=\frac{r_i}{s_{\varepsilon-i}\sqrt{1-h_i}}" alt="r_i^{\prime }=\frac{r_i}{s_{\varepsilon-i}\sqrt{1-h_i}}" class="tex"/>
</th>
<td>
<p>(36)
</p>
</td></tr></table>
<p>In the equations for the <b>Studentized</b> and <b>Studentized deleted</b> residuals, <img src="../images/Linear_Regression_Results/math-57339b77b3af15427b7154f4daf8a223.png?v=0" title="h_i" alt="h_i" class="tex"/> is the <i>i</i>th diagonal element of the matrix <img src="../images/Linear_Regression_Results/math-44c29edb103a2872f519ad0c9a0fdaaa.png?v=0" title="P" alt="P" class="tex"/>:
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-609f90ce395d2346200e55f1a632bab0.png?v=0" title="P=X(X&#39;X)^{-1}X^{\prime }" alt="P=X(X&#39;X)^{-1}X^{\prime }" class="tex"/>
</th>
<td>
<p>(37)
</p>
</td></tr></table>
<p><img src="../images/Linear_Regression_Results/math-7af4042b185319dd95cc2dcb158a88b2.png?v=0" title="s_{\varepsilon-i}" alt="s_{\varepsilon-i}" class="tex"/> means the variance is calculated based on all points but exclude the <i>i</i>th.
</p>
<h2><a name="Confidence_and_Prediction_Bands"></a><span class="mw-headline">Confidence and Prediction Bands</span></h2>
<p>For a particular value <img src="../images/Linear_Regression_Results/math-333d67b49be014c08ced5a715a0aa659.png?v=0" title="x_p\,\!" alt="x_p\,\!" class="tex"/>, the <img src="../images/Linear_Regression_Results/math-7f5c0246ea96d63005bb89b2c2966a90.png?v=0" title="100(1-\alpha )\% " alt="100(1-\alpha )\% " class="tex"/> <b>confidence interval</b> for the mean value of <img src="../images/Linear_Regression_Results/math-bfb6488d6c250ac5aeed1bbf139baaa5.png?v=0" title="y\,\!" alt="y\,\!" class="tex"/> at <img src="../images/Linear_Regression_Results/math-29b229c7c531f688ffb25ceadae2a0f6.png?v=0" title="x=x_p\,\!" alt="x=x_p\,\!" class="tex"/> is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-2fc729f483df6668e6cfd645dcb324fb.png?v=0" title="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" alt="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" class="tex"/>
</th>
<td>
<p>(38)  
</p>
</td></tr></table>
<p>And the <img src="../images/Linear_Regression_Results/math-7f5c0246ea96d63005bb89b2c2966a90.png?v=0" title="100(1-\alpha )\% " alt="100(1-\alpha )\% " class="tex"/> <b>prediction interval</b> for the mean value of <img src="../images/Linear_Regression_Results/math-bfb6488d6c250ac5aeed1bbf139baaa5.png?v=0" title="y\,\!" alt="y\,\!" class="tex"/> at <img src="../images/Linear_Regression_Results/math-29b229c7c531f688ffb25ceadae2a0f6.png?v=0" title="x=x_p\,\!" alt="x=x_p\,\!" class="tex"/>is: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-6825d104f5fef27aeab6053f04b8892c.png?v=0" title="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{1+\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" alt="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{1+\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" class="tex"/>
</th>
<td>
<p>(39)  
</p>
</td></tr></table>
<h2><a name="Confidence_Ellipses"></a><span class="mw-headline">Confidence Ellipses</span></h2>
<p>Assuming the pair of variables (X, Y) conforms to a bivariate normal distribution, we can examine the correlation between the two variables using a confidence ellipse. The confidence ellipse is centered at (<img src="../images/Linear_Regression_Results/math-35b4d0362fcb2694d21b2abf35ccfc7f.png?v=0" title="\bar x" alt="\bar x" class="tex"/>,<img src="../images/Linear_Regression_Results/math-096544be8a676d2c106621014410cb9d.png?v=0" title="\bar y" alt="\bar y" class="tex"/> ), and the major semiaxis a and minor semiaxis <i>b</i> can be expressed as follow: 
</p>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-eda409b7a7a814287c66467c2beba53c.png?v=0" title=" a=c\sqrt{\frac{\sigma _x^2+\sigma _y^2+\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" alt=" a=c\sqrt{\frac{\sigma _x^2+\sigma _y^2+\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" class="tex"/>
</th></tr>
<tr>
<th><img src="../images/Linear_Regression_Results/math-8fc5aa9e1aad046be36d3bf32f61d9aa.png?v=0" title=" b=c\sqrt{\frac{\sigma _x^2+\sigma _y^2-\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" alt=" b=c\sqrt{\frac{\sigma _x^2+\sigma _y^2-\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" class="tex"/>
</th>
<td>
<p>(40)  
</p>
</td></tr></table>
<p>For a given confidence level of <img src="../images/Linear_Regression_Results/math-d7d1320edf83ed676aa5f848c9711fe9.png?v=0" title=" (1-\alpha )\,\! " alt=" (1-\alpha )\,\! " class="tex"/>:
</p>
<ul><li>The confidence ellipse for the population <b>mean</b> is defined as: </li></ul>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-97456e57ebc619b9ce5476c311cf3eef.png?v=0" title=" c=\sqrt{\frac{2(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " alt=" c=\sqrt{\frac{2(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " class="tex"/>
</th>
<td>
<p>(41)  
</p>
</td></tr></table>
<ul><li>The confidence ellipse for <b>prediction</b> is defined as: </li></ul>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-cbb5ed712a41f59f2cd27d2f0e29b096.png?v=0" title=" c=\sqrt{\frac{2(n+1)(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " alt=" c=\sqrt{\frac{2(n+1)(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " class="tex"/>
</th>
<td>
<p>(42)  
</p>
</td></tr></table>
<ul><li>The <b>inclination angle</b> of the ellipse is defined as: </li></ul>
<table class="formula">

<tr>
<th><img src="../images/Linear_Regression_Results/math-4c3d04a2e2be2ca32d3e4ae7960b3966.png?v=0" title="\beta =\frac 12\arctan \frac{2r\sqrt{\sigma _x^2\sigma _y^2}}{\sigma _x^2-\sigma _y^2}" alt="\beta =\frac 12\arctan \frac{2r\sqrt{\sigma _x^2\sigma _y^2}}{\sigma _x^2-\sigma _y^2}" class="tex"/>
</th>
<td>
<p>(43)  
</p>
</td></tr></table>
<h2><a name="Finding_Y.2FX_from_X.2FY"></a><span class="mw-headline"><a href="../../UserGuide/UserGuide/Finding_Y_X_from_X_Y_Standard_Curves.html" title="UserGuide:Finding Y X from X Y Standard Curves">Finding Y/X from X/Y</a></span></h2>
<h2><a name="Residual_Plots"></a><span class="mw-headline">Residual Plots</span></h2>
<h3><a name="Resudial_Type"></a><span class="mw-headline">Resudial Type</span></h3>
<p>Select one residual type among <b>Regular</b>, <b>Standardized</b>, <b>Studentized</b>, <b>Studentized Deleted</b> for Plots.
</p>
<h3><a name="Residual_vs._Independent"></a><span class="mw-headline">Residual vs. Independent</span></h3>
<p>Scatter plot of residual <img src="../images/Linear_Regression_Results/math-9b207167e5381c47682c6b4f58a623fb.png?v=0" title="res" alt="res" class="tex"/> vs. indenpendent variable <img src="../images/Linear_Regression_Results/math-75087036f44bb88958df97586ae3bd1e.png?v=0" title="x_1,x_2,\dots,x_k" alt="x_1,x_2,\dots,x_k" class="tex"/>, each plot is located in a seperate graphs.
</p>
<h3><a name="Residual_vs._Predicted_Value"></a><span class="mw-headline">Residual vs. Predicted Value</span></h3>
<p>Scatter plot of residual <img src="../images/Linear_Regression_Results/math-9b207167e5381c47682c6b4f58a623fb.png?v=0" title="res" alt="res" class="tex"/> vs. fitted results <img src="../images/Linear_Regression_Results/math-c9e53cbdffe795c0913cd927f13ffb9b.png?v=0" title="\hat{y_i}" alt="\hat{y_i}" class="tex"/>.
</p>
<h3><a name="Residual_vs._Order_of_the_Data"></a><span class="mw-headline">Residual vs. Order of the Data</span></h3>
<p><img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/> vs. sequence number <img src="../images/Linear_Regression_Results/math-865c0c0b4ab0e063e5caa3387c1a8741.png?v=0" title="i" alt="i" class="tex"/>
</p>
<h3><a name="Histogram_of_the_Residual"></a><span class="mw-headline">Histogram of the Residual</span></h3>
<p>The Histogram plot of the Residual
</p>
<h3><a name="Residual_Lag_Plot"></a><span class="mw-headline">Residual Lag Plot</span></h3>
<p>Residuals <img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png?v=0" title="res_i" alt="res_i" class="tex"/> vs. lagged residual <img src="../images/Linear_Regression_Results/math-00407b58eb35fecbfea6dc5d4413e493.png?v=0" title="res_{(i&#8211;1)}" alt="res_{(i&#8211;1)}" class="tex"/>.
</p>
<h3><a name="Normal_Probability_Plot_of_Residuals"></a><span class="mw-headline">Normal Probability Plot of Residuals</span></h3>
<p>A normal probability plot of the residuals can be used to check whether the variance is normally distributed as well. If the resulting plot is approximately linear, we proceed to assume that the error terms are normally distributed. The plot is based on the percentiles versus ordered residual, and the percentiles is estimated by 
</p><p><img src="../images/Linear_Regression_Results/math-ec995bff3922b2290f85bed243de5eb3.png?v=0" title="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" alt="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" class="tex"/> 
</p><p>where <i>n</i> is the total number of dataset and  <i>i</i> is the <i>i</i> th data. Also refer to <a href="../../UserGuide/UserGuide/Probability_Plot_and_Q-Q_Plot.html" title="UserGuide:Probability Plot and Q-Q Plot">Probability Plot and Q-Q Plot</a>
</p>





