<h1 class="firstHeading">Algorithmus (Lineare Regression)</h1>

  <p class='urlname' style='display: none'>LR-Algorithm</p>

  <div class="toclimit-3">
    <div id="toc" class="toc">
      <div id="toctitle">
        <h2>Inhalt</h2>
      </div>

      <ul>
        <li class="toclevel-1 tocsection-1">
          <a href="#The_Linear_Regression_Model"><span class="tocnumber">1</span> <span class="toctext">Das Modell der linearen Regression</span></a>

          <ul>
            <li class="toclevel-2 tocsection-2"><a href="#Simple_Linear_Regression_Model"><span class="tocnumber">1.1</span> <span class="toctext">Einfaches lineares Regressionsmodell</span></a></li>
          </ul>
        </li>

        <li class="toclevel-1 tocsection-3">
          <a href="#Fit_Control"><span class="tocnumber">2</span> <span class="toctext">Fit-Steuerung</span></a>

          <ul>
            <li class="toclevel-2 tocsection-4">
              <a href="#Errors_as_Weight"><span class="tocnumber">2.1</span> <span class="toctext">Fehler als Gewichtung</span></a>

              <ul>
                <li class="toclevel-3 tocsection-5"><a href="#No_Weighting"><span class="tocnumber">2.1.1</span> <span class="toctext">Keine Gewichtung</span></a></li>

                <li class="toclevel-3 tocsection-6"><a href="#Direct_Weighting"><span class="tocnumber">2.1.2</span> <span class="toctext">Direkte Gewichtung</span></a></li>

                <li class="toclevel-3 tocsection-7"><a href="#Instrumental"><span class="tocnumber">2.1.3</span> <span class="toctext">Instrumental</span></a></li>
              </ul>
            </li>

            <li class="toclevel-2 tocsection-8"><a href="#Fix_Intercept_.28at.29"><span class="tocnumber">2.2</span> <span class="toctext">Schnittpunkt mit der Y-Achse festlegen (bei)</span></a></li>

            <li class="toclevel-2 tocsection-9"><a href="#Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"><span class="tocnumber">2.3</span> <span class="toctext">Skalierungsfehler mit Quadrat (Reduziertes Chi-Qdr.)</span></a></li>
          </ul>
        </li>

        <li class="toclevel-1 tocsection-10">
          <a href="#Fit_Results"><span class="tocnumber">3</span> <span class="toctext">Fit-Ergebnisse</span></a>

          <ul>
            <li class="toclevel-2 tocsection-11">
              <a href="#Fit_Parameters"><span class="tocnumber">3.1</span> <span class="toctext">Fit-Parameter</span></a>

              <ul>
                <li class="toclevel-3 tocsection-12"><a href="#Fitted_value"><span class="tocnumber">3.1.1</span> <span class="toctext">Angepasster Werte</span></a></li>

                <li class="toclevel-3 tocsection-13"><a href="#The_Parameter_Standard_Errors"><span class="tocnumber">3.1.2</span> <span class="toctext">Die Parameterstandardfehler</span></a></li>

                <li class="toclevel-3 tocsection-14"><a href="#t-Value_and_Confidence_Level"><span class="tocnumber">3.1.3</span> <span class="toctext">t-Wert und Konfidenzniveau</span></a></li>

                <li class="toclevel-3 tocsection-15"><a href="#Prob.3E.7Ct.7C"><span class="tocnumber">3.1.4</span> <span class="toctext">Wahrsch.&gt;|t|</span></a></li>

                <li class="toclevel-3 tocsection-16"><a href="#LCL_and_UCL"><span class="tocnumber">3.1.5</span> <span class="toctext">UEG und OEG</span></a></li>

                <li class="toclevel-3 tocsection-17"><a href="#CI_Half_Width"><span class="tocnumber">3.1.6</span> <span class="toctext">KI Halbe Breite</span></a></li>
              </ul>
            </li>

            <li class="toclevel-2 tocsection-18">
              <a href="#Fit_Statistics"><span class="tocnumber">3.2</span> <span class="toctext">Fit-Statistik</span></a>

              <ul>
                <li class="toclevel-3 tocsection-19"><a href="#Degrees_of_Freedom"><span class="tocnumber">3.2.1</span> <span class="toctext">Freiheitsgrade</span></a></li>

                <li class="toclevel-3 tocsection-20"><a href="#Residual_Sum_of_Squares"><span class="tocnumber">3.2.2</span> <span class="toctext">Residuensumme der Quadrate</span></a></li>

                <li class="toclevel-3 tocsection-21"><a href="#Reduced_Chi-Sqr"><span class="tocnumber">3.2.3</span> <span class="toctext">Reduziertes Chi-Quadrat</span></a></li>

                <li class="toclevel-3 tocsection-22"><a href="#R-Square_.28COD.29"><span class="tocnumber">3.2.4</span> <span class="toctext">R-Quadrat (COD)</span></a></li>

                <li class="toclevel-3 tocsection-23"><a href="#Adj._R-Square"><span class="tocnumber">3.2.5</span> <span class="toctext">Korr. R-Quadrat</span></a></li>

                <li class="toclevel-3 tocsection-24"><a href="#R_Value"><span class="tocnumber">3.2.6</span> <span class="toctext">R-Wert</span></a></li>

                <li class="toclevel-3 tocsection-25"><a href="#Pearson.27s_r"><span class="tocnumber">3.2.7</span> <span class="toctext">Pearsons r</span></a></li>

                <li class="toclevel-3 tocsection-26"><a href="#Root-MSE_.28SD.29"><span class="tocnumber">3.2.8</span> <span class="toctext">Wurzel-MSE (StAbw)</span></a></li>

                <li class="toclevel-3 tocsection-27"><a href="#Norm_of_Residuals"><span class="tocnumber">3.2.9</span> <span class="toctext">Norm der Residuen</span></a></li>
              </ul>
            </li>
          </ul>
        </li>

        <li class="toclevel-1 tocsection-28"><a href="#ANOVA_Table"><span class="tocnumber">4</span> <span class="toctext">ANOVA-Tabelle</span></a></li>

        <li class="toclevel-1 tocsection-29"><a href="#Lack_of_fit_table"><span class="tocnumber">5</span> <span class="toctext">Tabelle des Tests auf fehlende Anpassung</span></a></li>

        <li class="toclevel-1 tocsection-30"><a href="#Covariance_and_Correlation_Matrix"><span class="tocnumber">6</span> <span class="toctext">Kovarianz- und Korrelationsmatrix</span></a></li>

        <li class="toclevel-1 tocsection-31"><a href="#Outliers"><span class="tocnumber">7</span> <span class="toctext">Ausreißer</span></a></li>

        <li class="toclevel-1 tocsection-32">
          <a href="#Residual_Analysis"><span class="tocnumber">8</span> <span class="toctext">Residuenanalyse</span></a>

          <ul>
            <li class="toclevel-2 tocsection-33"><a href="#Standardized"><span class="tocnumber">8.1</span> <span class="toctext">Standardisiert</span></a></li>

            <li class="toclevel-2 tocsection-34"><a href="#Studentized"><span class="tocnumber">8.2</span> <span class="toctext">Studentisiert</span></a></li>

            <li class="toclevel-2 tocsection-35"><a href="#Studentized_deleted"><span class="tocnumber">8.3</span> <span class="toctext">Studentisiert gelöscht</span></a></li>
          </ul>
        </li>

        <li class="toclevel-1 tocsection-36"><a href="#Confidence_and_Prediction_Bands"><span class="tocnumber">9</span> <span class="toctext">Konfidenz- und Prognosebänder</span></a></li>

        <li class="toclevel-1 tocsection-37"><a href="#Confidence_Ellipses"><span class="tocnumber">10</span> <span class="toctext">Konfidenzellipsen</span></a></li>

        <li class="toclevel-1 tocsection-38"><a href="#Finding_Y.2FX_from_X.2FY"><span class="tocnumber">11</span> <span class="toctext">Y von X finden/X von Y finden</span></a></li>

        <li class="toclevel-1 tocsection-39">
          <a href="#Residual_Plots"><span class="tocnumber">12</span> <span class="toctext">Residuendiagramme</span></a>

          <ul>
            <li class="toclevel-2 tocsection-40"><a href="#Resudial_Type"><span class="tocnumber">12.1</span> <span class="toctext">Residuentyp</span></a></li>

            <li class="toclevel-2 tocsection-41"><a href="#Residual_vs._Independent"><span class="tocnumber">12.2</span> <span class="toctext">Residuen vs. Unabhängig</span></a></li>

            <li class="toclevel-2 tocsection-42"><a href="#Residual_vs._Predicted_Value"><span class="tocnumber">12.3</span> <span class="toctext">Residuen vs. Diagramm der prognostizierten Werte</span></a></li>

            <li class="toclevel-2 tocsection-43"><a href="#Residual_vs._Order_of_the_Data"><span class="tocnumber">12.4</span> <span class="toctext">Residuen vs. die Ordnung der Datendiagramme</span></a></li>

            <li class="toclevel-2 tocsection-44"><a href="#Histogram_of_the_Residual"><span class="tocnumber">12.5</span> <span class="toctext">Histogramm des Residuendiagramms</span></a></li>

            <li class="toclevel-2 tocsection-45"><a href="#Residual_Lag_Plot"><span class="tocnumber">12.6</span> <span class="toctext">Verzögerte Residuendiagramme</span></a></li>

            <li class="toclevel-2 tocsection-46"><a href="#Normal_Probability_Plot_of_Residuals"><span class="tocnumber">12.7</span> <span class="toctext">Wahrscheinlichkeitsnetz (Normal) für Residuen</span></a></li>
          </ul>
        </li>
      </ul>
    </div>
  </div>

  <h2><a name="The_Linear_Regression_Model"></a><span class="mw-headline">Das Modell der linearen Regression</span></h2>

  <h3><a name="Simple_Linear_Regression_Model"></a><span class="mw-headline">Einfaches lineares Regressionsmodell</span></h3>

  <p>Für einen gegebenen Datensatz <img src="../images/Linear_Regression_Results/math-f243bd7bc9626358c4cbdace83fad108.png" title="(x_i,y_i),i=1,2,\ldots n" alt="(x_i,y_i),i=1,2,\ldots n" class="tex"> -- wobei X die unabhängige Variable und Y die abhängige Variable ist, <img src="../images/Linear_Regression_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png" title="\beta_0" alt="\beta_0" class="tex"> und <img src="../images/Linear_Regression_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png" title="\beta_1" alt="\beta_1" class="tex"> die Parameter sind, <img src="../images/Linear_Regression_Results/math-3ac22ebe353c690d089056a1a61e884d.png" title="\varepsilon_i" alt="\varepsilon_i" class="tex"> ein Zufallsfehlerterm mit Mittelwert <img src="../images/Linear_Regression_Results/math-e08f05e02ca7379fa770d67c7f995ec4.png" title="E\left \{\varepsilon_i\right \}=0" alt="E\left \{\varepsilon_i\right \}=0" class="tex"> ist und die Varianz <img src="../images/Linear_Regression_Results/math-57e78781f84e3e3175bcc12c95318cfb.png" title="Var\left \{\varepsilon_i\right \}=\sigma^2" alt="Var\left \{\varepsilon_i\right \}=\sigma^2" class="tex"> -- passt die lineare Regression die Daten an ein Modell der folgenden Form an:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-e7275a1e9849b30e085befefdcf649e4.png" title="y_i=\beta _0+\beta _1x_i+\varepsilon_i" alt="y_i=\beta _0+\beta _1x_i+\varepsilon_i" class="tex"></th>

      <td>
        <p>(1)</p>
      </td>
    </tr>
  </table>

  <p>Die Schätzung der kleinsten Quadrate wird verwendet, um die Summe von <i>n</i> quadrierten Abweichungen zu minimieren.</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-88e33546e2dca0c68fbd64d62f733914.png" title="\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2" alt="\sum_{i=1}^{n}(Y_i-\beta_0-\beta_1X_i)^2" class="tex"></th>

      <td>
        <p>(2)</p>
      </td>
    </tr>
  </table>

  <p>Die geschätzten Parameter des linearen Modells können folgendermaßen berechnet werden:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-befdde5b640aa0a77538792c238ed5cd.png" title="\hat\beta _1=\frac{SXY}{SXX}" alt="\hat\beta _1=\frac{SXY}{SXX}" class="tex"></th>

      <td>
        <p>(3)</p>
      </td>
    </tr>
  </table>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-fb3f1891e62df179b758521b10084aa6.png" title="\hat\beta _0=\bar y-\hat\beta _1\bar x " alt="\hat\beta _0=\bar y-\hat\beta _1\bar x " class="tex"></th>

      <td>
        <p>(4)</p>
      </td>
    </tr>
  </table>

  <p>wobei:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-322527b1ed6496cd6cdf25a0002dfc7f.png" title="\bar x=\frac {1}{n}\sum_{i=1}^nx_i" alt="\bar x=\frac {1}{n}\sum_{i=1}^nx_i" class="tex">,<img src="../images/Linear_Regression_Results/math-595a61adb4402eacacded49c4e4bbf1a.png" title="\bar y=\frac {1}{n}\sum_{i=1}^ny_i" alt="\bar y=\frac {1}{n}\sum_{i=1}^ny_i" class="tex"></th>

      <td>
        <p>(5)</p>
      </td>
    </tr>
  </table>

  <p>und</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-475a6b18f39e0bc196ab8aeee961b0f0.png" title="SXY=\sum_{i=1}^nx_iy_i\; \; \; \; \; \; \; SXX=\sum_{i=1}^nx_i^2" alt="SXY=\sum_{i=1}^nx_iy_i\; \; \; \; \; \; \; SXX=\sum_{i=1}^nx_i^2" class="tex"> (korrigiert)</th>

      <td>
        <p>(6)</p>
      </td>
    </tr>
  </table>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-2b50c9938d5981cb89ffaec033abab1c.png" title="SXY=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)\; \; \; \; \; \; \; SXX=\sum_{i=1}^n(x_i-\bar x)^2" alt="SXY=\sum_{i=1}^n(x_i-\bar x)(y_i-\bar y)\; \; \; \; \; \; \; SXX=\sum_{i=1}^n(x_i-\bar x)^2" class="tex"> (unkorrigiert)</th>

      <td>
        <p>(7)</p>
      </td>
    </tr>
  </table>

  <table class="note">
    <tr>
      <td><b>Hinweis:</b> Wenn der Schnittpunkt vom Modell ausgeschlossen ist, werden die Koeffizienten mit der <b>unkorrigierten</b> Formel berechnet.</td>
    </tr>
  </table>

  <p>Daher schätzen wir die Regressionsfunktion folgendermaßen:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-4629c75f122b9d4b551eb493d45a295e.png" title="\hat{y}=\hat{\beta_0}+\hat{\beta_1}x" alt="\hat{y}=\hat{\beta_0}+\hat{\beta_1}x" class="tex"></th>

      <td>
        <p>(8)</p>
      </td>
    </tr>
  </table>

  <p>Das Residuum <img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png" title="res_i" alt="res_i" class="tex"> ist definiert als:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-1eccdc0d4d12d0c892775e7682366d77.png" title="res_i=y_i-\hat{y_i}" alt="res_i=y_i-\hat{y_i}" class="tex"></th>

      <td>
        <p>(9)</p>
      </td>
    </tr>
  </table>

  <p>Die Formel in (2) muss so minimiert werden, dass sie gleich der Summe der Fehlerquadrate ist.</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-3b4c813cc6108fdfb9d72a3346e2cd15.png" title="RSS=\sum_{i=1}^nres_i^2" alt="RSS=\sum_{i=1}^nres_i^2" class="tex"></th>

      <td>
        <p>(10)</p>
      </td>
    </tr>
  </table>

  <p>wenn die Schätzung der kleinsten Quadrate <img src="../images/Linear_Regression_Results/math-9dabc344cb060be9355c54cc39a038db.png" title="\hat{\beta_0}" alt="\hat{\beta_0}" class="tex"> und <img src="../images/Linear_Regression_Results/math-4db6565d3866d2baeaf7a66f389744f9.png" title="\hat{\beta_1}" alt="\hat{\beta_1}" class="tex"> zum Schätzen von <img src="../images/Linear_Regression_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png" title="\beta_0" alt="\beta_0" class="tex"> und <img src="../images/Linear_Regression_Results/math-b4ceec2c4656f5c1e7fc76c59c4f80f3.png" title="\beta_1" alt="\beta_1" class="tex"> verwendet werden.</p>

  <h2><a name="Fit_Control"></a><span class="mw-headline">Fit-Steuerung</span></h2>

  <h3><a name="Errors_as_Weight"></a><span class="mw-headline">Fehler als Gewichtung</span></h3>

  <p>Im obigen Abschnitt wird angenommen, dass es eine konstante Varianz in den Fehlern gibt. Wenn wir jedoch die Versuchsdaten anpassen, müssen wir vielleicht den Fehler des Instruments im Anpassungsprozess berücksichtigen (der die Genauigkeit und Präzision eines Messinstruments wiedergibt). Daher wird die Annahme der konstanten Varianz in den Fehlern verletzt. Wir müssen annehmen, dass <img src="../images/Linear_Regression_Results/math-3ac22ebe353c690d089056a1a61e884d.png" title="\varepsilon_i" alt="\varepsilon_i" class="tex"> normalverteilt ist mit einer nicht konstanten Varianz und die Fehler als <img src="../images/Linear_Regression_Results/math-10e16c6a764d367ca5077a54bf156f7e.png" title="\sigma^2" alt="\sigma^2" class="tex"> agieren, was als Gewichtung bei der Anpassung verwendet werden kann. Die Gewichtung wird definiert als:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-8845a41888129439e7f1b3fbdaeb2078.png" title="W=\begin{bmatrix} w_1&amp; 0 &amp; \dots &amp;0 \\ 0 &amp; w_2 &amp; \dots &amp;0 \\ \vdots&amp; \vdots &amp;\ \ddots &amp;\vdots \\ 0&amp; 0 &amp;\dots &amp; w_n \end{bmatrix}" alt="W=\begin{bmatrix} w_1&amp; 0 &amp; \dots &amp;0 \\ 0 &amp; w_2 &amp; \dots &amp;0 \\ \vdots&amp; \vdots &amp;\ \ddots &amp;\vdots \\ 0&amp; 0 &amp;\dots &amp; w_n \end{bmatrix}" class="tex"></th>

      <td></td>
    </tr>
  </table>

  <p>Das Anpassungsmodell wird wie folgt geändert:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-b91af30efe2f541053551a9080cdf9de.png" title="\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\hat{\beta _0}+\hat{\beta _1}x_i)]^2" alt="\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\hat{\beta _0}+\hat{\beta _1}x_i)]^2" class="tex"></th>

      <td>
        <p>(11)</p>
      </td>
    </tr>
  </table>

  <p>Die Gewichtungsfaktoren <img src="../images/Linear_Regression_Results/math-aa38f107289d4d73d516190581397349.png" title="w_i" alt="w_i" class="tex"> können durch drei Formeln gegeben sein:</p>

  <h4><a name="No_Weighting"></a><span class="mw-headline">Keine Gewichtung</span></h4>

  <p>Der Fehlerbalken wird in der Berechnung nicht als Gewichtung behandelt.</p>

  <h4><a name="Direct_Weighting"></a><span class="mw-headline">Direkte Gewichtung</span></h4>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-05c143b7a8ecc59d8edb905c4b6e3397.png" title="w_i=\sigma_i " alt="w_i=\sigma_i " class="tex"></th>

      <td>
        <p>(12)</p>
      </td>
    </tr>
  </table>

  <h4><a name="Instrumental"></a><span class="mw-headline">Instrumental</span></h4>

  <p>Der Wert der instrumentellen Gewichtung ist antiproportional zu Instrumentenfehlern, so dass ein Versuch mit kleinen Fehlern eine große Gewichtung haben wird, da er im Vergleich zu Versuchen mit größeren Fehlern präziser ist.</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-63f1838aba2791cf68521fe43a435763.png" title="w_i=\frac 1{\sigma_i^2}" alt="w_i=\frac 1{\sigma_i^2}" class="tex"></th>

      <td>
        <p>(13)</p>
      </td>
    </tr>
  </table>

  <table class="note">
    <tr>
      <td><b>Hinweis:</b> Die Fehler als Gewichtung sollten der Spalte "Y-Fehler" im Arbeitsblatt zugewiesen werden.</td>

      <td></td>
    </tr>
  </table>

  <h3><a name="Fix_Intercept_.28at.29"></a><span class="mw-headline">Fester Schnittpunkt mit der Y-Achse (bei)</span></h3>

  <p>Fester Schnittpunkt mit der Y-Achse legt den Y-Schnittpunkt <img src="../images/Linear_Regression_Results/math-5af9e28d609b16eb25693f44ea9d7a8f.png" title="\beta_0" alt="\beta_0" class="tex"> auf einen festen Wert fest, während der Gesamtfreiheitsgrad n*=n-1 ist aufgrund des festgelegten Schnittpunkts mit der Y-Achse.</p>

  <h3><a name="Scale_Error_with_sqrt.28Reduced_Chi-Sqr.29"></a><span class="mw-headline">Skalierungsfehler mit Quadrat (Reduziertes Chi-Quadrat)</span></h3>

  <p>Die Option <b>Skalierungsfehler mit Quadrat (Reduziertes Chi-Qdr.)</b> ist verfügbar, wenn mit Gewichtung angepasst wird. Diese Option beeinflusst nur den Fehler auf die Parameter, die der Anpassungsprozess meldet, und nicht den Anpassungsprozess selbst oder die Daten in irgendeiner Weise. Die Option ist standardmäßig aktiviert, und <img src="../images/Linear_Regression_Results/math-10e16c6a764d367ca5077a54bf156f7e.png" title="\sigma^2" alt="\sigma^2" class="tex"> wird zum Berechnen der Fehler auf die Parameter berücksichtigt. Ansonsten wird die Varianz von <img src="../images/Linear_Regression_Results/math-10e16c6a764d367ca5077a54bf156f7e.png" title="\sigma^2" alt="\sigma^2" class="tex"> nicht zur Fehlerberechnung berücksichtigt. Nehmen Sie die Kovarianzmatrix als ein Beispiel: Skalierungsfehler mit Quadrat (Reduziertes Chi-Qdr.) verwenden:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-a219511b6fc22ebe963c31f36a1b4dd7.png" title="Cov(\beta _i,\beta _j)=\sigma^2 (X^{\prime }X)^{-1}" alt="Cov(\beta _i,\beta _j)=\sigma^2 (X^{\prime }X)^{-1}" class="tex"></th>
    </tr>

    <tr>
      <th><img src="../images/Linear_Regression_Results/math-aa0f6ec7a20808737c0ce43c3cba8892.png" title="\sigma^2=\frac{RSS}{n^{*}-1}" alt="\sigma^2=\frac{RSS}{n^{*}-1}" class="tex"></th>

      <td>
        <p>(14)</p>
      </td>
    </tr>
  </table>

  <p>Keinen Skalierungsfehler mit Quadrat (Reduziertes Chi-Qdr.) verwenden:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-d654da511627b08f8bb2817849d7a889.png" title="Cov(\beta _i,\beta _j)=(X'X)^{-1}\,\!" alt="Cov(\beta _i,\beta _j)=(X'X)^{-1}\,\!" class="tex"></th>

      <td>
        <p>(15)</p>
      </td>
    </tr>
  </table>

  <p>Für die gewichtete Anpassung wird <img src="../images/Linear_Regression_Results/math-6f369f83036bd2eab887744dd10a6eb7.png" title="(X'WX)^{-1}\,\!" alt="(X'WX)^{-1}\,\!" class="tex"> anstatt <img src="../images/Linear_Regression_Results/math-8f5640d983fa3d67b37e5356b6a62a78.png" title="(X'X)^{-1}\,\!" alt="(X'X)^{-1}\,\!" class="tex"> verwendet.</p>

  <h2><a name="Fit_Results"></a><span class="mw-headline">Fit-Ergebnisse</span></h2>

  <p>Wenn Sie eine lineare Anpassung durchführen, erstellen Sie ein <a class="external text" href="../../UserGuide/UserGuide/Analysis_Report_Sheets_and_Columns.html">Analyseberichtsblatt</a>, dass die berechneten Eigenschaften enthält. Die Tabellenberichte Parameter modellieren Steigung und Schnittpunkt mit der Y-Achse (Zahlen in Klammern zeigen, wie die Eigenschaften abgeleitet werden):</p>

  <h3><a name="Fit_Parameters"></a><span class="mw-headline">Fit-Parameter</span></h3>

  <p><a class="image"><img alt="Fitted-paramater.png" src="../images/Linear_Regression_Results/Fitted-paramater.png" width="655"></a></p>

  <h4><a name="Fitted_value"></a><span class="mw-headline">Angepasster Wert</span></h4>

  <p>Siehe Formel (3)&amp;(4).</p>

  <h4><a name="The_Parameter_Standard_Errors"></a><span class="mw-headline">Die Parameterstandardfehler</span></h4>

  <p>Für jeden Parameter kann der Standardfehler, wie folgt, ermittelt werden:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-a2e4c535ba2702d3744f6a49bfa4ac1c.png" title="\varepsilon _{\hat \beta _0}=s_\varepsilon \sqrt{\frac{\sum x_i^2}{nSXX}}" alt="\varepsilon _{\hat \beta _0}=s_\varepsilon \sqrt{\frac{\sum x_i^2}{nSXX}}" class="tex"></th>

      <td>
        <p>(16)</p>
      </td>
    </tr>
  </table>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-01aff625e5ad40105a12347ba05339bc.png" title="\varepsilon _{\hat \beta _1}=\frac{s_\varepsilon }{\sqrt{SXX}}" alt="\varepsilon _{\hat \beta _1}=\frac{s_\varepsilon }{\sqrt{SXX}}" class="tex"></th>

      <td>
        <p>(17)</p>
      </td>
    </tr>
  </table>

  <p>wobei die Beispielvarianz <img src="../images/Linear_Regression_Results/math-a9fc1a03386ae38b64e06c8172994963.png" title="MSE" alt="MSE" class="tex"> (oder Quadrat des Mittelwertfehlers <img src="../images/Linear_Regression_Results/math-a9fc1a03386ae38b64e06c8172994963.png" title="MSE" alt="MSE" class="tex">) folgendermaßen geschätzt werden kann:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-63b692ed73b623d4f2f13eddcc9c7fb0.png" title="s_\varepsilon ^2=\frac{RSS}{df_{Error}}=\frac{\sum_{i=1}^n (y_i-\hat y_i)^2}{n^{*}-1}" alt="s_\varepsilon ^2=\frac{RSS}{df_{Error}}=\frac{\sum_{i=1}^n (y_i-\hat y_i)^2}{n^{*}-1}" class="tex"></th>

      <td>
        <p>(18)</p>
      </td>
    </tr>
  </table>

  <p>RSS steht für die Residuensumme des Quadrats (oder Fehlersumme des Quadrats, SSE), die tatsächlich die Summe der Quadrate der vertikalen Abweichungen von jedem Datenpunkt aus zur angepassten Linie darstellt. Es kann wie folgt berechnet werden:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-60dcb5431cb468da75a936e22c6b4470.png" title="RSS=\sum_{i=1}^n e_i=\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\beta _0+\beta _1x_i)]^2" alt="RSS=\sum_{i=1}^n e_i=\sum_{i=1}^n w_i (y_i-\hat y_i)^2=\sum_{i=1}^n w_i [y_i-(\beta _0+\beta _1x_i)]^2" class="tex"></th>

      <td>
        <p>(19)</p>
      </td>
    </tr>
  </table>

  <table class="note">
    <tr>
      <td><b>Hinweis:</b> Im Bezug auf <img src="../images/Linear_Regression_Results/math-be2982d2ce999da5c6b4601e87933c95.png" title="n*" alt="n*" class="tex">, wenn der Schnittpunkt mit der Y-Achse in dem Modell enthalten ist, ist <img src="../images/Linear_Regression_Results/math-31a8d4eab62ae5b74866f9e002f08ce5.png" title="n*=n-1" alt="n*=n-1" class="tex">. Ansonsten <img src="../images/Linear_Regression_Results/math-b53ed76ba17eb8f2704fb7b8c9e36d44.png" title="n*=n" alt="n*=n" class="tex">.</td>
    </tr>
  </table>

  <h4><a name="t-Value_and_Confidence_Level"></a><span class="mw-headline">t-Wert und Konfidenzniveau</span></h4>

  <p>Gelten die Regressionsannahmen, haben wir:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-0ed681c4ca83831abc9e0afe2f959db2.png" title="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _0}-\beta _0}{\varepsilon _{\hat \beta _0}}\sim t_{n^{*}-1}" class="tex"> und <img src="../images/Linear_Regression_Results/math-b502ef25a978991781d791a9f6275a83.png" title="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" alt="\frac{{\hat \beta _1}-\beta _1}{\varepsilon _{\hat \beta _1}}\sim t_{n^{*}-1}" class="tex"></th>

      <td>
        <p>(20)</p>
      </td>
    </tr>
  </table>

  <p>Die <i>t-</i>Tests können verwendet werden, um zu untersuchen, ob die Fit-Parameter signifikant von Null abweichen. Das bedeutet, wir können testen, ob <img src="../images/Linear_Regression_Results/math-ae198091d3f29f3101f5b0b86bf85b5d.png" title="\beta _0= 0\,\!" alt="\beta _0= 0\,\!" class="tex"> (falls wahr, bedeutet dies, dass die angepasste Linie durch den Ursprung verläuft) oder <img src="../images/Linear_Regression_Results/math-c1cb3b8aa03a36c23962a83cf7d2dc40.png" title="\beta _1= 0\,\!" alt="\beta _1= 0\,\!" class="tex">. Die Hypothesen der <i>t</i>-Tests sind:</p>

  <dl>
    <dd><img src="../images/Linear_Regression_Results/math-d15540fa2e02fa998eea73fd85bf1952.png" title="H_0: \beta _0= 0\,\! " alt="H_0: \beta _0= 0\,\! " class="tex"> <img src="../images/Linear_Regression_Results/math-57dce98807a9a866ee09b5bbe88bb68e.png" title="H_0: \beta _1= 0\,\!" alt="H_0: \beta _1= 0\,\!" class="tex"></dd>

    <dd><img src="../images/Linear_Regression_Results/math-00f883e098653776a99683aae70c3783.png" title="H_\alpha: \beta _0 \neq 0\,\!" alt="H_\alpha: \beta _0 \neq 0\,\!" class="tex"> <img src="../images/Linear_Regression_Results/math-27aac7aa6524d0b486a1890feffff3e1.png" title="H_\alpha: \beta _1 \neq 0\,\!" alt="H_\alpha: \beta _1 \neq 0\,\!" class="tex"></dd>
  </dl>

  <p>Die <i>t</i>-Werte können wie folgt berechnet werden:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-6368690db66f6b16f642c2976ba74d59.png" title="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" alt="t_{\hat \beta _0}=\frac{{\hat \beta _0}-0}{\varepsilon _{\hat \beta _0}}" class="tex"> und <img src="../images/Linear_Regression_Results/math-abcdef6859e44b55ed86ce261400d36c.png" title="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" alt="t_{\hat \beta _1}=\frac{{\hat \beta _1}-0}{\varepsilon _{\hat \beta _1}}" class="tex"></th>

      <td>
        <p>(21)</p>
      </td>
    </tr>
  </table>

  <p>Mit dem berechneten <i>t</i>-Wert können wir entscheiden, ob die entsprechende Nullhypothese verworfen werden soll oder nicht. Gewöhnlich können wir für ein gegebenes Konfidenzintervall <img src="../images/Linear_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png" title="\alpha\,\!" alt="\alpha\,\!" class="tex"> die Hypothese <img src="../images/Linear_Regression_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png" title="H_0\,\!" alt="H_0\,\!" class="tex"> verwerfen, wenn <img src="../images/Linear_Regression_Results/math-9d38e8e7363d66af41530c3fbdf494f5.png" title="|t|&gt;t_{\frac \alpha 2}" alt="|t|&gt;t_{\frac \alpha 2}" class="tex">. Außerdem wird der <i>p</i>-Wert oder die Signifikanzebene mit einem <i>t</i>-Test angezeigt. Wir weisen auch die Nullhypothese <img src="../images/Linear_Regression_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png" title="H_0\,\!" alt="H_0\,\!" class="tex"> zurück, wenn der <i>p</i>-Wert kleiner ist als <img src="../images/Linear_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png" title="\alpha\,\!" alt="\alpha\,\!" class="tex">.</p>

  <h4><a name="Prob.3E.7Ct.7C"></a><span class="mw-headline">Wahrsch.&gt;|t|</span></h4>

  <p>Die Wahrscheinlichkeit, dass <img src="../images/Linear_Regression_Results/math-647d2f77597fee86bb9c3c77cdfdaa3a.png" title="H_0\,\!" alt="H_0\,\!" class="tex"> in dem <i>t</i>-Test oben wahr ist.</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-3e5335a131705b3ef0bb8b250033167d.png" title="prob=2(1-tcdf(|t|,df_{Error}))\,\!" alt="prob=2(1-tcdf(|t|,df_{Error}))\,\!" class="tex"></th>

      <td>
        <p>(22)</p>
      </td>
    </tr>
  </table>

  <p>wobei <i>tcdf(t, df)</i> die untere Wahrscheinlichkeit für die studentisierte <i>t-</i>Verteilung mit dem <i>df</i>-Freiheitsgrad berechnet.</p>

  <h4><a name="LCL_and_UCL"></a><span class="mw-headline">UEG und OEG</span></h4>

  <p>Mit dem <i>t</i>-Wert können wir das <img src="../images/Linear_Regression_Results/math-0cdaba532a6f8ad706e6b8e5e97f0d36.png" title="(1-\alpha )\times 100\%" alt="(1-\alpha )\times 100\%" class="tex">-<b>Konfidenzintervall</b> für jeden Parameter berechnen:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-a27fc4294ee80efb51793fbcd4e23df2.png" title="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" alt="\hat \beta _j-t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}\leq \hat \beta _j\leq \hat \beta _j+t_{(\frac \alpha 2,n^{*}-k)}\varepsilon _{\hat \beta _j}" class="tex"></th>

      <td>
        <p>(23)</p>
      </td>
    </tr>
  </table>

  <p>wobei <img src="../images/Linear_Regression_Results/math-fd9dfeda668ac9b0c5ef311ffdca8f71.png" title="OEG" alt="OEG" class="tex"> und <img src="../images/Linear_Regression_Results/math-a4df64dc5e32716f6de0ec15b0340950.png" title="LCL" alt="LCL" class="tex"> für <b>Oberes Konfidenzintervall</b> bzw. <b>Unteres Konfidenzintervall</b> steht.</p>

  <h4><a name="CI_Half_Width"></a><span class="mw-headline">KI halbe Breite</span></h4>

  <p>Das Konfidenzintervall halbe Breite ist:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-77ec7c61ac9d06bfc9ad4c0ae47b85ce.png" title="CI=\frac{UCL-LCL}2" alt="CI=\frac{UCL-LCL}2" class="tex"></th>

      <td>
        <p>(24)</p>
      </td>
    </tr>
  </table>

  <p>wobei OEG und UEG das <b>obere Konfidenzintervall</b> bzw. <b>untere Konfidenzintervall</b> ist.</p>

  <h3><a name="Fit_Statistics"></a><span class="mw-headline">Statistik zum Fit</span></h3>

  <p>Die Schlüsselwerte der linearen Anpassung werden in der Statistiktabelle zusammengefasst (Zahlen in Klammern zeigen, wie Eigenschaften berechnet werden):</p>

  <dl>
    <dd><a class="image"><img alt="FitStats.png" src="../images/Linear_Regression_Results/FitStats.png" width="313"></a></dd>
  </dl>

  <h4><a name="Degrees_of_Freedom"></a><span class="mw-headline">Freiheitsgrade</span></h4>

  <p>Der Freiheitsgrad des Fehlers Weitere Einzelheiten finden Sie in der <a href="../../UserGuide/UserGuide/Linear_Regression_Results.html#ANOVA_Table" title="UserGuide:Linear Regression Results">ANOVA</a>-Tabelle.</p>

  <h4><a name="Residual_Sum_of_Squares"></a><span class="mw-headline">Summe der Fehlerquadrate</span></h4>

  <p>Die Residuensumme der Quadrate, siehe Formel (19).</p>

  <h4><a name="Reduced_Chi-Sqr"></a><span class="mw-headline">Reduziertes Chi-Quadrat</span></h4>

  <p>Siehe Formel (14).</p>

  <h4><a name="R-Square_.28COD.29"></a><span class="mw-headline">R-Quadrat (COD)</span></h4>

  <p>Die Qualität der linearen Regression kann mit dem <b>Determinationskoeffizienten (COD)</b> oder <b><img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png" title="R^2" alt="R^2" class="tex"></b> gemessen werden, die folgendermaßen berechnet werden können:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-9ce15fecaa71e5021c67564b9f212551.png" title="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" alt="R^2=\frac{SXY}{SXX*TSS}=1-\frac{RSS}{TSS}" class="tex"></th>

      <td>
        <p>(25)</p>
      </td>
    </tr>

    <tr>
      <th><img src="../images/Linear_Regression_Results/math-700ffa56572a1641462ec2aadfb69b87.png" title="TSS=\sum(y_i-\bar{y})^2" alt="TSS=\sum(y_i-\bar{y})^2" class="tex"></th>
    </tr>
  </table>

  <p>wobei <i>TSS</i> die Gesamtsumme der Quadrate und <i>RSS</i> die Residuensumme des Quadrats ist. <img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png" title="R^2" alt="R^2" class="tex"> ist ein Wert zwischen 0 und 1. Liegt er nahe 1, wird die Beziehung zwischen X und Y als stark betrachtet, und wir können einen höheren Konfidenzgrad in unserem Regressionsmodell haben.</p>

  <h4><a name="Adj._R-Square"></a><span class="mw-headline">Kor. R-Quadrat</span></h4>

  <p>Des Weiteren können wir den <b>korrigierten <img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png" title="R^2" alt="R^2" class="tex"></b> wie folgt berechnen:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-68a88ab44a828cde18104979d28917fe.png" title="{\bar R}^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" alt="{\bar R}^2=1-\frac{RSS/df_{Error}}{TSS/df_{Total}}" class="tex"></th>

      <td>
        <p>(26)</p>
      </td>
    </tr>
  </table>

  <h4><a name="R_Value"></a><span class="mw-headline">R-Wert</span></h4>

  <p>Der <i>R</i>-Wert ist die Quadratwurzel von <i><img src="../images/Linear_Regression_Results/math-e31b458b48dd58470b662e66b9742071.png" title="R^2" alt="R^2" class="tex"></i>:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-8dea6550a6f24458c884bb59c192c935.png" title="R=\sqrt{R^2}" alt="R=\sqrt{R^2}" class="tex"></th>

      <td>
        <p>(27)</p>
      </td>
    </tr>
  </table>

  <h4><a name="Pearson.27s_r"></a><span class="mw-headline">Pearson r</span></h4>

  <p>Bei der einfachen linearen Regression ist der Korrelationskoeffizient zwischen x und y, der als <i>r</i> bezeichnet wird, gleich:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-f7f1dc7cce20e9b9b19e47b012ee173d.png" title="r=R\,\!" alt="r=R\,\!" class="tex"> <i>falls <img src="../images/Linear_Regression_Results/math-3a1e29703e364979554e6bd76761bed6.png" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"> positiv ist</i></th>

      <td>
        <p>(28)</p>
      </td>
    </tr>

    <tr>
      <th><img src="../images/Linear_Regression_Results/math-556cec8e44138ef9cf002c70434cb55d.png" title="r=-R\,\!" alt="r=-R\,\!" class="tex"> <i>falls<img src="../images/Linear_Regression_Results/math-3a1e29703e364979554e6bd76761bed6.png" title="\beta _1\,\!" alt="\beta _1\,\!" class="tex"> negativ ist</i></th>
    </tr>
  </table>

  <h4><a name="Root-MSE_.28SD.29"></a><span class="mw-headline">Wurzel-MSE (StAbw)</span></h4>

  <p>Quadratwurzel des Mittelwerts des Fehlers oder die residuale Standardabweichung ist gleich:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-5d65f33336e7ea1ead5adec051bf5572.png" title="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" alt="RootMSE=\sqrt{\frac{RSS}{df_{Error}}}" class="tex"></th>

      <td>
        <p>(29)</p>
      </td>
    </tr>
  </table>

  <h4><a name="Norm_of_Residuals"></a><span class="mw-headline">Betrag der Residuen</span></h4>

  <p>Ist gleich der Quadratwurzel von RSS:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-edb52b53a5f0e033c1872d55206bf72f.png" title="Norm \,of \,Residuals=\sqrt{RSS}" alt="Norm \,of \,Residuals=\sqrt{RSS}" class="tex"></th>

      <td>
        <p>(30)</p>
      </td>
    </tr>
  </table>

  <h2><a name="ANOVA_Table"></a><span class="mw-headline">ANOVA-Tabelle</span></h2>

  <p>Die ANOVA-Tabelle der linearen Anpassung ist:</p>

  <table class="simple">
    <tr>
      <th></th>

      <th>Freiheitsgrade</th>

      <th>Summe der Quadrate</th>

      <th>Mittelwert der Quadrate</th>

      <th>F -Wert</th>

      <th>Wahrsch. &gt; F</th>
    </tr>

    <tr>
      <th>Modell</th>

      <td><i>1</i></td>

      <td><i><img src="../images/Linear_Regression_Results/math-76d28c12ecef51969ed6e4493c3934cf.png" title="SS_{reg} = TSS - RSS" alt="SS_{reg} = TSS - RSS" class="tex"></i></td>

      <td><i><img src="../images/Linear_Regression_Results/math-e5dc62d3c446d80f3ce90dab19161a25.png" title="MS_{reg} = SS_{reg} / 1 " alt="MS_{reg} = SS_{reg} / 1 " class="tex"></i></td>

      <td><i><img src="../images/Linear_Regression_Results/math-d5ea59127172a534421d4b08fd6d3135.png" title="MS_{reg} / MSE " alt="MS_{reg} / MSE " class="tex"></i></td>

      <td><i>p-Wert</i></td>
    </tr>

    <tr>
      <th>Fehler</th>

      <td><i>n* - 1</i></td>

      <td><i>RSS</i></td>

      <td><i>MSE = RSS / (n* - 1)</i></td>

      <td></td>

      <td></td>
    </tr>

    <tr>
      <th>Gesamt</th>

      <td><i>n*</i></td>

      <td><i>TSS</i></td>

      <td></td>

      <td></td>

      <td></td>
    </tr>
  </table>

  <table class="note">
    <tr>
      <td><b>Hinweis</b>: Ist der Schnittpunkt im Modell enthalten, ist <i>n*=n-1</i>. Andernfalls ist <i>n</i>*=n und die Gesamtsumme der Quadrate ist unkorrigiert. Wenn die Steigung fest ist, ist <i><img src="../images/Linear_Regression_Results/math-dfc3cbf5cc5bf940257cfcf8c86ddec0.png" title="df_{Model}" alt="df_{Model}" class="tex"></i> = 0.</td>
    </tr>
  </table>

  <p>Dabei ist hier die Gesamtsumme der Quadrate, TSS:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-4d28d19e4d080da070b15388db314470.png" title="TSS =\sum_{i=1}^nw_i(y_i -\frac{\sum_{i=1}^n w_i y_i} {\sum_{i=1}^n w_i})^2" alt="TSS =\sum_{i=1}^nw_i(y_i -\frac{\sum_{i=1}^n w_i y_i} {\sum_{i=1}^n w_i})^2" class="tex"> <i>(korrigiert)</i></th>

      <td>(31)</td>
    </tr>

    <tr>
      <th><img src="../images/Linear_Regression_Results/math-7c28116e7d91e4726fe4c0d26fa4e93e.png" title="TSS=\sum_{i=1}^n w_iy_i^2" alt="TSS=\sum_{i=1}^n w_iy_i^2" class="tex"> (unkorrigiert)</th>
    </tr>
  </table>

  <p>Der F-Wert ist ein Test, ob das Anpassungsmodell sich signifikant von dem Modell Y = konstant unterscheidet.</p>

  <p>Der <i>p</i>-Wert bzw. die Signifikanzebene wird mit einem <i>F</i>-Test ermittelt. Wenn der <i>p</i>-Wert kleiner als <img src="../images/Linear_Regression_Results/math-06f7895cd704b1cb0921cf98aec71926.png" title="\alpha\,\!" alt="\alpha\,\!" class="tex"> ist, unterscheidet sich das Anpassungsmodell signifikant von dem Modell Y = konstant.</p>

  <p>Wenn der Schnittpunkt mit der Y-Achse bei einem bestimmten Wert festgelegt wird, ist der p-Wert für den <i>F</i>-Test nicht bedeutungsvoll und unterscheidet sich von dem in der linearen Regression ohne die Nebenbedingung des Schnittpunkts mit der Y-Achse.</p>

  <h2><a name="Lack_of_fit_table"></a><span class="mw-headline">Tabelle des Tests auf fehlende Anpassung</span></h2>

  <p>Um den Test auf fehlende Anpassung auszuführen, müssen Sie sich wiederholende Beobachtungen zur Verfügung haben, d. h. "replizierte Daten" , so dass mindestens einer der X-Werte sich innerhalb des Datensatzes oder innerhalb mehrerer Datensätze wiederholt, wenn der Modus Zusammengefasster Fit ausgewählt ist.</p>

  <p>Notationen, die für die Anpassung mit replizierten Daten verwenden werden:</p>

  <table class="formula">
    <tr>
      <td><img src="../images/Linear_Regression_Results/math-9ec98a656b3500239d77f631b67811cc.png" title="y_{ij}" alt="y_{ij}" class="tex"> ist die j-te Messung, die beim i-ten X-Wert im Datensatz gemacht wurde.<br></td>
    </tr>

    <tr>
      <td><img src="../images/Linear_Regression_Results/math-dbebef52b4445e3b11d20855d6e8d5dd.png" title="\bar{y}_{i}" alt="\bar{y}_{i}" class="tex"> ist der Durchschnitt von allen Y-Werten beim i-ten X-Wert.<br></td>
    </tr>

    <tr>
      <td><img src="../images/Linear_Regression_Results/math-0ff3619d52350e78a9bb731ef2c23991.png" title="\hat{y}_{ij}" alt="\hat{y}_{ij}" class="tex"> ist die prognostizierte Antwort für die j-te Messung, die beim i-ten X-Wert gemacht wurde.<br></td>
    </tr>
  </table>

  <p>Die Summe der Quadrate in der Tabelle unten wird ausgedrückt mit:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-5c76b3b72fc70ebfe8fed6e8daf6a7c7.png" title="RSS=\sum_{i}\sum_{j}(y_{ij}-\hat{y}_{ij})^2" alt="RSS=\sum_{i}\sum_{j}(y_{ij}-\hat{y}_{ij})^2" class="tex"></th>
    </tr>

    <tr>
      <th><img src="../images/Linear_Regression_Results/math-3e215ad1665f7e78c1af881e4c6e4ac6.png" title="LFSS=\sum_{i}\sum_{j}(\bar{y}_{i}-\hat{y}_{ij})^2" alt="LFSS=\sum_{i}\sum_{j}(\bar{y}_{i}-\hat{y}_{ij})^2" class="tex"></th>
    </tr>

    <tr>
      <th><img src="../images/Linear_Regression_Results/math-b8b02bc61db58ee0bcf8927677ab3c59.png" title="PESS=\sum_{i}\sum_{j}(y_{ij}-\bar{y}_{i})^2" alt="PESS=\sum_{i}\sum_{j}(y_{ij}-\bar{y}_{i})^2" class="tex"></th>
    </tr>
  </table>

  <p>Die Tabelle des Tests auf fehlende Anpassung der linearen Anpassung ist:</p>

  <table class="simple">
    <tr>
      <th></th>

      <th>Freiheitsgrade</th>

      <th>Summe der Quadrate</th>

      <th>Mittelwert der Quadrate</th>

      <th>F -Wert</th>

      <th>Wahrsch. &gt; F</th>
    </tr>

    <tr>
      <th>Fehlende Anpassung</th>

      <td><i>c-2</i></td>

      <td><i>LFSS</i></td>

      <td><i>MSLF = LFSS / (c - 2)</i></td>

      <td><i>MSLF / MSPE</i></td>

      <td><i>p-Wert</i></td>
    </tr>

    <tr>
      <th>Reiner Fehler</th>

      <td><i>n - c</i></td>

      <td><i>PESS</i></td>

      <td><i>MSPE = PESS / (n - c)</i></td>

      <td></td>

      <td></td>
    </tr>

    <tr>
      <th>Fehler</th>

      <td><i>n*-1</i></td>

      <td><i>RSS</i></td>

      <td></td>

      <td></td>

      <td></td>
    </tr>
  </table>

  <table class="note">
    <tr>
      <td>
        <b>Hinweis</b>:<br>

        <p>Wenn der Schnittpunkt mit der Y-Achse im Modell enthalten ist, dann ist <i>n*=n-1</i>. Andernfalls ist <i>n</i>*=n und die Gesamtsumme der Quadrate ist unkorrigiert. Wenn die Steigung fest ist, ist <i><img src="../images/Linear_Regression_Results/math-dfc3cbf5cc5bf940257cfcf8c86ddec0.png" title="df_{Model}" alt="df_{Model}" class="tex"></i> = 0.</p>

        <p><i>c</i> bezeichnet die Anzahl der eindeutigen X-Werte. Wenn der Schnittpunkt mit der Y-Achse festgelegt ist, ist der Freiheitsgrad für die fehlende Anpassung c-1.</p>
      </td>
    </tr>
  </table>

  <h2><a name="Covariance_and_Correlation_Matrix"></a><span class="mw-headline">Kovarianz- und Korrelationsmatrix</span></h2>

  <p>Die Kovarianzmatrix der linearen Regression wird berechnet durch:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-b0bcaee4a98e9106fc977c7ab6cf5b79.png" title="\begin{pmatrix} Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\ Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1) \end{pmatrix}=\sigma ^2\frac 1{SXX}\begin{pmatrix} \sum \frac{x_i^2}n &amp; -\bar x \\-\bar x &amp; 1 \end{pmatrix}" alt="\begin{pmatrix} Cov(\beta _0,\beta _0) &amp; Cov(\beta _0,\beta _1)\\ Cov(\beta _1,\beta _0) &amp; Cov(\beta _1,\beta _1) \end{pmatrix}=\sigma ^2\frac 1{SXX}\begin{pmatrix} \sum \frac{x_i^2}n &amp; -\bar x \\-\bar x &amp; 1 \end{pmatrix}" class="tex"></th>

      <td>
        <p>(32)</p>
      </td>
    </tr>
  </table>

  <p>Die Korrelation zwischen zwei beliebigen Parametern ist:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-b029662d99694045f6e48fab550e2b23.png" title="\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " alt="\rho (\beta _i,\beta _j)=\frac{Cov(\beta _i,\beta _j)}{\sqrt{Cov(\beta _i,\beta _i)}\sqrt{Cov(\beta _j,\beta _j)}} " class="tex"></th>

      <td>
        <p>(33)</p>
      </td>
    </tr>
  </table>

  <h2><a name="Outliers"></a><span class="mw-headline">Ausreißer</span></h2>

  <p>Die Ausreißer sind die Punkte, deren absolute Werte im studentisierten Residuendiagramm größer als 2 sind.</p>

  <p><img src="../images/Linear_Regression_Results/math-3efc2a4ea2a6baf34c2d5f2bf5bd4765.png" title="abs(Studentized Residual)&gt;2" alt="abs(Studentized Residual)&gt;2" class="tex"></p>

  <p>Studentisiertes Residuum wird in <a href="../../UserGuide/UserGuide/Graphic_Residual_Analysis.html#Detecting_outliers_by_transforming_residuals" title="UserGuide:Graphic Residual Analysis">Ausreißer durch Transformieren der Residuen erkennen</a> eingeführt.</p>

  <h2><a name="Residual_Analysis"></a><span class="mw-headline">Residuenanalyse</span></h2>

  <p><img src="../images/Linear_Regression_Results/math-af2593b49488f8bc44396795792c2d79.png" title="r_i" alt="r_i" class="tex"> steht für reguläres Residuum <img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png" title="res_i" alt="res_i" class="tex">.</p>

  <h3><a name="Standardized"></a><span class="mw-headline">Standardisiert</span></h3>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-edba44ad3ec53a729a081e552ffd8d52.png" title="r_i^{\prime }=\frac{r_i}s_\varepsilon" alt="r_i^{\prime }=\frac{r_i}s_\varepsilon" class="tex"></th>

      <td>
        <p>(34)</p>
      </td>
    </tr>
  </table>

  <h3><a name="Studentized"></a><span class="mw-headline">Studentisiert</span></h3>

  <p>Sind auch bekannt als intern studentisierte Residuen.</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-5b4f1a55a60f41732cd45963fc4d7899.png" title="r_i^{\prime }=\frac{r_i}{s_\varepsilon\sqrt{1-h_i}}" alt="r_i^{\prime }=\frac{r_i}{s_\varepsilon\sqrt{1-h_i}}" class="tex"></th>

      <td>
        <p>(35)</p>
      </td>
    </tr>
  </table>

  <h3><a name="Studentized_deleted"></a><span class="mw-headline">Studentisiert gelöscht</span></h3>

  <p>Sind auch bekannt als extern studentisierte Residuen.</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-33555ae8465559528419ee80ff636f91.png" title="r_i^{\prime }=\frac{r_i}{s_{\varepsilon-i}\sqrt{1-h_i}}" alt="r_i^{\prime }=\frac{r_i}{s_{\varepsilon-i}\sqrt{1-h_i}}" class="tex"></th>

      <td>
        <p>(36)</p>
      </td>
    </tr>
  </table>

  <p>In den Gleichungen der <b>studentisierten</b> und <b>studentisiert gelöschten</b> Residuen ist <img src="../images/Linear_Regression_Results/math-57339b77b3af15427b7154f4daf8a223.png" title="h_i" alt="h_i" class="tex"> das <i>i</i>-te diagonale Element der Matrix <img src="../images/Linear_Regression_Results/math-44c29edb103a2872f519ad0c9a0fdaaa.png" title="P" alt="P" class="tex">:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-609f90ce395d2346200e55f1a632bab0.png" title="P=X(X'X)^{-1}X^{\prime }" alt="P=X(X'X)^{-1}X^{\prime }" class="tex"></th>

      <td>
        <p>(37)</p>
      </td>
    </tr>
  </table>

  <p><img src="../images/Linear_Regression_Results/math-7af4042b185319dd95cc2dcb158a88b2.png" title="s_{\varepsilon-i}" alt="s_{\varepsilon-i}" class="tex"> bedeutet die Varianz wird berechnet, basierend auf alle Punkte, schließt aber den <i>i</i>ten Punkt aus.</p>

  <h2><a name="Confidence_and_Prediction_Bands"></a><span class="mw-headline">Konfidenz- und Prognosebänder</span></h2>

  <p>Für einen bestimmten Wert <img src="../images/Linear_Regression_Results/math-333d67b49be014c08ced5a715a0aa659.png" title="x_p\,\!" alt="x_p\,\!" class="tex"> liegt das <img src="../images/Linear_Regression_Results/math-7f5c0246ea96d63005bb89b2c2966a90.png" title="100(1-\alpha )\% " alt="100(1-\alpha )\% " class="tex">-<b>Konfidenzintervall</b> für den Mittelwert von <img src="../images/Linear_Regression_Results/math-bfb6488d6c250ac5aeed1bbf139baaa5.png" title="y\,\!" alt="y\,\!" class="tex"> bei <img src="../images/Linear_Regression_Results/math-29b229c7c531f688ffb25ceadae2a0f6.png" title="x=x_p\,\!" alt="x=x_p\,\!" class="tex">:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-2fc729f483df6668e6cfd645dcb324fb.png" title="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" alt="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" class="tex"></th>

      <td>
        <p>(38)</p>
      </td>
    </tr>
  </table>

  <p>Und das <img src="../images/Linear_Regression_Results/math-7f5c0246ea96d63005bb89b2c2966a90.png" title="100(1-\alpha )\% " alt="100(1-\alpha )\% " class="tex">-<b>Prognoseintervall</b> für den Mittelwert von <img src="../images/Linear_Regression_Results/math-bfb6488d6c250ac5aeed1bbf139baaa5.png" title="y\,\!" alt="y\,\!" class="tex"> bei <img src="../images/Linear_Regression_Results/math-29b229c7c531f688ffb25ceadae2a0f6.png" title="x=x_p\,\!" alt="x=x_p\,\!" class="tex"> ist:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-6825d104f5fef27aeab6053f04b8892c.png" title="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{1+\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" alt="\hat y\pm t_{(\frac \alpha 2,n^{*}-1)}s_\varepsilon \sqrt{1+\frac 1n+\frac{(x_p-\bar x)^2}{SXX}}" class="tex"></th>

      <td>
        <p>(39)</p>
      </td>
    </tr>
  </table>

  <h2><a name="Confidence_Ellipses"></a><span class="mw-headline">Konfidenzellipsen</span></h2>

  <p>Angenommen das Variablenpaar (X, Y) folgt einer zweidimensionalen Normalverteilung, so können wir die Korrelation zwischen zwei Variablen durch eine Konfidenzellipse untersuchen. Die Konfidenzellipse ist bei (<img src="../images/Linear_Regression_Results/math-35b4d0362fcb2694d21b2abf35ccfc7f.png" title="\bar x" alt="\bar x" class="tex">,<img src="../images/Linear_Regression_Results/math-096544be8a676d2c106621014410cb9d.png" title="\bar y" alt="\bar y" class="tex"> ) zentriert und die große Halbachse a und die kleine Halbachse <i>b</i> können folgendermaßen ausgedrückt werden:</p>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-eda409b7a7a814287c66467c2beba53c.png" title=" a=c\sqrt{\frac{\sigma _x^2+\sigma _y^2+\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" alt=" a=c\sqrt{\frac{\sigma _x^2+\sigma _y^2+\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" class="tex"></th>
    </tr>

    <tr>
      <th><img src="../images/Linear_Regression_Results/math-8fc5aa9e1aad046be36d3bf32f61d9aa.png" title=" b=c\sqrt{\frac{\sigma _x^2+\sigma _y^2-\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" alt=" b=c\sqrt{\frac{\sigma _x^2+\sigma _y^2-\sqrt{(\sigma _x^2-\sigma _y^2)+4r^2\sigma _x^2\sigma _y^2}}2}" class="tex"></th>

      <td>
        <p>(40)</p>
      </td>
    </tr>
  </table>

  <p>Für ein gegebenes Konfidenzniveau von <img src="../images/Linear_Regression_Results/math-d7d1320edf83ed676aa5f848c9711fe9.png" title=" (1-\alpha )\,\! " alt=" (1-\alpha )\,\! " class="tex">:</p>

  <ul>
    <li>Die Konfidenzellipse für die Grundgesamtheit <b>Mittelwert</b> wird definiert als:</li>
  </ul>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-97456e57ebc619b9ce5476c311cf3eef.png" title=" c=\sqrt{\frac{2(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " alt=" c=\sqrt{\frac{2(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " class="tex"></th>

      <td>
        <p>(41)</p>
      </td>
    </tr>
  </table>

  <ul>
    <li>Die Konfidenzellipse für <b>Prognose</b> wird definiert als:</li>
  </ul>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-cbb5ed712a41f59f2cd27d2f0e29b096.png" title=" c=\sqrt{\frac{2(n+1)(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " alt=" c=\sqrt{\frac{2(n+1)(n-1)}{n(n-2)}(\alpha ^{\frac 2{2-n}}-1)} " class="tex"></th>

      <td>
        <p>(42)</p>
      </td>
    </tr>
  </table>

  <ul>
    <li>Der <b>Neigungswinkel</b> der Ellipse wird definiert als:</li>
  </ul>

  <table class="formula">
    <tr>
      <th><img src="../images/Linear_Regression_Results/math-4c3d04a2e2be2ca32d3e4ae7960b3966.png" title="\beta =\frac 12\arctan \frac{2r\sqrt{\sigma _x^2\sigma _y^2}}{\sigma _x^2-\sigma _y^2}" alt="\beta =\frac 12\arctan \frac{2r\sqrt{\sigma _x^2\sigma _y^2}}{\sigma _x^2-\sigma _y^2}" class="tex"></th>

      <td>
        <p>(43)</p>
      </td>
    </tr>
  </table>

  <h2><a name="Finding_Y.2FX_from_X.2FY"></a><span class="mw-headline"><a href="../../UserGuide/UserGuide/Finding_Y_X_from_X_Y_Standard_Curves.html" title="UserGuide:Finding Y X from X Y Standard Curves">Y von X finden/X von Y finden</a></span></h2>

  <h2><a name="Residual_Plots"></a><span class="mw-headline">Residuendiagramme</span></h2>

  <h3><a name="Resudial_Type"></a><span class="mw-headline">Residuentyp</span></h3>

  <p>Wählen Sie einen Residuentyp unter <b>Regulär</b>, <b>Standardisiert</b>, <b>Studentisiert</b>, <b>Studentisiert gelöscht</b> für die Diagramme.</p>

  <h3><a name="Residual_vs._Independent"></a><span class="mw-headline">Residuen vs. Unabhängig</span></h3>

  <p>Punktdiagramm der Residuen <img src="../images/Linear_Regression_Results/math-9b207167e5381c47682c6b4f58a623fb.png" title="res" alt="res" class="tex"> vs. unabhängige Variable <img src="../images/Linear_Regression_Results/math-75087036f44bb88958df97586ae3bd1e.png" title="x_1,x_2,\dots,x_k" alt="x_1,x_2,\dots,x_k" class="tex">; jede Zeichnung befindet sich in einem separaten Diagramm.</p>

  <h3><a name="Residual_vs._Predicted_Value"></a><span class="mw-headline">Residuen vs. prognostizierte Werte</span></h3>

  <p>Punktdiagramm der Residuen <img src="../images/Linear_Regression_Results/math-9b207167e5381c47682c6b4f58a623fb.png" title="res" alt="res" class="tex"> vs. Anpassungsergebnisse <img src="../images/Linear_Regression_Results/math-c9e53cbdffe795c0913cd927f13ffb9b.png" title="\hat{y_i}" alt="\hat{y_i}" class="tex"></p>

  <h3><a name="Residual_vs._Order_of_the_Data"></a><span class="mw-headline">Residuen vs. die Ordnung der Datendiagramme</span></h3>

  <p><img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png" title="res_i" alt="res_i" class="tex"> vs. Abfolgenummer <img src="../images/Linear_Regression_Results/math-865c0c0b4ab0e063e5caa3387c1a8741.png" title="i" alt="i" class="tex"></p>

  <h3><a name="Histogram_of_the_Residual"></a><span class="mw-headline">Histogramm des Residuums</span></h3>

  <p>Histogramm des Residuums</p>

  <h3><a name="Residual_Lag_Plot"></a><span class="mw-headline">Verzögertes Residuendiagramm</span></h3>

  <p>Residuen <img src="../images/Linear_Regression_Results/math-2bcba7a477778a0e5cadc454c7a01628.png" title="res_i" alt="res_i" class="tex"> vs. verzögertes Residuum <img src="../images/Linear_Regression_Results/math-00407b58eb35fecbfea6dc5d4413e493.png" title="res_{(i–1)}" alt="res_{(i–1)}" class="tex"></p>

  <h3><a name="Normal_Probability_Plot_of_Residuals"></a><span class="mw-headline">Wahrscheinlichkeitsnetz (Normal) für Residuen</span></h3>

  <p>Das Wahrscheinlichkeitsnetz der Residuen (Normal) kann verwendet werden, um zu prüfen, ob die Varianz ebenfalls normalverteilt ist. Wenn das sich ergebende Diagramm ungefähr linear ist, nehmen wir weiterhin an, dass die Fehlerterme normal verteilt sind. Das Diagramm basiert auf Perzentilen versus geordnete Residuen. Die Perzentile werden geschätzt mit</p>

  <p><img src="../images/Linear_Regression_Results/math-ec995bff3922b2290f85bed243de5eb3.png" title="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" alt="\frac{(i-\frac{3}{8})}{(n+\frac{1}{4})}" class="tex"></p>

  <p>wobei <i>n</i> die Gesamtanzahl der Datensätze und <i>i</i> die <i>i</i>-ten Daten sind. Bitte lesen Sie auch <a href="../../UserGuide/UserGuide/Probability_Plot_and_Q-Q_Plot.html" title="UserGuide:Probability Plot and Q-Q Plot">Wahrscheinlichkeitsdiagramm und Q-Q-Diagramm</a>.</p>
