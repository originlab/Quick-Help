<h1 class="firstheading">クラスター分析</h1><p class='urlname' style='display: none'>Cluster-Analysis</p>
<table id="toc" class="toc"><tr><td><div id="toctitle"><h2>内容</h2></div>
<ul>
<li class="toclevel-1 tocsection-1"><a href="#Goals"><span class="tocnumber">1</span> <span class="toctext">目的</span></a></li>
<li class="toclevel-1 tocsection-2"><a href="#Hierarchical_Cluster_Analysis"><span class="tocnumber">2</span> <span class="toctext">階層的クラスター分析</span></a>
<ul>
<li class="toclevel-2 tocsection-3"><a href="#Classifying_Observations"><span class="tocnumber">2.1</span> <span class="toctext">観測値を分類する</span></a></li>
<li class="toclevel-2 tocsection-4"><a href="#Classifying_Variables"><span class="tocnumber">2.2</span> <span class="toctext">変数を分類する</span></a></li>
<li class="toclevel-2 tocsection-5"><a href="#Selecting_Cluster_Methods"><span class="tocnumber">2.3</span> <span class="toctext">クラスター手法を選ぶ</span></a>
<ul>
<li class="toclevel-3 tocsection-6"><a href="#Number_of_Clusters"><span class="tocnumber">2.3.1</span> <span class="toctext">クラスターの数</span></a></li>
<li class="toclevel-3 tocsection-7"><a href="#Standardizing_the_Variables"><span class="tocnumber">2.3.2</span> <span class="toctext">変数の標準化</span></a></li>
<li class="toclevel-3 tocsection-8"><a href="#Distance_Measures"><span class="tocnumber">2.3.3</span> <span class="toctext">距離の測定</span></a></li>
<li class="toclevel-3 tocsection-9"><a href="#Cluster_Methods"><span class="tocnumber">2.3.4</span> <span class="toctext">クラスター手法</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toclevel-1 tocsection-10"><a href="#K-Means_Cluster_Analysis"><span class="tocnumber">3</span> <span class="toctext">K-means法クラスター分析</span></a>
<ul>
<li class="toclevel-2 tocsection-11"><a href="#Selecting_Cluster_Methods_2"><span class="tocnumber">3.1</span> <span class="toctext">クラスター手法を選ぶ</span></a></li>
</ul>
</li>
<li class="toclevel-1 tocsection-12"><a href="#Handling_Missing_Values"><span class="tocnumber">4</span> <span class="toctext">欠損値の扱い</span></a></li>
</ul>
</td></tr></table>
<p>クラスター分析は大きいデータを小さいグループ(クラスター)に分けるのによく使われる手法です。<a href="../../UserGuide/Category/Discriminant_Analysis.html" title="カテゴリー：判別分析">判別分析</a>と同じように、クラスター分析は観測データを分類することを重要な課題としています。ところが判別分析はグループメンバーシップが分かっている状態でのみ分類のルールを生成します。クラスター分析はより初歩的な手法でグループの数またはグループメンバーシップについての仮定は何も持っていません。
</p>
<h2><a name="Goals"></a><span class="mw-headline">目的</span></h2>
<ul><li><b>分類</b>
<dl><dd>クラスター分析はユーザの方に関係性の示唆をしたり、大きい数の変数や観測値がある中で系統的構造を作成する手段を提供しています。
</dd></dl>
</li></ul>
<h2><a name="Hierarchical_Cluster_Analysis"></a><span class="mw-headline">階層的クラスター分析</span></h2>
<p>階層的クラスター分析は、測定された特徴について比較的均一的なクラスターを見つける統計的な第一手法です。まず、それぞれの特徴が別々のクラスターに分類され、順次クラスター同士が統合していきます。各段階でクラスターの数を減らしていき、最終的に1つのクラスターが残るまで行われます。クラスター手法としては相違点、あるいはクラスターを形成する際の距離を使用しています。
</p><p><br />
</p>
<h3><a name="Classifying_Observations"></a><span class="mw-headline">観測値を分類する</span></h3>
<p>階層的クラスター分析は小さい数のサンプルに適しています。もしサンプル(n)が大きい場合、結果にたどり着くまでのアルゴリズムがとても遅くなるかもしれません。通常、サンプルサイズが200より大きくなる場合、K-means法クラスター分析を使用することをお勧めします。
</p><p><br />
</p>
<h3><a name="Classifying_Variables"></a><span class="mw-headline">変数を分類する</span></h3>
<p>階層的クラスター分析は均一的な変数のグループがいかに形成されたのかを見ることができる唯一の手法になります。K-means法クラスター分析は観測値を分類することしかできません。 
</p><p><br />
</p>
<h3><a name="Selecting_Cluster_Methods"></a><span class="mw-headline">クラスター手法を選ぶ</span></h3>
<h4><a name="Number_of_Clusters"></a><span class="mw-headline"> クラスターの数</span></h4>
<p>分析にいくつのクラスターを設定すればいいのか、ということに対して決まった方法はありません。一度<a href="../../UserGuide/UserGuide/Interpreting_Results_of_Hierarchical_Cluster_Analysis.html#Dendrogram" title="ユーザガイド:階層的クラスター分析の結果を解釈するには">樹形図</a>とクラスターの特徴を確認して、それからクラスターの数を反復しながら調整していく必要があるかもしれません。
</p>
<h4><a name="Standardizing_the_Variables"></a><span class="mw-headline"> 変数の標準化</span></h4>
<p>もし変数が別々のスケール(目盛り)で計測されていた場合、3つの方法のうち1つを使い変数を標準化することができます。全てのこの結果は距離の計測により等しい割合で貢献していますが、変数内の違いに関する情報が失われる可能性もあります。
</p>
<h4><a name="Distance_Measures"></a><span class="mw-headline">距離の測定</span></h4>
<ul><li>ユークリッド距離 
<dl><dd>ユークリッド距離は最も一般的な距離の測定方法で、これは多次元空間の幾何学的距離を表しています。この方法は連続変数にのみ使用できます。 
</dd></dl>
</li></ul>
<ul><li>平方ユークリッド距離 
<dl><dd>平方ユークリッド距離はより離れたオブジェクト間の距離に重点を置いています。 
</dd></dl>
</li></ul>
<ul><li>city-block 距離
<dl><dd>このcity-block距離とユークリッド距離はどちらもミンコフスキー計量の特殊な事例です。ユークリッド距離が2点間の最短距離を表しているならば、city-block距離は各次元における距離の合計になります。
</dd></dl>
</li></ul>
<dl><dd><table class="note">

<tr>
<td><b>Notes</b>:データが正規化された場合、ユークリッドと平方ユークリッド距離は両方とも影響されます。分析の途中でデータを正規化したい場合、city-block距離をお使いください。
</td></tr></table></dd></dl>
<ul><li>コサイン距離
<dl><dd>2つのベクトル間の角度のコサイン
</dd></dl>
</li></ul>
<ul><li>ピアソンの相関距離
<dl><dd>1とコサイン係数の2つの観測値の違いです。コサイン係数は、2つのベクトル間の距離の余弦です。
</dd></dl>
</li></ul>
<ul><li>Jaccard距離
<dl><dd>1とJaccard係数の2つの観測値の違いです。バイナリデータに関しては、Jaccard係数は2つの観測値の交点と統合したサイズの比と等しくなります。
</dd></dl>
</li></ul>
<h4><a name="Cluster_Methods"></a><span class="mw-headline">クラスター法</span></h4>
<ul><li>Nearest Neighbor (最短距離) 
<dl><dd>この方法では、2つのクラスター間の距離がそれらに最も近いオブジェクトの距離として使用されます。この方法はプロットされたクラスターが引き延ばされている(連なっている)場合に有効です。
</dd></dl>
</li></ul>
<ul><li>Furthest neighbor (最長距離) 
<dl><dd>この方法では2つのクラスター間の距離が別々のクラスター内にある2つのオブジェクトの最大距離として使用されます。この方法はプロットされたクラスターが特定のまとまりになっている場合(連なった鎖状ではないとき)に有効です。
</dd></dl>
</li></ul>
<ul><li>群平均 
<dl><dd>この方法では2つのクラスター間の距離は別々のクラスター内にある2つのオブジェクトの平均距離として計算されます。通常の場合、この方法がより多くの情報を使用するため、最もお勧めです。 
</dd></dl>
</li></ul>
<ul><li>重心 
<dl><dd>クラスターの重心とすべての変数の距離の合計が最も小さいものが統合されます。クラスターの重心は多次元空間の平均点になります。 
</dd></dl>
</li></ul>
<ul><li> 中央値
<dl><dd> この方法は重心法ととても似ていますが、こちらは重み付けがされていません。クラスターの大きさが著しく違う場合は使用しないでください。
</dd></dl>
</li></ul>
<dl><dd><table class="note">

<tr>
<td>重心法または中央値法が選択された場合、平方ユークリッド距離を使用することをお勧めします。
</td></tr></table></dd></dl>
<ul><li>Ward
<dl><dd>各クラスターで、そのクラスターに属する変数の平均が計算されます。そしてすべての場合で、クラスター平均間の平方ユークリッド距離が計算されます。全ての場合でこちらの距離を合計します。統合されるクラスターはこの合計の上昇が最も小さい2つが統合されます。この方法はクラスター内距離の平方和の上昇を最小に抑えます。この方法はたいてい小さいクラスターを生成します。
</dd></dl>
</li></ul>
<h2><a name="K-Means_Cluster_Analysis"></a><span class="mw-headline">K-Means 法クラスター分析</span></h2>
<p>K-means法クラスター分析は観測値を分類するのに、K個のクラスターを使います。この考え方はデータとそれが対応するクラスター重心間の距離が最小になるようにするものです。K-means法分析はクラスター分析を行う中でも最も簡単なアルゴリズムの１つを使う方法なので階層的クラスター分析よりも速く分析結果が出てきます。
</p><p>一般にサンプルサイズが100より大きくなる場合、K-means法分析を使用することを考慮してください。しかしK-meansクラスター分析はユーザが観測データの重心、または最低限、クラスターに分けられるグループ数をすでに知っているものであると仮定しています。
</p>
<h3><a name="Selecting_Cluster_Methods_2"></a><span class="mw-headline">クラスター手法を選ぶ</span></h3>
<p>K-meansクラスター分析での最初のステップはクラスター中心を見つけることです。<a href="../../UserGuide/Category/Cluster_Analysis.html#Hierarchical_Cluster_Analysis" title="カテゴリー：クラスター分析">階層的クラスター分析</a>を小さなサンプルサイズで行い、適当な初期のクラスター中心を求めてください。または、クラスター数を指定してOriginが自動で十分に離れている値を初期のクラスター中心として設定します。この自動解析は外れ値に敏感にできています。ですので、分析を始める前に外れ値の有無を確認してください。
</p>
<h2><a name="Handling_Missing_Values"></a><span class="mw-headline">欠損値の扱い</span></h2>
<p>トレーニングデータ/グループ範囲に、欠損値がある場合、全てのケース(全行)が分析から除外されます。
</p><p><br />
</p>
<table class="topiclist">

<tr>
<td>
<p><i><b>このセクションで説明している項目</b></i> 
</p>
<ul><li><b>階層的クラスター分析</b>
<ul><li><a href="../../UserGuide/UserGuide/The_Hierarchical_Cluster_Analysis_Dialog_Box.html" title="ユーザガイド:階層的クラスター分析ダイアログボックス">階層的クラスター分析ダイアログボックス</a>
</li><li><a href="../../UserGuide/UserGuide/Interpreting_Results_of_Hierarchical_Cluster_Analysis.html" title="ユーザガイド：階層的クラスター分析の結果を解釈するには">結果の解釈</a>
</li><li><a href="../../UserGuide/UserGuide/Algorithm_(Hierarchical_Cluster_Analysis).html" title="ユーザガイド:アルゴリズム(階層的クラスター分析)">アルゴリズム</a>
</li></ul>
</li><li><b>K-Means 法クラスター分析</b>
<ul><li><a href="../../UserGuide/UserGuide/The_K-Means_Cluster_Analysis_Dialog_Box.html" title="ユーザガイド:K-means法クラスター分析ダイアログボックス">K-means法クラスター分析ダイアログボックス</a>
</li><li><a href="../../UserGuide/UserGuide/Interpreting_Results_of_K-Means_Cluster_Analysis.html" title="ユーザガイド:K-means法クラスター分析の結果を解釈するには">結果の解釈</a>
</li><li><a href="../../UserGuide/UserGuide/Algorithm_(K-Means_Cluster_Analysis).html" title="ユーザガイド:アルゴリズム(K-means法クラスター分析)">アルゴリズム</a>
</li></ul>
</li><li><a class="external text" href="../../Tutorial/Tutorial/Cluster_Analysis.html">チュートリアル</a>
</li><li><a href="../../UserGuide/UserGuide/Reference_(Cluster_Analysis).html" title="ユーザガイド:参考(クラスター分析)"><b>参考文献</b></a>
</li></ul>
</td></tr></table>





    