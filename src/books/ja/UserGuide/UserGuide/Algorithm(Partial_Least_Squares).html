<h1 class="firstheading">アルゴリズム(部分最小二乗法)</h1>
<table id="toc" class="toc">
<tr>
<td>
<div id="toctitle">
<h2>内容</h2>
</div>
<ol>
<li class="toclevel-1 tocsection-1"><a href="#Partial_Least_Squares_Method"><span class="tocnumber">1</span> <span class="toctext">部分最小二乗法</span></a>
<ol>
<li class="toclevel-2 tocsection-2"><a href="#Wold.27s_Iterative"><span class="tocnumber">1.1</span> <span class="toctext">Woldの反復法</span></a></li>
<li class="toclevel-2 tocsection-3"><a href="#SVD"><span class="tocnumber">1.2</span> <span class="toctext">SVD</span></a></li>
</ol>
</li>
<li class="toclevel-1 tocsection-4"><a href="#Cross_Validation"><span class="tocnumber">2</span> <span class="toctext">交差確認</span></a></li>
<li class="toclevel-1 tocsection-5"><a href="#Response_Prediction"><span class="tocnumber">3</span> <span class="toctext">目的変数予測</span></a></li>
<li class="toclevel-1 tocsection-6"><a href="#Quantities"><span class="tocnumber">4</span> <span class="toctext">量</span></a></li>
</ol>
</td>
</tr>
</table>
<p><br /></p>
<p class="urlname" style="display: none;">PLS-Algorithm</p>
<p><b style="font-weight: bold;">部分最小二乗法</b>は多くの修正された説明変数と既知の目的変数を使用して不明な目的変数を予測するモデルを作成するために使用します。このような場合、複数の線形フィットは、通常、オーバーフィットを起こすので適切ではありません。部分最小二乗法はしばしば、混合物のスペクトルからどのような内容物が含まれているのか推測する際に使用されます。</p>
<p>部分最小二乗法は因子を説明変数の線形結合によって抽出し、抽出された因子空間に対して説明変数と目的変数を投影します。</p>
<p>欠損値がある観測データは分析から除外、つまりリストワイズ形式で除外されます。</p>
<p>仮に、観測データ数、説明変数、目的変数を順に<b>n</b>、<b>m</b>、<b>r</b> とします。説明変数はサイズが <img src="../images/Algorithm(Partial_Least_Squares)/math-6bfb727b656d02ac43818be9f0eb9951.png" title="n \times m" alt="n \times m" class="tex" border="0" />である行列<b>X</b>で表わされ、目的変数は同じようにサイズが<img src="../images/Algorithm(Partial_Least_Squares)/math-af98676085fb502b67ab7824d5d311ab.png" title="n \times r" alt="n \times r" class="tex" border="0" />である行列<b>Y</b>で表わします。行列<b>X</b>と<b>Y</b>のそれぞれの列から平均値を引き、<img src="../images/Algorithm(Partial_Least_Squares)/math-7204a8dfdb61b3496295b04c057e337d.png" title="X_0" alt="X_0" class="tex" border="0" /> と <img src="../images/Algorithm(Partial_Least_Squares)/math-85e22683116ee1466ea1a9ec5036fc20.png" title="Y_0" alt="Y_0" class="tex" border="0" />のようにします。</p>
<ul>
<li>スケール変数</li>
</ul>
<p>行列内の各列、<img src="../images/Algorithm(Partial_Least_Squares)/math-7204a8dfdb61b3496295b04c057e337d.png" title="X_0" alt="X_0" class="tex" border="0" />は標準偏差によって割られます。</p>
<a name="Partial_Least_Squares_Method" id="Partial_Least_Squares_Method"></a>
<h2><span class="mw-headline">部分最小二乗法</span></h2>
<p>Originは2種類の手法、<b>Woldの</b><b>反復法</b>と<b>特異値分解(SVD)</b>で抽出した因子を計算します。</p>
<a name="Wold.27s_Iterative" id="Wold.27s_Iterative"></a>
<h3><span class="mw-headline">Woldの反復法</span></h3>
<p>初期ベクトルとして<b>u</b>を使用します。r=1で初期化したu=Yの場合、他の時はuはランダムな値のベクトルになります。</p>
<ul>
<li>それぞれの反復を<b>w</b>が収束するまで続けます。</li>
</ul>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-b6b50101810e64cf8accaa230ecdb182.png" title="w=X_0^Tu" alt="w=X_0^Tu" class="tex" border="0" />となり、wを <img src="../images/Algorithm(Partial_Least_Squares)/math-b66a5ae450a857691794bb99dcfbfb49.png" title="w=w/|| w ||" alt="w=w/|| w ||" class="tex" border="0" />で正規化します。</dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-38f9932f42606abf00f0c4b37a0e8a1c.png" title="t=X_0w" alt="t=X_0w" class="tex" border="0" />となり、tを <img src="../images/Algorithm(Partial_Least_Squares)/math-bdb5de551100700f2538f41d5d7e4568.png" title="t=t/|| t ||" alt="t=t/|| t ||" class="tex" border="0" />で正規化します。</dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-8af0e35756f39c13f5e14818e651daf8.png" title="q=Y_0^Tt" alt="q=Y_0^Tt" class="tex" border="0" />となり、qを<img src="../images/Algorithm(Partial_Least_Squares)/math-122282269731d839a51bb2afdea640f9.png" title="q=q/|| q ||" alt="q=q/|| q ||" class="tex" border="0" />で正規化します。</dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-ccf5221c571441660f8e3c74bca4b9ed.png" title="u=Y_0q" alt="u=Y_0q" class="tex" border="0" /></dd>
</dl>
<p><b style="font-weight: bold;">w</b>が収束すると更新され、</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-38f9932f42606abf00f0c4b37a0e8a1c.png" title="t=X_0w" alt="t=X_0w" class="tex" border="0" />となり、tを <img src="../images/Algorithm(Partial_Least_Squares)/math-bdb5de551100700f2538f41d5d7e4568.png" title="t=t/|| t ||" alt="t=t/|| t ||" class="tex" border="0" />で正規化します。</dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-730e6da5ac16fd9ceefe41c082da1252.png" title="p=X_0^Tt" alt="p=X_0^Tt" class="tex" border="0" /></dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-8af0e35756f39c13f5e14818e651daf8.png" title="q=Y_0^Tt" alt="q=Y_0^Tt" class="tex" border="0" /></dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-ccf5221c571441660f8e3c74bca4b9ed.png" title="u=Y_0q" alt="u=Y_0q" class="tex" border="0" /></dd>
</dl>
<dl>
<dd>ここで、<b>w</b>、<b>t</b>、<b>u</b>、<b>p</b>、<b>q</b> はそれぞれ x 重み、x スコア、yスコア、x ローディング、yローディングを表します。</dd>
</dl>
<ul>
<li>上記の手法で残差行列を<b>k</b>回処理します。</li>
</ul>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-93d56c341b0ca427c9f7726ae13b7422.png" title="\hat{X}_0=X_0-tp^T" alt="\hat{X}_0=X_0-tp^T" class="tex" border="0" /></dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-51e61b0988638a3c901508377b8abf9a.png" title="\hat{Y}_0=Y_0-tq^T" alt="\hat{Y}_0=Y_0-tq^T" class="tex" border="0" /></dd>
</dl>
<p>そして<b>k</b>因子は作成できます。k因子についてのx 重み、x スコア、yスコア、x ローディング、yローディングはそれぞれ行列<b>W</b>、<b>T</b>、<b>U</b>、<b>P</b>、<b>Q</b>で表わせます。</p>
<p>Originでは各因子のx スコア、yスコア、x ローディング、yローディングの符号は、各因子のx重みの合計を行う事で正規化され、正の数にします。</p>
<a name="SVD" id="SVD"></a>
<h3><span class="mw-headline">SVD</span></h3>
<ul>
<li>Xの重みは第一因子に対して働きます。</li>
</ul>
<p>wは<img src="../images/Algorithm(Partial_Least_Squares)/math-d3b7fcc35bfc8a4c8754ac87eb445dce.png" title="X_0^TY_0" alt="X_0^TY_0" class="tex" border="0" />の正規化した第一左辺特異ベクトルで、</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-38f9932f42606abf00f0c4b37a0e8a1c.png" title="t=X_0w" alt="t=X_0w" class="tex" border="0" />となり、tを <img src="../images/Algorithm(Partial_Least_Squares)/math-bdb5de551100700f2538f41d5d7e4568.png" title="t=t/|| t ||" alt="t=t/|| t ||" class="tex" border="0" />で正規化します。</dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-730e6da5ac16fd9ceefe41c082da1252.png" title="p=X_0^Tt" alt="p=X_0^Tt" class="tex" border="0" /></dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-8af0e35756f39c13f5e14818e651daf8.png" title="q=Y_0^Tt" alt="q=Y_0^Tt" class="tex" border="0" /></dd>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-ccf5221c571441660f8e3c74bca4b9ed.png" title="u=Y_0q" alt="u=Y_0q" class="tex" border="0" /></dd>
</dl>
<ul>
<li>上記の手法で残差行列を<b>k</b>回処理します。</li>
</ul>
<p>そして、<b>k</b>因子を抽出します。</p>
<a name="Cross_Validation" id="Cross_Validation"></a>
<h2><span class="mw-headline">交差確認</span></h2>
<p>OriginはLeave-one-out手法で最適な因子数を検出します。この手法は1つの観測を省き、他の観測値だけでモデルを作成してその省いた観測値に対して反応を推測します。</p>
<ul>
<li>PRESS</li>
</ul>
<p><b style="font-weight: bold;">PRESS</b>は予測した残差の二乗和です。これは次式のように計算されます。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-2bb99673c7dae505a5074d37a34238cc.png" title="\text{PRESS} = \sum_{i=1}^n \sum_{j=1}^r (Y_{ij} - \hat{Y}_{ij})^2" alt="\text{PRESS} = \sum_{i=1}^n \sum_{j=1}^r (Y_{ij} - \hat{Y}_{ij})^2" class="tex" border="0" /></dd>
<dd>ここで、<img src="../images/Algorithm(Partial_Least_Squares)/math-d338719f401d4c4ebc7500be0b29514f.png" title="\hat{Y}_{ij}" alt="\hat{Y}_{ij}" class="tex" border="0" /> はLeave-one-out 手法で予測されたY値です。</dd>
</dl>
<p><b style="font-weight: bold;">Note</b>：もし変数がスケール化されている場合、PRESSはそのスケール化した結果になります。</p>
<p>因子の最大数がkの場合、PRESSは 0, 1, ... <b>k</b>因子分まで計算します。0因子の場合、</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-b7d18e5222c06e7b0e22724d6663c1a0.png" title="\text{PRESS} = \sum_{i=1}^n \sum_{j=1}^r (Y_{ij} - \bar{Y}_{j})^2" alt="\text{PRESS} = \sum_{i=1}^n \sum_{j=1}^r (Y_{ij} - \bar{Y}_{j})^2" class="tex" border="0" /></dd>
<dd>ここで、<img src="../images/Algorithm(Partial_Least_Squares)/math-8c45df17a99a3ed466ee1e93bd254c0b.png" title="\bar{Y}_{j}" alt="\bar{Y}_{j}" class="tex" border="0" /> は<b>j</b>番目のY値に関する平均値です。</dd>
</dl>
<ul>
<li>PRESSのルート平均</li>
</ul>
<p>PRESSのルート平均はPRESSの平方根を取った平均値です。定義は、次のようになります。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-c89e91c6b17eb49a2f586459c42e1a28.png" title="\text{Root Mean PRESS} = \sqrt{ \frac{\text{PRESS}}{ (n-1)r } }" alt="\text{Root Mean PRESS} = \sqrt{ \frac{\text{PRESS}}{ (n-1)r } }" class="tex" border="0" /></dd>
</dl>
<p>Originは最小<b>PRESSのルート平均</b>を使用して<b>交差確認</b>内で最適な因子数を探します。</p>
<a name="Response_Prediction" id="Response_Prediction"></a>
<h2><span class="mw-headline">反応予測</span></h2>
<p>モデルが作成できると、フィットしたモデルの係数によって反応の予測が可能になります。係数は、重みとローディングの行列から計算されます。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-02f4f161619c2ac7b8f60b1a8ec75377.png" title=" C = W(P^TW)^{-1}Q^T" alt=" C = W(P^TW)^{-1}Q^T" class="tex" border="0" /></dd>
</dl>
<p>そして、予測された反応は次のように計算されます。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-863ee5c8217b01dc657f2a57041bf0c4.png" title="\hat{Y}_0 = C X_{0}" alt="\hat{Y}_0 = C X_{0}" class="tex" border="0" /></dd>
</dl>
<p>ここで、変数は中心化されています。変数がスケール化もされている場合、反応もスケール化されて返されます。</p>
<a name="Quantities" id="Quantities"></a>
<h2><span class="mw-headline">量</span></h2>
<ul>
<li>Xの効果の因子寄与</li>
</ul>
<p><i style="font-style: italic;">i</i>番目のX変数に因子寄与します。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-9454ddb3b66073775022dfe4c601d7e6.png" title="\frac{ \sum_{j=1}^{k} P_{lj} }{ \sum_{i=1}^{n} {X_0}_{il} }" alt="\frac{ \sum_{j=1}^{k} P_{lj} }{ \sum_{i=1}^{n} {X_0}_{il} }" class="tex" border="0" /></dd>
</dl>
<p>X変数の因子寄与は次の通りです。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-e227ae838508b8c3dbe33344011b25cf.png" title="\frac{ \sum_{l=1}^{m} \sum_{j=1}^{k} P_{lj} }{ \sum_{l=1}^{m} \sum_{i=1}^{n} {X_0}_{il} }" alt="\frac{ \sum_{l=1}^{m} \sum_{j=1}^{k} P_{lj} }{ \sum_{l=1}^{m} \sum_{i=1}^{n} {X_0}_{il} }" class="tex" border="0" /></dd>
</dl>
<ul>
<li>Yの応答の因子寄与</li>
</ul>
<p><i style="font-style: italic;">i</i>番目のY変数に因子寄与します。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-e484a3503851dbdad1acb9ab9d2669e5.png" title="\frac{ \sum_{j=1}^{k} Q_{lj} }{ \sum_{i=1}^{n} {Y_0}_{il} }" alt="\frac{ \sum_{j=1}^{k} Q_{lj} }{ \sum_{i=1}^{n} {Y_0}_{il} }" class="tex" border="0" /></dd>
</dl>
<p>Y変数の因子寄与は次の通りです。</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-8fa4ab98f0ecff18c88fde7a722e512b.png" title="\frac{ \sum_{l=1}^{r} \sum_{j=1}^{k} Q_{lj} }{ \sum_{l=1}^{r} \sum_{i=1}^{n} {Y_0}_{il} }" alt="\frac{ \sum_{l=1}^{r} \sum_{j=1}^{k} Q_{lj} }{ \sum_{l=1}^{r} \sum_{i=1}^{n} {Y_0}_{il} }" class="tex" border="0" /></dd>
</dl>
<ul>
<li>VIP統計</li>
</ul>
<p>VIP(投影に影響を与える変数)は、各説明変数が目的変数では平均分散を有している理由を説明します。</p>
<ul>
<li>残差</li>
</ul>
<p>Xの残差は、</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-39aea3e39f856d1056159322814df5be.png" title="X_r = X_0 - TP^T" alt="X_r = X_0 - TP^T" class="tex" border="0" /></dd>
</dl>
<p>Yの残差は、</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-9a5df1c150cac2d9932562fb94f2bb68.png" title="Y_r = Y_0 - TQ^T" alt="Y_r = Y_0 - TQ^T" class="tex" border="0" /></dd>
</dl>
<p>変数がスケール化されている場合、残差もスケール化されて返されます。</p>
<ul>
<li>距離</li>
</ul>
<p>i番目の観測に対するXモデルまでの距離</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-d6dc7ff1a8b24c9af5825689cecd65bf.png" title="\text{Dist}_x = \sqrt{ \sum_{j=1}^m X_{rij}^2 }" alt="\text{Dist}_x = \sqrt{ \sum_{j=1}^m X_{rij}^2 }" class="tex" border="0" /></dd>
</dl>
<p>i番目の観測に対するYモデルまでの距離</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-b570af1cb5d8f2d9d602291ffffcda89.png" title="\text{Dist}_y = \sqrt{ \sum_{j=1}^r Y_{rij}^2 }" alt="\text{Dist}_y = \sqrt{ \sum_{j=1}^r Y_{rij}^2 }" class="tex" border="0" /></dd>
</dl>
<ul>
<li>T二乗</li>
</ul>
<p>1番目の観測に関するT二乗値</p>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-485318be3ff60bd28173132ec5f68bac.png" title="T^2=\sum_{j=1}^k \frac{T_{ij}}{\text{Var}_j}" alt="T^2=\sum_{j=1}^k \frac{T_{ij}}{\text{Var}_j}" class="tex" border="0" /></dd>
</dl>
<p>ここで、<img src="../images/Algorithm(Partial_Least_Squares)/math-fb263895839a43be9f27e4303d05371b.png" title="\text{Var}_j" alt="\text{Var}_j" class="tex" border="0" />はj番目の因子に対するXスコアの分散を表します。</p>
<ul>
<li>T二乗の管理限界線</li>
</ul>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-f0892d0e56f3583c093037f56a0a205b.png" title="n(n-1)^2\text{betainv}(0.95,k/2.0,(n-k-1)/2.0)" alt="n(n-1)^2\text{betainv}(0.95,k/2.0,(n-k-1)/2.0)" class="tex" border="0" /></dd>
</dl>
<ul>
<li>スコアプロットの信頼楕円の半径</li>
</ul>
<dl>
<dd><img src="../images/Algorithm(Partial_Least_Squares)/math-5875c784787a0d8a31f68b77e53eccf9.png" title="\sqrt{(n-1)^2/n \cdot \text{betainv}(0.95,1,(n-3)/2.0) \cdot \text{Var}_j}" alt="\sqrt{(n-1)^2/n \cdot \text{betainv}(0.95,1,(n-3)/2.0) \cdot \text{Var}_j}" class="tex" border="0" /></dd>
</dl>
<p>ここで、<img src="../images/Algorithm(Partial_Least_Squares)/math-fb263895839a43be9f27e4303d05371b.png" title="\text{Var}_j" alt="\text{Var}_j" class="tex" border="0" />はj番目の因子に対するXスコアまたはYスコアの分散を表します。</p>


